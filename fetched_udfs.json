{
    "17068": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Target Price@1"
            }
        ],
        "code": "\treturn previous_line.replace(\"\u00a3\",\"\").replace(\",\",\"\")",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360079000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17069": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Broker Deadline@1"
            }
        ],
        "code": "\tfrom datetime import datetime, timedelta\n\t\n\t\n\tdef add_calendar_days(start_date, days):\n\t    \"\"\"Add calendar days (including weekends) to a given date.\"\"\"\n\t    return start_date + timedelta(days=days)\n\t\n\tdef add_weekdays(start_date, days):\n\t    \"\"\"Add weekdays to a given date, skipping weekends.\"\"\"\n\t    while days > 0:\n\t        start_date += timedelta(days=1)\n\t        if start_date.weekday() < 5:  # Weekdays: Mon-Fri = 0-4\n\t            days -= 1\n\t    return start_date\n\t\n\tdef process_date(input_str):\n\t    today = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)\n\t\n\t    # Try to parse dd/mm/yyyy\n\t    try:\n\t        parsed_date = datetime.strptime(input_str, \"%d/%m/%Y\")\n\t    except ValueError:\n\t        # If year not present, try dd/mm check\n\t        try:\n\t            try:\n\t                parsed_date = datetime.strptime(input_str, \"%d/%m\")\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t            except:\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t        except ValueError:\n\t            final_date = add_calendar_days(today, 5)\n\t            return final_date.strftime(\"%d/%m/%Y\")\n\t            \n\t    # Compare dates\n\t    base_date = parsed_date if parsed_date >= today else today\n\t\n\t    return base_date.strftime(\"%d/%m/%Y\")\n\t\n\tresult = process_date(previous_line) \n\treturn result",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360079000",
        "name": "adding_business_logic",
        "udf_type": "REFINER"
    },
    "17070": {
        "args": [],
        "code": "\treturn \"Fleet\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360079000",
        "name": "default_value",
        "udf_type": "REFINER"
    },
    "17071": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Incepts_On",
                "value": "Incepts On"
            }
        ],
        "code": "\tfrom datetime import datetime, timedelta\n\t\n\tdef add_one_year_minus_one_day(date_str):\n\t    # Parse the input date (DD/MM/YYYY)\n\t    input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t    try:\n\t        # Try to add one year directly\n\t        one_year_later = input_date.replace(year=input_date.year + 1)\n\t    except ValueError:\n\t        # Handle Feb 29 (leap year issue) and other invalid dates\n\t        temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t        one_year_later = temp_date\n\t    # Subtract one day\n\t    final_date = one_year_later - timedelta(days=1)\n\t    # Return formatted date as DD/MM/YYYY with leading zeros\n\t    return final_date.strftime(\"%d/%m/%Y\")\n\t    \n\ttry:\n\t    expiry_date = add_one_year_minus_one_day(Incepts_On)\n\t    return expiry_date\n\texcept Exception as e:\n\t    return \"\"\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360079000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17072": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Main Cover Type@1"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360080000",
        "name": "check_cover_type",
        "udf_type": "REFINER"
    },
    "17073": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Driver Licence Tenure@0"
            }
        ],
        "code": "\tcolumns = ['Driver Name', 'Licence Tenure']\n\tif previous_line == '[]':\n\t    return [columns]\n\telse:\n\t  return previous_line",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360080000",
        "name": "clean_llm_result",
        "udf_type": "REFINER"
    },
    "17074": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Incepts_On",
                "value": "Incepts On"
            }
        ],
        "code": "\treturn Incepts_On",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360080000",
        "name": "get_inception_date",
        "udf_type": "REFINER"
    },
    "17075": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Expires_On",
                "value": "Expires On"
            }
        ],
        "code": "\treturn Expires_On",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360081000",
        "name": "get_expiry_date",
        "udf_type": "REFINER"
    },
    "17076": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Is_Holding_Broker_Details",
                "value": "Is Holding Broker Details"
            },
            {
                "data_type": "FIELD",
                "name": "Is_Holding_Broker_From_Context",
                "value": "Is Holding Broker From Context"
            }
        ],
        "code": "\timport json\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef extract_holding_broker_from_text(text: str) -> str:\n\t    brace_stack = []\n\t    start_idx = -1\n\t    end_idx = -1\n\t\n\t    # Step 1: Find the first complete {...} block\n\t    for i, ch in enumerate(text):\n\t        if ch == '{':\n\t            if not brace_stack:\n\t                start_idx = i\n\t            brace_stack.append('{')\n\t        elif ch == '}':\n\t            if brace_stack:\n\t                brace_stack.pop()\n\t                if not brace_stack:\n\t                    end_idx = i + 1  # Include the closing brace\n\t                    break\n\t\n\t    # Step 2: If we found a full JSON block\n\t    if start_idx != -1 and end_idx != -1:\n\t        json_str = text[start_idx:end_idx]\n\t\n\t        try:\n\t            data = json.loads(json_str)\n\t            print(data)\n\t            holding_broker = data.get('holding_broker')  \n\t            print(\"holding broker\", holding_broker)\n\t            return holding_broker\n\t        except Exception as e:\n\t            print(e)\n\t            return ''\n\t\n\t    return ''\n\t\n\tdef normalize_holding_broker(value: str) -> str:\n\t    value = value.strip().lower()\n\t    if value == 'yes':\n\t        return 'Yes'\n\t    elif value == 'no':\n\t        return 'No'\n\t    return ''\n\t\n\tholding_broker_explicit = extract_holding_broker_from_text(Is_Holding_Broker_Details)\n\tholding_broker_context = extract_holding_broker_from_text(Is_Holding_Broker_From_Context)\n\t\n\tholding_broker_explicit = normalize_holding_broker(holding_broker_explicit)\n\tholding_broker_context = normalize_holding_broker(holding_broker_context)\n\t\n\tprint(\"holding_broker_explicit\",holding_broker_explicit)\n\tprint(\"holding_broker_context\",holding_broker_context)\n\t\n\t# Prefer explicit if available\n\tif holding_broker_explicit:\n\t    print('if')\n\t    result = holding_broker_explicit\n\telse:\n\t    print('else')\n\t    result = holding_broker_context\n\treturn result",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360081000",
        "name": "return_holding_broker_value",
        "udf_type": "REFINER"
    },
    "17077": {
        "args": [],
        "code": "\treturn \"New Business\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360081000",
        "name": "get_transaction_type",
        "udf_type": "REFINER"
    },
    "17078": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Driver Details@0"
            }
        ],
        "code": "\timport pandas as pd\n\timport json, re, ast\n\timport traceback\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = [\n\t         \"Driver Name\",\n\t         \"Driver DOB\",\n\t         \"Licence Date\",\n\t         \"Conviction Code\",\n\t          \"Driver Claims\"\n\t    ]\n\t\n\t\n\tdef get_df_with_regex_match(previous_line, columns_order):\n\t    parsed, inner_content = None, None\n\t    df = pd.DataFrame(columns=columns_order)\n\t    try:\n\t        match = re.search(r\"```(?:\\w+)?\\s*([\\s\\S]*?)\\s*```\", previous_line)\n\t        if match:\n\t            inner_content = match.group(1).strip()\n\t    \n\t            # Step 2: Try to parse it as JSON\n\t            try:\n\t                parsed = json.loads(inner_content)\n\t                print(parsed)\n\t            except json.JSONDecodeError:\n\t                # If JSON parsing fails, fallback to literal_eval\n\t                try:\n\t                    parsed = ast.literal_eval(inner_content)\n\t                except Exception as e:\n\t                    print(\"Parsing failed:\", e)\n\t                    parsed = None\n\t          \n\t        if parsed:\n\t            df = convert_to_dataframe(parsed)\n\t            return True, df \n\t        else:\n\t            return False, df \n\t    except Exception as e:\n\t      return False, pd.DataFrame(columns=columns_order)\n\t\n\tdef parse_markdown_format(previous_line, columns_order):\n\t    try:\n\t        lines = [line.strip() for line in previous_line.strip().split('\\n') if line.strip().startswith('|') and '---' not in line]\n\t        list_of_lists = [ [cell.strip() for cell in line.strip('|').split('|')] for line in lines ]\n\t        if list_of_lists:\n\t            df = convert_to_dataframe(list_of_lists)\n\t            return True, df\n\t        else:\n\t            return False, pd.DataFrame(columns=columns_order)\n\t    except:\n\t        return False, pd.DataFrame(columns=columns_order)\n\t\n\ttry:\n\t    \n\t    df = pd.DataFrame(columns=columns_order)\n\t    \n\t    flg, df = get_df_with_regex_match(previous_line, columns_order)\n\t    print(flg)\n\t    if not flg:\n\t        flg, df = parse_markdown_format(previous_line, columns_order)\n\t    # print(previous_line)\n\t    print(flg)\n\t    if not flg:\n\t      try:\n\t          data = json.loads(previous_line)\n\t          # print(\"data\",data)\n\t          df = convert_to_dataframe(data)\n\t          # print(\"1st\",df)\n\t\n\t      except Exception as e:\n\t          data = ast.literal_eval(previous_line)\n\t          df = convert_to_dataframe(data)\n\t\n\t    # print(\"2nd \",df)\n\t    df.fillna(\"\", inplace=True)\n\t    \n\t    if len(df.columns.tolist()) > 0:\n\t        # df[\"Conviction Code\"] = df[\"Conviction Code\"].replace([\"No\", \"no\"], \"\")\n\t        df[\"Conviction Code\"] = df[\"Conviction Code\"].astype(str).str.strip().replace([\"no\", \"No\"], \"\")\n\t        df[\"Licence Date\"] = df[\"Licence Date\"].apply(\n\t    lambda x: f\"01/01/{x.strip()}\" if re.fullmatch(r\"\\s*(19|20)\\d{2}\\s*\", str(x)) else x)\n\t        df = df[columns_order]\n\t        df = [df.columns.tolist()] + df.values.tolist()\n\t        return df\n\t    else:\n\t        return [columns_order]\n\t      \n\texcept Exception as e:\n\t    return [columns_order]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360081000",
        "name": "get_cleaned_df",
        "udf_type": "REFINER"
    },
    "17079": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Driver Age@0"
            }
        ],
        "code": "\tcolumns = ['Driver Name', 'Age']\n\tif previous_line == '[]':\n\t    return [columns]\n\telse:\n\t  return previous_line",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360082000",
        "name": "clean_llm_result",
        "udf_type": "REFINER"
    },
    "17080": {
        "args": [],
        "code": "\tfrom datetime import date\n\ttoday = str(date.today())\n\tprint(today)\n\treturn today",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360082000",
        "name": "get_todays_date",
        "udf_type": "REFINER"
    },
    "17081": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Driver_Data_Merged",
                "value": "Driver Data Merged"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\t\n\tdef classify_driver(row):\n\t    try:\n\t        age = int(row[\"Age\"])\n\t        conviction_code = str(row[\"Conviction Code\"]).strip()\n\t        \n\t        if age < 25:\n\t            return \"Young Driver\"\n\t        elif age >= 25 and str(conviction_code).strip() not in [\"\", None,\"N/A\",\"null\"]:\n\t            return \"Convicted Driver\"\n\t        else:\n\t            return \"\"\n\t    except:\n\t        return \"\"  # Fallback in case of data issues\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Driver Type']\n\t\n\ttry:\n\t  table = json.loads(Driver_Data_Merged)\n\t  df = convert_to_dataframe(table)\n\t  df[\"Driver Type\"] = df.apply(classify_driver, axis=1)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360082000",
        "name": "get_driver_type",
        "udf_type": "REFINER"
    },
    "17082": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Driver_Details",
                "value": "Driver Details"
            },
            {
                "data_type": "FIELD",
                "name": "Driver_Age",
                "value": "Driver Age"
            },
            {
                "data_type": "FIELD",
                "name": "Driver_Licence_Tenure",
                "value": "Driver Licence Tenure"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\ttry:\n\t  Driver_Details = json.loads(Driver_Details)\n\t  Driver_Age = json.loads(Driver_Age)\n\t  Driver_Licence_Tenure = json.loads(Driver_Licence_Tenure)\n\t  merged_df = []\n\t  \n\t  try:\n\t      Driver_Details = convert_to_dataframe(Driver_Details)\n\t      merged_df = Driver_Details\n\t  except Exception as e:\n\t      print(\"no driver details \", e)\n\t\n\t  try:\n\t      Driver_Age = convert_to_dataframe(Driver_Age)\n\t  except Exception as e:\n\t      print(\"no driver conviction details \", e)\n\t\n\t  try:\n\t      Driver_Licence_Tenure = convert_to_dataframe(Driver_Licence_Tenure)\n\t  except Exception as e:\n\t      print(\"no driver licence details \", e)\n\t\n\t  # print(Driver_Details.columns)\n\t  # print(Driver_Age.columns)\n\t\n\t  try:\n\t      merged_df = pd.merge(Driver_Details, Driver_Age,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(\"no driver licence details \", e)\n\t\n\t  try:\n\t      merged_df_all = pd.merge(merged_df, Driver_Licence_Tenure, on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      merged_df_all = merged_df.copy()\n\t      merged_df_all.fillna('', inplace=True)\n\t      if len(merged_df_all.columns.tolist()) > 0:\n\t          merged_Df = [merged_df_all.columns.tolist()] + merged_df_all.values.tolist()\n\t          print(\"merging issue \", e)\n\t          return merged_Df\n\t      else:\n\t          return [[\"Driver DOB\", \"Licence Date\", \"Conviction Code\", \"Age\", \"Driver Name\", \"Licence Tenure\"]]\n\t\n\t  merged_df_all.fillna('', inplace=True)\n\t\n\t  if len(merged_df_all.columns.tolist()) > 0:\n\t      merged_Df = [merged_df_all.columns.tolist()] + merged_df_all.values.tolist()\n\t      return merged_Df\n\t  else:\n\t      return [[\"Driver DOB\", \"Licence Date\", \"Conviction Code\", \"Age\", \"Driver Name\", \"Licence Tenure\"]]\n\texcept:\n\t  return [[\"Driver DOB\", \"Licence Date\", \"Conviction Code\", \"Age\", \"Driver Name\", \"Licence Tenure\"]]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360083000",
        "name": "merge_driver_details",
        "udf_type": "REFINER"
    },
    "17083": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Driver_Data_Merged",
                "value": "Driver Data Merged"
            },
            {
                "data_type": "FIELD",
                "name": "Drive_Type",
                "value": "Drive Type"
            }
        ],
        "code": "\timport pandas as pd\n\timport json, traceback\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\ttry:\n\t  Driver_Data_Merged = json.loads(Driver_Data_Merged)\n\t  Drive_Type = json.loads(Drive_Type)\n\t\n\t  merged_df = []\n\t\n\t  try:\n\t      Driver_Data_Merged = convert_to_dataframe(Driver_Data_Merged)\n\t  except Exception as e:\n\t      print(e)\n\t      print(traceback.format_exc())\n\t  try:\n\t      Drive_Type = convert_to_dataframe(Drive_Type)\n\t  except Exception as e:\n\t      merged_df = Driver_Data_Merged.copy()\n\t      print(traceback.format_exc())\n\t      print(e)\n\t\n\t  print(Driver_Data_Merged.columns,\"hi\", Drive_Type.columns)\n\t  try:\n\t      merged_df = pd.merge(Driver_Data_Merged, Drive_Type,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(e)\n\t      print(traceback.format_exc())\n\t  \n\t  try:\n\t      rename_dict = {\n\t        'Driver Name': 'Driver Name',\n\t        'Driver DOB' : 'Driver D.O.B',\n\t        'Licence Date': 'Drivers Licence - Date obtained',\n\t        'Licence Tenure':'Driver: Years Appropriate Licence Held',\n\t        'Conviction Code': 'Conviction Details',\n\t        'Driver Type': 'Driver Type'\n\t      }\n\t\n\t      safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t      merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t      \n\t      columns_order = [ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']\n\t\n\t      for col in columns_order:\n\t          if col not in merged_df.columns:\n\t              merged_df[col] = ''\n\t      \n\t      merged_df['Driver: Years Appropriate Licence Held'] = merged_df['Driver: Years Appropriate Licence Held'].applymap(lambda x: x if x in ['1', '2', '3', '4', '5'] else '5+')\n\t\n\t      existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t                  \n\t      merged_df.replace('N/A', '', inplace=True)\n\t      merged_df.replace('\"\"', '', inplace=True)\n\t      merged_df.replace('null', '', inplace=True)\n\t\n\t      merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t      merged_df = merged_df.dropna(how='all')\n\t      # merged_df = merged_df.where(pd.notna(merged_df), \"\")\n\t\n\t      merged_df = merged_df.fillna('')\n\t\n\t      merged_df = merged_df[existing_columns]\n\t\n\t      # merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t  except Exception as e:\n\t      print(\"Exception in renaming \", e)\n\t\n\t      for col in columns_order:\n\t          if col not in merged_df.columns:\n\t              merged_df[col] = ''\n\t      \n\t      if len(merged_df.columns.tolist()) > 0:\n\t          merged_df = merged_df[columns_order]\n\t          merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t          return merged_df\n\t      else:\n\t          return [columns_order]\n\t  \n\t  if len(merged_df.columns.tolist()) > 0:\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept:\n\t  return [[ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360083000",
        "name": "form_driver_table",
        "udf_type": "REFINER"
    },
    "17084": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Vehicle Schedule Data@1"
            },
            {
                "data_type": "FIELD",
                "name": "Effective_From",
                "value": "Effective From"
            },
            {
                "data_type": "FIELD",
                "name": "Effective_To",
                "value": "Effective To"
            }
        ],
        "code": "\t\n\timport pandas as pd\n\timport json, re\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\tcolumn_order = ['Effective From', 'Effective To','Vehicle Registration', 'Cover - Vehicle']\n\tprint(previous_line)\n\ttry:\n\t    try:\n\t        vehicel_schedule_data = json.loads(str(previous_line))\n\t    except:\n\t        # previous_line = previous_line.replace(\"```\",\"\").replace(\"json\",\"\")\n\t        matches = re.findall(r'\\[.*?\\]', previous_line)\n\t        \n\t        if len(matches) == 1:\n\t            vehicel_schedule_data = json.loads(matches[0])\n\t        else:\n\t            # Case 2: Multiple lists, convert all\n\t            vehicel_schedule_data = [json.loads(m) for m in matches]\n\t\n\t    merged_df = []\n\t    \n\t    try:\n\t        vehicel_schedule_data = convert_to_dataframe(vehicel_schedule_data)\n\t        \n\t    except Exception as e:\n\t        print(\"no vehicel_schedule_data details \", e)\n\t    \n\t    try:\n\t        vehicel_schedule_data['Effective From'] = Effective_From\n\t    except:\n\t        vehicel_schedule_data['Effective From'] = ''\n\t    \n\t    try:\n\t        vehicel_schedule_data['Effective To'] = Effective_To\n\t    except:\n\t        vehicel_schedule_data['Effective To'] = ''\n\t\n\t    vehicel_schedule_data.fillna('', inplace=True)\n\t    vehicel_schedule_data = vehicel_schedule_data.replace('N/A', '')\n\t    vehicel_schedule_data = vehicel_schedule_data.replace('\"\"', '')\n\t    try:\n\t        vehicel_schedule_data = vehicel_schedule_data.rename(columns={'Cover Vehicle': 'Cover - Vehicle'})\n\t    except Exception as e:\n\t        print(e)\n\t\n\t    for col in column_order:\n\t        if col not in vehicel_schedule_data.columns:\n\t            vehicel_schedule_data[col] = ''\n\t\n\t\n\t    if len(vehicel_schedule_data.columns.tolist()) > 0:\n\t        vehicel_schedule_data = vehicel_schedule_data[column_order]\n\t        vehicel_schedule_data = [vehicel_schedule_data.columns.tolist()] + vehicel_schedule_data.values.tolist()\n\t        return vehicel_schedule_data\n\t    else:\n\t        return [['Effective From', 'Effective To','Vehicle Registration', 'Cover - Vehicle']]\n\texcept:\n\t    return [['Effective From', 'Effective To','Vehicle Registration', 'Cover - Vehicle']]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360083000",
        "name": "form_vehicle_schedule",
        "udf_type": "REFINER"
    },
    "17085": {
        "args": [],
        "code": "\treturn \"Mid Market\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360083000",
        "name": "return_business_category",
        "udf_type": "REFINER"
    },
    "17086": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Vehicle_Schedule_Table",
                "value": "Vehicle Schedule Table"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame( data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\ttry:\n\t  if Vehicle_Schedule_Table == \"[]\" or Vehicle_Schedule_Table == 'n/a':\n\t    return \"0\"\n\t  else:\n\t    Vehicle_Schedule_Table = json.loads(Vehicle_Schedule_Table)\n\t    Vehicle_Schedule_Table = convert_to_dataframe(Vehicle_Schedule_Table)\n\t    non_empty_count = Vehicle_Schedule_Table['Vehicle Registration'].replace('', np.nan).dropna().shape[0]\n\t    return non_empty_count\n\texcept:\n\t  return \"0\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360084000",
        "name": "get_number_of_notifiable_vehicles",
        "udf_type": "REFINER"
    },
    "17087": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Number_of_Notifiable_Vehicles",
                "value": "Vehicle Count From Table"
            },
            {
                "data_type": "FIELD",
                "name": "Target_Price",
                "value": "Target Price"
            }
        ],
        "code": "\t# def clean_premium(premium_value):\n\t#     if premium_value is None:\n\t#         return None\n\t#     if isinstance(premium_value, (int, float)):\n\t#         return float(premium_value)\n\t#     try:\n\t#         # Remove currency symbols, commas, and whitespace\n\t#         # print(premium_value)\n\t#         # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t#         cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t#         print(cleaned)\n\t#         return float(cleaned)\n\t#     except Exception:\n\t#         return None\n\t# def determine_offering_type(vehicle_count, premium_cleaned=None):\n\t#     if vehicle_count is None:\n\t#         return \"\"\n\t#     if vehicle_count <= 19:\n\t#         if premium_cleaned is None or premium_cleaned < 10000:\n\t#             return \"Mini Fleet\"\n\t#     elif 20 <= vehicle_count <= 149:\n\t#         if premium_cleaned is None or 10000 <= premium_cleaned < 250000:\n\t#             # print(\"yes\")\n\t#             return \"Vantage Fleet\"\n\t#     elif vehicle_count >= 150:\n\t#         if premium_cleaned is None or premium_cleaned >= 250000:\n\t#             return \"Mid Corp\"\n\t#     return \"None\"\n\t# try:\n\t#     Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\t# except:\n\t#     return \"None\"\n\t# try:\n\t#     premium_cleaned = clean_premium(Target_Price)\n\t#     print(premium_cleaned)\n\t# except:\n\t#     premium_cleaned = None\n\t\n\t# offering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\t# return offering_type\n\t\n\t\n\tdef clean_premium(premium_value):\n\t    if premium_value is None:\n\t        return None\n\t    if isinstance(premium_value, (int, float)):\n\t        return float(premium_value)\n\t    try:\n\t        # Remove currency symbols, commas, and whitespace\n\t        # print(premium_value)\n\t        # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t        cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t        print(cleaned)\n\t        return float(cleaned)\n\t    except Exception:\n\t        return None\n\t\n\tdef determine_offering_type(vehicle_count, premium_cleaned=None):    \n\t\n\t    if vehicle_count == 0:\n\t        if premium_cleaned is None or premium_cleaned < 10000:\n\t            return \"Mini Fleet\"\n\t        \n\t        elif 10000 <= premium_cleaned < 250000:\n\t            return \"Vantage Fleet\"\n\t        \n\t        elif premium_cleaned >= 250000:\n\t            return \"Mid Corp\"\n\t\n\t    if vehicle_count <= 19:\n\t            return \"Mini Fleet\"\n\t            \n\t    elif 20 <= vehicle_count <= 149:\n\t            return \"Vantage Fleet\"\n\t        \n\t    elif vehicle_count >= 150:\n\t            return \"Mid Corp\"\n\t\n\t\n\ttry:\n\t    Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\texcept:\n\t    Number_of_Notifiable_Vehicles = 0\n\t\n\ttry:\n\t    premium_cleaned = clean_premium(Target_Price)\n\texcept:\n\t    premium_cleaned = None\n\t\n\toffering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\treturn offering_type",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360084000",
        "name": "get_offering_type",
        "udf_type": "REFINER"
    },
    "17088": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Vehicle_Schedule_Data",
                "value": "Vehicle Schedule Data"
            }
        ],
        "code": "\t# # Import Python packages\n\timport json\n\timport pandas as pd\n\t\n\t# list_of_dct = json.loads(Vehicle_Schedule_Data)\n\t# # print(list_of_dct)\n\t\n\t# try:\n\t#     unique_cover_basis_set = set(item['Cover Vehicle'] for item in list_of_dct)\n\t#     unique_cover_basis_list = list(unique_cover_basis_set)\n\t#     unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t#     return unique_cover_basis_list_without_na\n\t\n\t# except Exception as e:\n\t#     print(\"in ex\", e)\n\t#     # print(type(list_of_dct))\n\t#     unique_first_col = list(set(row[0] for row in list_of_dct))\n\t#     if 'Cover Vehicle' in unique_first_col:\n\t#         unique_first_col.remove('Cover Vehicle')\n\t\n\t#     return unique_first_col\n\t    \n\t\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tVehicle_Schedule_Data = json.loads(Vehicle_Schedule_Data)\n\t\n\tif Vehicle_Schedule_Data in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tVehicle_Schedule_Data = convert_to_dataframe(Vehicle_Schedule_Data)\n\tunique_vals = Vehicle_Schedule_Data['Cover - Vehicle'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360084000",
        "name": "get_unique_values_from_cover_basis_col",
        "udf_type": "REFINER"
    },
    "17089": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Cover Basis Mapping@0"
            }
        ],
        "code": "\timport json\n\t\n\ttry:\n\t    if previous_line not in [\"\", None, \"[]\", \" \", \"N/A\", \"null\"]:\n\t        result = previous_line\n\t    else:\n\t        result = \"{}\"\n\t\n\texcept Exception as e:\n\t    print(e)\n\t    result = \"{}\"\n\t\n\treturn result  # Ensure this is inside a function\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360084000",
        "name": "refine_result",
        "udf_type": "REFINER"
    },
    "17090": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Cover_Basis_Mapping",
                "value": "Cover Basis Mapping"
            },
            {
                "data_type": "FIELD",
                "name": "Unique__Values__Cover__Basis",
                "value": "Unique_Values_Cover_Basis"
            },
            {
                "data_type": "FIELD",
                "name": "Vehicle_Schedule_Data",
                "value": "Vehicle Schedule Data"
            },
            {
                "data_type": "FIELD",
                "name": "Effective_From",
                "value": "Effective From"
            },
            {
                "data_type": "FIELD",
                "name": "Effective_To",
                "value": "Effective To"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tcolumns_order = ['Effective From', \\\n\t                  'Effective To', \\\n\t                'Vehicle Registration', \\\n\t                'Cover - Vehicle', \\\n\t                'Cover - Vehicle - Mapped']\n\t\n\t\n\tif Vehicle_Schedule_Data == \"[]\":\n\t    return [columns_order]\n\ttry:\n\t    \n\t    vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t    vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t    \n\t    # if vehicle_num_and_cover_type_table_df.empty and ((Effective_From not in ['','N/A']) or (Effective_To not in ['','N/A'])):\n\t    #   vehicle_num_and_cover_type_table_df.loc[0] = {'Effective From': Effective_From, 'Effective To': Effective_To}\n\t\n\t\n\t    vehicle_num_and_cover_type_table_df['Effective From'] = str(Effective_From)\n\t    vehicle_num_and_cover_type_table_df['Effective To'] = str(Effective_To)\n\t    \n\t    # print(\"hi\", vehicle_num_and_cover_type_table_df)\n\t\n\t    vehicle_num_and_cover_type_table_df.rename(columns={\n\t        'Cover - Vehicle' : 'Cover - Vehicle',\n\t        'Vehicle Registration Number' : 'Vehicle Registration' \n\t      }, inplace=True)\n\t    \n\t    try:\n\t        Unique__Values__Cover__Basis = json.loads(Unique__Values__Cover__Basis)\n\t    except:\n\t        Unique__Values__Cover__Basis = eval(Unique__Values__Cover__Basis)\n\t\n\t    if Unique__Values__Cover__Basis:\n\t        Cover_Basis_Mapping_dct = json.loads(Cover_Basis_Mapping)\n\t        vehicle_num_and_cover_type_table_df['Cover - Vehicle - Mapped'] = vehicle_num_and_cover_type_table_df['Cover - Vehicle'].replace(Cover_Basis_Mapping_dct)\n\t\n\t    # print(vehicle_num_and_cover_type_table_df, Effective_From, Effective_To)\n\t    \n\t    for col in columns_order:\n\t          if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t\n\t    invalid_values = [\"\", \"n/a\", \"null\", \"none\", \"nan\"]\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t\n\t\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.replace('N/A', '')\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.replace('\"\"', '')\n\t    vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t\n\t    if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t        vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t        return vehicle_num_and_cover_type_table_df\n\t    else:\n\t        return [columns_order]\n\t\n\texcept Exception as e:\n\t    print(\"In exception \", e)\n\t    try:\n\t        vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t        vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t        \n\t        # if vehicle_num_and_cover_type_table_df.empty and ((Effective_From not in ['','N/A']) or (Effective_To not in ['','N/A'])):\n\t        #   vehicle_num_and_cover_type_table_df.loc[0] = {'Effective From': Effective_From, 'Effective To': Effective_To}\n\t\n\t\n\t        try:\n\t            vehicle_num_and_cover_type_table_df.rename(columns={\n\t                'Cover - Vehicle' : 'Cover - Vehicle',\n\t                'Vehicle Registration Number' : 'Vehicle Registration' \n\t              }, inplace=True)\n\t\n\t        except:\n\t            print()\n\t\n\t        for col in columns_order:\n\t          if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t        \n\t        if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t            vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t            vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t            vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t            return vehicle_num_and_cover_type_table_df\n\t        else:\n\t            return [columns_order]\n\t    except:\n\t        return [columns_order]\n\t    return [columns_order]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360085000",
        "name": "form_vehicle_schedule_table",
        "udf_type": "REFINER"
    },
    "17091": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Vehicle_Count_From_Email",
                "value": "Vehicle Count From Email"
            },
            {
                "data_type": "FIELD",
                "name": "Vehicle_Count_From_Table",
                "value": "Vehicle Count From Table"
            }
        ],
        "code": "\t# try:\n\t#     Vehicle_Count_From_Email = int(Vehicle_Count_From_Email)\n\t#     return Vehicle_Count_From_Email\n\t# except:\n\ttry:\n\t    Vehicle_Count_From_Table = int(Vehicle_Count_From_Table)\n\t    return Vehicle_Count_From_Table\n\texcept:\n\t    return 0",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360085000",
        "name": "get_count_of_notifiable_vehicles",
        "udf_type": "REFINER"
    },
    "17092": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Main Cover Type Mapped@0"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360085000",
        "name": "check_cover_type_mapped",
        "udf_type": "REFINER"
    },
    "17093": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Line_1",
                "value": "Party Address"
            },
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t# # Import Python packages\n\t# # import json\n\t\n\t# # Log statements using print()\n\t# # print(\"This will appear in the logs\")\n\t\n\t# # Return the cleaned output\n\t# # print(previous_line)\n\t# # return previous_line\n\t# import json\n\t# key = \"Line_2\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start,len(text)):\n\t#     if text[i] == '{':\n\t#       brace_count += 1\n\t#     elif text[i] == '}':\n\t#       brace_count  -= 1\n\t#       if brace_count == 0:\n\t#         end = i\n\t#         break\n\t#   if end is None:\n\t#     return None\n\t#   return text[start:end+1]\n\t\n\t# if Party_Address_Line_1:\n\t#   json_part = extract_first_json_object(Party_Address_Line_1)\n\t#   if json_part is None:\n\t#     return None\n\t#   try:\n\t#     data = json.loads(json_part)\n\t#   except json.JSONDecodeError:\n\t#     return \"Extracted JSON is invalid.\"\n\t#   if key == \"Line_3\":\n\t#     parts = []\n\t#     line3 = data.get(\"Line_3\")\n\t#     if line3:\n\t#       parts.append(line3)\n\t#     extra = data.get(\"Extra_Lines\",[])\n\t#     if extra and isinstance(extra,list):\n\t#       parts.extend(extra)\n\t#     return ', '.join(parts) if parts else None\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# else:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360085000",
        "name": "get_address_line",
        "udf_type": "REFINER"
    },
    "17094": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 3\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360086000",
        "name": "get_address_line3",
        "udf_type": "REFINER"
    },
    "17095": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Country\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360086000",
        "name": "get_country",
        "udf_type": "REFINER"
    },
    "17096": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"State\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360086000",
        "name": "get_state",
        "udf_type": "REFINER"
    },
    "17097": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"City\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360086000",
        "name": "get_city",
        "udf_type": "REFINER"
    },
    "17098": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t# def get_value(data, key):\n\t#     if isinstance(data, dict):\n\t#         return data.get(key, \"\")\n\t#     return \"\"\n\t\n\t# val = get_value(Party_Address_Details, \"Postcode\")\n\t# return val\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Postcode\")\n\t  return val\n\texcept:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360087000",
        "name": "get_postcode",
        "udf_type": "REFINER"
    },
    "17099": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360087000",
        "name": "get_party_address_line1",
        "udf_type": "REFINER"
    },
    "17100": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Party Address Details@0"
            }
        ],
        "code": "\timport json\n\t\n\ttry:\n\t  previous_line = eval(previous_line)\n\t  # print(type(previous_line))\n\texcept:\n\t  previous_line = json.loads(previous_line)\n\t  # print(type(previous_line))\n\t\n\treturn previous_line",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360087000",
        "name": "clean",
        "udf_type": "REFINER"
    },
    "17101": {
        "args": [],
        "code": "\tdata = [\n\t  {\n\t    \"trade\": \"General Manufacturing\",\n\t    \"aliases\": [\"steel\", \"metal\", \"samples\", \"manufacturing of glass\", \"manufacturing of furniture\", \"manufacturing\", \"general manufacturing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Fleet - Unclassified\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Contractor\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Builders\",\n\t    \"aliases\": [\"Building Contractors\", \"house builders\", \"builders\"]\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers UK only\",\n\t    \"aliases\": [\"haulage contractors - uk only\", \"hauliers uk only\"]\n\t  },\n\t  {\n\t    \"trade\": \"Electrician\",\n\t    \"aliases\": [\"Electrical Contractors\", \"electrical installation\", \"electrical testing\", \"electrician\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plumbing & Heating Engineer\",\n\t    \"aliases\": [\"plumbing & heating contractors\", \"gas servicing\", \"boiler maintenance\", \"plumber\", \"heating engineer\"]\n\t  },\n\t  {\n\t    \"trade\": \"Engineering\",\n\t    \"aliases\": [ \"engineers\", \"mechanical engineers\", \"engineering\", \"electrical engineers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Civil engineering\",\n\t    \"aliases\": [\"civil engineering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Scaffolding Contractors\",\n\t    \"aliases\": [\"scaffolders\", \"scaffolding contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Wholesale (non food and drink)\",\n\t    \"aliases\": [\"wholesalers of electrical components\", \"building materials\", \"wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Cleaning Contractors\",\n\t    \"aliases\": [\"domestic cleaners\", \"office cleaners\", \"cleaning contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plant Hire\",\n\t    \"aliases\": [\"plant hire operator\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Wholesale\",\n\t    \"aliases\": [\"cash & carry\", \"beer & wine wholesalers\", \"food wholesalers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Retail (non food and drink)\",\n\t    \"aliases\": [\"department store\", \"shopping centre\"]\n\t  },\n\t  {\n\t    \"trade\": \"Property Owners\",\n\t    \"aliases\": [\"landlords\", \"commercial property owners\"]\n\t  },\n\t  {\n\t    \"trade\": \"Telecommunications & IT\",\n\t    \"aliases\": [\"telecommunications installation\", \"it installation\", \"it services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Double Glazing\",\n\t    \"aliases\": [\"double glazing manufacture\", \"double glazing installation\"]\n\t  },\n\t  {\n\t    \"trade\": \"Landscape Gardener\",\n\t    \"aliases\": [\"gardening\", \"landscape gardener\"]\n\t  },\n\t  {\n\t    \"trade\": \"Other Prof/Sci/Tech\",\n\t    \"aliases\": [\"laboratory\"]\n\t  },\n\t  {\n\t    \"trade\": \"Removal Contractor\",\n\t    \"aliases\": [\"removals and storage\"]\n\t  },\n\t  {\n\t    \"trade\": \"Security and investigation\",\n\t    \"aliases\": [\"private detectives\", \"security guarding\", \"security services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Builders Merchant\",\n\t    \"aliases\": [\"building supplies\", \"suppliers of aggregates\"]\n\t  },\n\t  {\n\t    \"trade\": \"Service Engineers\",\n\t    \"aliases\": [\"mechanical servicing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Asphalters/Pavers/Engineers\",\n\t    \"aliases\": [\"road maintenance\", \"road surfacing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Flooring and Carpet\",\n\t    \"aliases\": [\"carpet fitters\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure Industry\",\n\t    \"aliases\": [\"nightclub\", \"pub\", \"leisure centre\", \"gym\"]\n\t  },\n\t  {\n\t    \"trade\": \"Social Religious or Charitable\",\n\t    \"aliases\": [\"charity\"]\n\t  },\n\t  {\n\t    \"trade\": \"Business Services\",\n\t    \"aliases\": [\"document storage\", \"administration\", \"consultants\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Services\",\n\t    \"aliases\": [\"food delivery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Retail\",\n\t    \"aliases\": [\"restaurant\", \"pub\", \"takeaway\"]\n\t  },\n\t  {\n\t    \"trade\": \"Estate Agent\",\n\t    \"aliases\": [\"lettings agents\", \"estate agent\"]\n\t  },\n\t  {\n\t    \"trade\": \"Furniture Sale and Manufacture\",\n\t    \"aliases\": [\"furniture retail\", \"furniture showroom\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure (Hotel, clubs & pubs)\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Shop Fitting\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"NHS Trust\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Import/Export\",\n\t    \"aliases\": [\"Import\", \"Export\"]\n\t  },\n\t  {\n\t    \"trade\": \"Catering\",\n\t    \"aliases\": [\"licensed catering\", \"unlicensed catering\", \"outside catering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Residential Care\",\n\t    \"aliases\": [\"care homes\", \"retirement homes\"]\n\t  },\n\t  {\n\t    \"trade\": \"Manufacturing Timber/Furniture\",\n\t    \"aliases\": [\"joinery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Printers and publishers\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Demolition Contractors\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Housing Association\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Timber Merchant\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Private Ambulance Service\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers Overseas\",\n\t    \"aliases\": [\"haulage contractors - overseas\", \"hauliers overseas\"]\n\t  },\n\t  {\n\t    \"trade\": \"Textiles & Clothing\",\n\t    \"aliases\": [\"clothing manufacturing\", \"textile manufacturing\", \"clothing retail\"]\n\t  },\n\t  {\n\t    \"trade\": \"Automotive Industry\",\n\t    \"aliases\": [\"vehicle manufacturing\", \"vehicle repairs\", \"vehicle parts manufacturing\", \"vehicle parts wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Pharmaceutical\",\n\t    \"aliases\": [\"pharmacy\", \"medical laboratories\"]\n\t  },\n\t  {\n\t    \"trade\": \"Farmer\",\n\t    \"aliases\": [\"dairy farmers\", \"arable farmers\", \"livestock farmers\"]\n\t  }\n\t]\n\t\n\treturn data\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360087000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17102": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "AXA Trade Description@0"
            },
            {
                "data_type": "FIELD",
                "name": "Business_Description",
                "value": "Business Description"
            }
        ],
        "code": "\tif Business_Description:\n\t  if Business_Description == \"\":\n\t      return \"\"\n\t  else:\n\t      return previous_line\n\telse:\n\t    return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360088000",
        "name": "clean",
        "udf_type": "REFINER"
    },
    "17103": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "AXA Trade Description@1"
            },
            {
                "data_type": "FIELD",
                "name": "Trade_Descriptions",
                "value": "Trade Descriptions"
            }
        ],
        "code": "\t# Import Python packages\n\timport json\n\t\n\t\n\t\n\t# try:\n\t#   Trade_Descriptions = eval(Trade_Descriptions)\n\t# except:\n\t#   try:\n\t#       Trade_Descriptions = json.loads(Trade_Descriptions)\n\t#   except:\n\t#       # return \"Fleet - Unclassified\"\n\t#       return \"Other\"\n\t\n\t# # Trade_Descriptions_ls = [item.lower() for item in Trade_Descriptions]\n\t\n\t# Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t\n\t# if previous_line.lower() in Trade_Descriptions_ls:\n\t#   return previous_line\n\t# else:\n\t#   return \"Other\"\n\t\n\tprint(repr(previous_line))\n\tif previous_line == '\"\"' or previous_line == '' or previous_line == \"\":\n\t    return \"\"\n\t\n\tif previous_line:\n\t  try:\n\t      Trade_Descriptions = eval(Trade_Descriptions)\n\t  except:\n\t      try:\n\t          Trade_Descriptions = json.loads(Trade_Descriptions)\n\t      except:\n\t          # return \"Fleet - Unclassified\"\n\t          return \"Fleet - Unclassified\"\n\t\n\t  Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t    \n\t  if previous_line.lower() in Trade_Descriptions_ls:\n\t    return previous_line\n\t  else:\n\t    return \"Fleet - Unclassified\"\n\telse:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360088000",
        "name": "test_wih_exisitng_list",
        "udf_type": "REFINER"
    },
    "17104": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Target Price@1"
            }
        ],
        "code": "\treturn previous_line.replace(\"\u00a3\",\"\").replace(\",\",\"\")",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360088000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17105": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Broker Deadline@1"
            }
        ],
        "code": "\tfrom datetime import datetime, timedelta\n\t\n\tdef add_calendar_days(start_date, days):\n\t    \"\"\"Add calendar days (including weekends) to a given date.\"\"\"\n\t    return start_date + timedelta(days=days)\n\t\n\tdef add_weekdays(start_date, days):\n\t    \"\"\"Add weekdays to a given date, skipping weekends.\"\"\"\n\t    while days > 0:\n\t        start_date += timedelta(days=1)\n\t        if start_date.weekday() < 5:  # Weekdays: Mon-Fri = 0-4\n\t            days -= 1\n\t    return start_date\n\t\n\tdef process_date(input_str):\n\t    today = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)\n\t\n\t    # Try to parse dd/mm/yyyy\n\t    try:\n\t        parsed_date = datetime.strptime(input_str, \"%d/%m/%Y\")\n\t    except ValueError:\n\t        # If year not present, try dd/mm check\n\t        try:\n\t            try:\n\t                parsed_date = datetime.strptime(input_str, \"%d/%m\")\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t            except:\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t        except ValueError:\n\t            final_date = add_calendar_days(today, 5)\n\t            return final_date.strftime(\"%d/%m/%Y\")\n\t            \n\t    # Compare dates\n\t    base_date = parsed_date if parsed_date >= today else today\n\t\n\t    return base_date.strftime(\"%d/%m/%Y\")\n\t\n\tresult = process_date(previous_line) \n\treturn result",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360088000",
        "name": "business_logic_for_broker_deadline",
        "udf_type": "REFINER"
    },
    "17106": {
        "args": [],
        "code": "\treturn \"Fleet\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360089000",
        "name": "get_product_name",
        "udf_type": "REFINER"
    },
    "17107": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Incepts_On",
                "value": "Incepts On"
            }
        ],
        "code": "\tfrom datetime import datetime, timedelta\n\t\n\tdef add_one_year_minus_one_day(date_str):\n\t    # Parse the input date (DD/MM/YYYY)\n\t    input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t    try:\n\t        # Try to add one year directly\n\t        one_year_later = input_date.replace(year=input_date.year + 1)\n\t    except ValueError:\n\t        # Handle Feb 29 (leap year issue) and other invalid dates\n\t        temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t        one_year_later = temp_date\n\t    # Subtract one day\n\t    final_date = one_year_later - timedelta(days=1)\n\t    # Return formatted date as DD/MM/YYYY with leading zeros\n\t    return final_date.strftime(\"%d/%m/%Y\")\n\t    \n\ttry:\n\t    expiry_date = add_one_year_minus_one_day(Incepts_On)\n\t    return expiry_date\n\texcept Exception as e:\n\t    return \"\"\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360089000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17108": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Main Cover Type@1"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360089000",
        "name": "check_cover_type",
        "udf_type": "REFINER"
    },
    "17109": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Incepts_On",
                "value": "Incepts On"
            }
        ],
        "code": "\tif Incepts_On:\n\t  return Incepts_On\n\telse:\n\t  return ''",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360089000",
        "name": "get_effective_date",
        "udf_type": "REFINER"
    },
    "17110": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Expires_On",
                "value": "Expires On"
            }
        ],
        "code": "\tif Expires_On:\n\t  return Expires_On\n\telse:\n\t  return ''",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360090000",
        "name": "get_expiry_date",
        "udf_type": "REFINER"
    },
    "17111": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Holding Broker@0"
            }
        ],
        "code": "\timport json\n\t\n\tdef extract_holding_broker_from_text(text: str) -> str:\n\t    brace_stack = []\n\t    start_idx = -1\n\t    end_idx = -1\n\t\n\t    # Step 1: Find the first complete {...} block\n\t    for i, ch in enumerate(text):\n\t        if ch == '{':\n\t            if not brace_stack:\n\t                start_idx = i\n\t            brace_stack.append('{')\n\t        elif ch == '}':\n\t            if brace_stack:\n\t                brace_stack.pop()\n\t                if not brace_stack:\n\t                    end_idx = i + 1  # Include the closing brace\n\t                    break\n\t\n\t    # Step 2: If we found a full JSON block\n\t    if start_idx != -1 and end_idx != -1:\n\t        json_str = text[start_idx:end_idx]\n\t\n\t        try:\n\t            data = json.loads(json_str)\n\t            holding_broker = data.get('holding_broker')  \n\t            return holding_broker\n\t        except:\n\t            return ''\n\t\n\t    return ''\n\t\n\treturn extract_holding_broker_from_text(previous_line)\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360090000",
        "name": "return_holding_broker",
        "udf_type": "REFINER"
    },
    "17112": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Is Holding Broker@0"
            },
            {
                "data_type": "FIELD",
                "name": "Holding_Broker",
                "value": "Holding Broker"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t\n\tprint(Holding_Broker)\n\tif Holding_Broker =='':\n\t  return ''\n\t# return previous_line\n\t\n\t\n\t\n\timport json\n\t\n\tdef extract_holding_broker_from_text(text: str) -> str:\n\t    brace_stack = []\n\t    start_idx = -1\n\t    end_idx = -1\n\t\n\t    # Step 1: Find the first complete {...} block\n\t    for i, ch in enumerate(text):\n\t        if ch == '{':\n\t            if not brace_stack:\n\t                start_idx = i\n\t            brace_stack.append('{')\n\t        elif ch == '}':\n\t            if brace_stack:\n\t                brace_stack.pop()\n\t                if not brace_stack:\n\t                    end_idx = i + 1  # Include the closing brace\n\t                    break\n\t\n\t    # Step 2: If we found a full JSON block\n\t    if start_idx != -1 and end_idx != -1:\n\t        json_str = text[start_idx:end_idx]\n\t\n\t        try:\n\t            data = json.loads(json_str)\n\t            holding_broker = data.get('holding_broker')  \n\t            print(holding_broker)\n\t            return holding_broker\n\t        except:\n\t            return ''\n\t\n\t    return ''\n\t\n\treturn extract_holding_broker_from_text(previous_line)\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360090000",
        "name": "check_return_holding_broker_value",
        "udf_type": "REFINER"
    },
    "17113": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Driver Details@0"
            }
        ],
        "code": "\timport pandas as pd\n\timport json, re, ast\n\timport traceback\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = [\n\t         \"Driver Name\",\n\t         \"Driver DOB\",\n\t         \"Licence Date\",\n\t         \"Conviction Code\",\n\t          \"Driver Claims\"\n\t    ]\n\t\n\t\n\tdef get_df_with_regex_match(previous_line, columns_order):\n\t    parsed, inner_content = None, None\n\t    df = pd.DataFrame(columns=columns_order)\n\t    try:\n\t        match = re.search(r\"```(?:\\w+)?\\s*([\\s\\S]*?)\\s*```\", previous_line)\n\t        if match:\n\t            inner_content = match.group(1).strip()\n\t    \n\t            # Step 2: Try to parse it as JSON\n\t            try:\n\t                parsed = json.loads(inner_content)\n\t                print(parsed)\n\t            except json.JSONDecodeError:\n\t                # If JSON parsing fails, fallback to literal_eval\n\t                try:\n\t                    parsed = ast.literal_eval(inner_content)\n\t                except Exception as e:\n\t                    print(\"Parsing failed:\", e)\n\t                    parsed = None\n\t          \n\t        if parsed:\n\t            df = convert_to_dataframe(parsed)\n\t            return True, df \n\t        else:\n\t            return False, df \n\t    except Exception as e:\n\t      return False, pd.DataFrame(columns=columns_order)\n\t\n\tdef parse_markdown_format(previous_line, columns_order):\n\t    try:\n\t        lines = [line.strip() for line in previous_line.strip().split('\\n') if line.strip().startswith('|') and '---' not in line]\n\t        list_of_lists = [ [cell.strip() for cell in line.strip('|').split('|')] for line in lines ]\n\t        if list_of_lists:\n\t            df = convert_to_dataframe(list_of_lists)\n\t            return True, df\n\t        else:\n\t            return False, pd.DataFrame(columns=columns_order)\n\t    except:\n\t        return False, pd.DataFrame(columns=columns_order)\n\t\n\ttry:\n\t    \n\t    df = pd.DataFrame(columns=columns_order)\n\t    \n\t    flg, df = get_df_with_regex_match(previous_line, columns_order)\n\t    \n\t    if not flg:\n\t        flg, df = parse_markdown_format(previous_line, columns_order)\n\t\n\t    if not flg:\n\t      try:\n\t          data = json.loads(previous_line)\n\t          df = convert_to_dataframe(data)\n\t      except Exception as e:\n\t          data = ast.literal_eval(previous_line)\n\t          df = convert_to_dataframe(data)\n\t\n\t    \n\t    df.fillna(\"\", inplace=True)\n\t    \n\t    if len(df.columns.tolist()) > 0:\n\t        # df[\"Conviction Code\"] = df[\"Conviction Code\"].replace([\"No\", \"no\"], \"\")\n\t        df[\"Conviction Code\"] = df[\"Conviction Code\"].astype(str).str.strip().replace([\"no\", \"No\"], \"\")\n\t        df[\"Licence Date\"] = df[\"Licence Date\"].apply(\n\t    lambda x: f\"01/01/{x.strip()}\" if re.fullmatch(r\"\\s*(19|20)\\d{2}\\s*\", str(x)) else x)\n\t        df.replace(to_replace=[\"N/A\", \"null\"], value=\"\", inplace=True)\n\t        df = df[columns_order]\n\t        df = [df.columns.tolist()] + df.values.tolist()        \n\t        return df\n\t    else:\n\t        return  [columns_order]\n\t      \n\t      \n\texcept Exception as e:\n\t    return [columns_order]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360090000",
        "name": "get_cleaned_df",
        "udf_type": "REFINER"
    },
    "17114": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Driver Age@0"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Age']\n\t\n\ttry:\n\t  table = json.loads(previous_line)\n\t  df = convert_to_dataframe(table)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360091000",
        "name": "refine_result",
        "udf_type": "REFINER"
    },
    "17115": {
        "args": [],
        "code": "\tfrom datetime import date\n\ttoday = str(date.today())\n\tprint(today)\n\treturn today",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360091000",
        "name": "get_todays_date",
        "udf_type": "REFINER"
    },
    "17116": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Driver Licence Tenure@0"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Licence Tenure']\n\t\n\ttry:\n\t  table = json.loads(previous_line)\n\t  df = convert_to_dataframe(table)\n\t  try:\n\t    df['Licence Tenure'] = df['Licence Tenure'].apply(lambda x: '5+' if x > 5 else x)\n\t  except:\n\t    df['Licence Tenure'] = df['Licence Tenure'].apply(lambda x: '5+' if str(x).replace('.', '', 1).isdigit() and float(x) > 5 else x)\n\t  print(df)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360091000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17117": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Vehicle Registration Numbers And Cover Basis@0"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = ['Vehicle Registration', 'Cover Basis']\n\ttry:\n\t  previous_line = json.loads(previous_line)\n\t  if previous_line:\n\t      df = convert_to_dataframe(previous_line)\n\t      if len(df.columns.tolist()) > 0:\n\t          return [df.columns.tolist()] + df.values.tolist()\n\t      else:\n\t        return [columns_order]\n\t  else:\n\t      return [columns_order]\n\texcept:\n\t  return [columns_order]\n\t  ",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360091000",
        "name": "clean_llm_result",
        "udf_type": "REFINER"
    },
    "17118": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Driver_Age",
                "value": "Driver Age"
            },
            {
                "data_type": "FIELD",
                "name": "Driver_Licence_Tenure",
                "value": "Driver Licence Tenure"
            },
            {
                "data_type": "FIELD",
                "name": "Driver_Details",
                "value": "Driver Details"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\ttry:\n\t  print(\"yes\", Driver_Licence_Tenure)\n\t  merged_df = []\n\t\n\t  try:\n\t      Driver_Details = json.loads(Driver_Details)\n\t      Driver_Details = convert_to_dataframe(Driver_Details)\n\t      merged_df = Driver_Details.copy()\n\t  except Exception as e:\n\t      \n\t      print(\"no driver details \", e)\n\t\n\t  # try:\n\t  #     Conviction_Code = json.loads(Conviction_Code)\n\t  #     Conviction_Code = convert_to_dataframe(Conviction_Code)\n\t  #     print(Conviction_Code)\n\t  # except Exception as e:\n\t  #     print(\"no driver details \", e)\n\t\n\t  try:\n\t      Driver_Age = json.loads(Driver_Age)\n\t      Driver_Age = convert_to_dataframe(Driver_Age)\n\t  except Exception as e:\n\t      print(\"no driver conviction details \", e)\n\t\n\t  try:\n\t      Driver_Licence_Tenure = json.loads(Driver_Licence_Tenure)\n\t      Driver_Licence_Tenure = convert_to_dataframe(Driver_Licence_Tenure)\n\t     \n\t  except Exception as e:\n\t      print(\"no driver licence details \", e)\n\t\n\t  try:\n\t      merged_df = pd.merge(Driver_Details, Driver_Age,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      merged_df = Driver_Details.copy()\n\t      print(\"merging issue  \", e)\n\t\n\t  try:\n\t      \n\t      merged_df = pd.merge(merged_df, Driver_Licence_Tenure,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(\"merging issue  \", e)\n\t\n\t  # try:\n\t  #     # print(\"before\", merged_df.columns)\n\t  #     if Conviction_Code.empty:\n\t  #       print(\"yes\")\n\t  #     else:\n\t  #       merged_df.drop(columns=['Conviction Code'], inplace=True)\n\t  #       merged_df = pd.merge(merged_df, Conviction_Code, on='Driver Name', how='outer')\n\t  #     # print(\"after\", merged_df.columns)\n\t  # except Exception as e:\n\t\n\t      # merged_df_all.fillna('', inplace=True)\n\t      # merged_Df = [merged_df_all.columns.tolist()] + merged_df_all.values.tolist()\n\t      print(\"merging issue in conviction details\", e)\n\t      # return merged_f\n\t      \n\t\n\t  merged_df.fillna('', inplace=True)\n\t\n\t  if len(merged_df.columns.tolist()) > 0:\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [[\"Driver Name\",\t\"Driver DOB\",\t\"Licence Date\",\t\"Age\",\t\"Licence Tenure\",\t\"Conviction Code\"]]\n\t\n\texcept:\n\t  return [[\"Driver Name\",\t\"Driver DOB\",\t\"Licence Date\",\t\"Age\",\t\"Licence Tenure\",\t\"Conviction Code\"]]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360091000",
        "name": "merge_driver_age_licence_tenure",
        "udf_type": "REFINER"
    },
    "17119": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Driver_Merged_Data",
                "value": "Driver Merged Data"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\t\n\tdef classify_driver(row):\n\t    try:\n\t        age = int(row[\"Age\"])\n\t        conviction_code = str(row[\"Conviction Code\"]).strip()\n\t        \n\t        if age < 25:\n\t            return \"Young Driver\"\n\t        elif age >= 25 and str(conviction_code).strip() not in [\"\", None,\"N/A\",\"null\"]:\n\t            return \"Convicted Driver\"\n\t        else:\n\t            return \"\"\n\t    except:\n\t        return \"\"  # Fallback in case of data issues\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Driver Type']\n\t\n\ttry:\n\t  table = json.loads(Driver_Merged_Data)\n\t  df = convert_to_dataframe(table)\n\t  df[\"Driver Type\"] = df.apply(classify_driver, axis=1)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360092000",
        "name": "get_driver_type",
        "udf_type": "REFINER"
    },
    "17120": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Vehicle_Registration_Numbers_And_Cover_Basis",
                "value": "Vehicle Registration Numbers And Cover Basis"
            }
        ],
        "code": "\t# # Import Python packages\n\t# import json\n\t\n\t# list_of_dct = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t# unique_cover_basis_set = set(item['Cover Basis'] for item in list_of_dct)\n\t# unique_cover_basis_list = list(unique_cover_basis_set)\n\t# unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t# return unique_cover_basis_list_without_na\n\t\n\t# # Import Python packages\n\timport json\n\timport pandas as pd\n\t\n\t# list_of_dct = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t# # print(list_of_dct)\n\t\n\t# try:\n\t#     unique_cover_basis_set = set(item['Cover Vehicle'] for item in list_of_dct)\n\t#     unique_cover_basis_list = list(unique_cover_basis_set)\n\t#     unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t#     return unique_cover_basis_list_without_na\n\t\n\t# except Exception as e:\n\t#     print(\"in ex\", e)\n\t#     # print(type(list_of_dct))\n\t#     unique_first_col = list(set(row[0] for row in list_of_dct))\n\t#     if 'Cover Vehicle' in unique_first_col:\n\t#         unique_first_col.remove('Cover Vehicle')\n\t\n\t#     return unique_first_col\n\t    \n\t\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tVehicle_Registration_Numbers_And_Cover_Basis = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t\n\tif Vehicle_Registration_Numbers_And_Cover_Basis in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tVehicle_Registration_Numbers_And_Cover_Basis = convert_to_dataframe(Vehicle_Registration_Numbers_And_Cover_Basis)\n\tunique_vals = Vehicle_Registration_Numbers_And_Cover_Basis['Cover Basis'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360092000",
        "name": "get_unique_values_from_cover_basis_col",
        "udf_type": "REFINER"
    },
    "17121": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Cover Basis Mapping@0"
            }
        ],
        "code": "\timport json\n\t\n\ttry:\n\t    if previous_line not in [\"\", None, \"[]\", \" \", \"N/A\", \"null\"]:\n\t        result = previous_line\n\t    else:\n\t        result = \"{}\"\n\t\n\texcept Exception as e:\n\t    print(e)\n\t    result = \"{}\"\n\t\n\treturn result  # Ensure this is inside a function\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360092000",
        "name": "refine_result",
        "udf_type": "REFINER"
    },
    "17122": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Vehicle_Registration_Numbers_And_Cover_Basis",
                "value": "Vehicle Registration Numbers And Cover Basis"
            },
            {
                "data_type": "FIELD",
                "name": "Unique__Values__Cover__Basis",
                "value": "Unique_Values_Cover_Basis"
            },
            {
                "data_type": "FIELD",
                "name": "Cover_Basis_Mapping",
                "value": "Cover Basis Mapping"
            },
            {
                "data_type": "FIELD",
                "name": "Effective_From",
                "value": "Effective From"
            },
            {
                "data_type": "FIELD",
                "name": "Effective_To",
                "value": "Effective To"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tcolumns_order = ['Effective From', \\\n\t                    'Effective To', \\\n\t                    'Vehicle Registration', \\\n\t                    'Cover - Vehicle', \\\n\t                    'Cover - Vehicle - Mapped']\n\t\n\tif Vehicle_Registration_Numbers_And_Cover_Basis == \"[]\":\n\t  return [columns_order]\n\t\n\ttry:\n\t    vehicle_num_and_cover_type_table = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t    vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t\n\t\n\t    vehicle_num_and_cover_type_table_df.rename(columns={\n\t        'Cover Basis' : 'Cover - Vehicle',\n\t        'Vehicle Registration Number' : 'Vehicle Registration' \n\t      }, inplace=True)\n\t\n\t    try:\n\t        Unique__Values__Cover__Basis = json.loads(Unique__Values__Cover__Basis)\n\t    except:\n\t        Unique__Values__Cover__Basis = eval(Unique__Values__Cover__Basis)\n\t\n\t\n\t    if Unique__Values__Cover__Basis:\n\t      try:\n\t        Cover_Basis_Mapping_dct = json.loads(Cover_Basis_Mapping)\n\t        vehicle_num_and_cover_type_table_df['Cover - Vehicle - Mapped'] = vehicle_num_and_cover_type_table_df['Cover - Vehicle'].replace(Cover_Basis_Mapping_dct)\n\t      except:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.copy()\n\t\n\t    vehicle_num_and_cover_type_table_df['Effective From'] = Effective_From\n\t    try:\n\t      vehicle_num_and_cover_type_table_df['Effective To'] = Effective_To\n\t    except:\n\t      vehicle_num_and_cover_type_table_df['Effective To'] = ''\n\t\n\t    for col in columns_order:\n\t        if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t    \n\t    invalid_values = [\"\", \"n/a\", \"null\", \"none\", \"nan\"]\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t\n\t    # if vehicle_num_and_cover_type_table_df.empty and ((Effective_From not in ['','N/A']) or (Effective_To not in ['','N/A'])):\n\t    #   vehicle_num_and_cover_type_table_df.loc[0] = {'Effective From': Effective_From, 'Effective To': Effective_To}\n\t\n\t    vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t    \n\t    if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t        vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t        return vehicle_num_and_cover_type_table_df\n\t    else:\n\t        return [columns_order]\n\texcept:\n\t    return [columns_order]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360092000",
        "name": "prepare_vehicle_schedule_table",
        "udf_type": "REFINER"
    },
    "17123": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Vehicle_Schedule_Table",
                "value": "Vehicle Schedule Table"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame( data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\ttry:\n\t  if Vehicle_Schedule_Table == \"[]\" or Vehicle_Schedule_Table == 'n/a':\n\t    return \"0\"\n\t  else:\n\t    Vehicle_Schedule_Table = json.loads(Vehicle_Schedule_Table)\n\t    Vehicle_Schedule_Table = convert_to_dataframe(Vehicle_Schedule_Table)\n\t    return len(Vehicle_Schedule_Table['Vehicle Registration'])\n\texcept:\n\t  return \"0\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360093000",
        "name": "get_number_of_notifiable_vehicles_count",
        "udf_type": "REFINER"
    },
    "17124": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Target_Price",
                "value": "Target Price"
            },
            {
                "data_type": "FIELD",
                "name": "Number_of_Notifiable_Vehicles",
                "value": "Number of Notifiable Vehicles"
            }
        ],
        "code": "\t# def clean_premium(premium_value):\n\t\n\t#     if premium_value is None:\n\t#         return None\n\t\n\t#     if isinstance(premium_value, (int, float)):\n\t#         return float(premium_value)\n\t\n\t#     try:\n\t#         # Remove currency symbols, commas, and whitespace\n\t#         cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t#         return float(cleaned)\n\t#     except Exception:\n\t#         return None\n\t\n\t# def determine_offering_type(vehicle_count, premium_cleaned=None):\n\t\n\t#     if vehicle_count is None:\n\t#         return \"Unknown\"\n\t#     if vehicle_count <= 19:\n\t#         if premium_cleaned is None or premium_cleaned < 10000:\n\t#             return \"Mini Fleet\"\n\t#     elif 20 <= vehicle_count <= 149:\n\t#         if premium_cleaned is None or 10000 <= premium_cleaned < 250000:\n\t#             return \"Vantage Fleet\"\n\t#     elif vehicle_count >= 150:\n\t#         if premium_cleaned is None or premium_cleaned >= 250000:\n\t#             return \"Mid Corp\"\n\t\n\t#     return \"Unknown\"\n\t\n\t# try:\n\t#   Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\t# except:\n\t#   return \"\"\n\t\n\t# try:\n\t#   premium_cleaned = clean_premium(Target_Price)\n\t#   print(premium_cleaned)\n\t# except:\n\t#   premium_cleaned = None\n\t\n\t# offering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\t# # print(offering_type)\n\t# return offering_type\n\t\n\t\n\t\n\tdef clean_premium(premium_value):\n\t    if premium_value is None:\n\t        return None\n\t    if isinstance(premium_value, (int, float)):\n\t        return float(premium_value)\n\t    try:\n\t        # Remove currency symbols, commas, and whitespace\n\t        # print(premium_value)\n\t        # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t        cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t        print(cleaned)\n\t        return float(cleaned)\n\t    except Exception:\n\t        return None\n\t\n\tdef determine_offering_type(vehicle_count, premium_cleaned=None):    \n\t\n\t    if vehicle_count == 0:\n\t        if premium_cleaned is None or premium_cleaned < 10000:\n\t            return \"Mini Fleet\"\n\t        \n\t        elif 10000 <= premium_cleaned < 250000:\n\t            return \"Vantage Fleet\"\n\t        \n\t        elif premium_cleaned >= 250000:\n\t            return \"Mid Corp\"\n\t\n\t    if vehicle_count <= 19:\n\t            return \"Mini Fleet\"\n\t            \n\t    elif 20 <= vehicle_count <= 149:\n\t            return \"Vantage Fleet\"\n\t        \n\t    elif vehicle_count >= 150:\n\t            return \"Mid Corp\"\n\t\n\t\n\ttry:\n\t    Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\texcept:\n\t    Number_of_Notifiable_Vehicles = 0\n\t\n\ttry:\n\t    premium_cleaned = clean_premium(Target_Price)\n\texcept:\n\t    premium_cleaned = None\n\t\n\toffering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\treturn offering_type\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360093000",
        "name": "get_offering_type",
        "udf_type": "REFINER"
    },
    "17125": {
        "args": [],
        "code": "\treturn \"New Business\"\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360093000",
        "name": "get_transaction_type",
        "udf_type": "REFINER"
    },
    "17126": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Driver_Merged_Data",
                "value": "Driver Merged Data"
            },
            {
                "data_type": "FIELD",
                "name": "Driver_Type",
                "value": "Driver Type"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\tcolumns_order = [ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']\n\t\n\ttry:\n\t  Driver_Data_Merged = json.loads(Driver_Merged_Data)\n\t  Drive_Type = json.loads(Driver_Type)\n\t\n\t  merged_df = []\n\t\n\t  try:\n\t      Driver_Data_Merged = convert_to_dataframe(Driver_Data_Merged)\n\t  except Exception as e:\n\t      print(e)\n\t  try:\n\t      Drive_Type = convert_to_dataframe(Drive_Type)\n\t  except Exception as e:\n\t      merged_df = Driver_Data_Merged.copy()\n\t      print(e)\n\t\n\t  # print(Driver_Data_Merged.columns)\n\t\n\t  try:\n\t      merged_df = pd.merge(Driver_Data_Merged, Drive_Type,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(e)\n\t  \n\t  try:\n\t      \n\t      rename_dict = {\n\t        'Driver Name': 'Driver Name',\n\t        'Driver DOB' : 'Driver D.O.B',\n\t        'Licence Date': 'Drivers Licence - Date obtained',\n\t        'Licence Tenure':'Driver: Years Appropriate Licence Held',\n\t        'Conviction Code': 'Conviction Details',\n\t        'Driver Type': 'Driver Type'\n\t      }\n\t\n\t      safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t      merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t      \n\t      columns_order = [ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']\n\t\n\t      for col in columns_order:\n\t          if col not in merged_df.columns:\n\t              merged_df[col] = ''\n\t     \n\t      existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t                  \n\t      merged_df.replace('N/A', '', inplace=True)\n\t      merged_df.replace('null', '', inplace=True)\n\t      merged_df = merged_df.fillna('')\n\t      merged_df = merged_df[existing_columns]\n\t  \n\t  except Exception as e:\n\t      print(\"Exception in renaming \", e)\n\t      if len(merged_df.columns.tolist()) > 0:\n\t          merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t          return merged_df\n\t      else:\n\t          return [columns_order]\n\t  \n\t  if len(merged_df.columns.tolist()) > 0:\n\t\n\t      merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t      merged_df = merged_df.dropna(how='all')\n\t      merged_df = merged_df.where(pd.notna(merged_df), \"\")\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept:\n\t  return [columns_order]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360093000",
        "name": "form_driver_party_table",
        "udf_type": "REFINER"
    },
    "17127": {
        "args": [],
        "code": "\treturn \"Mid Market\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360094000",
        "name": "get_business_category",
        "udf_type": "REFINER"
    },
    "17128": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Driver_Details",
                "value": "Driver Details"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\t\n\tcolumns = ['Driver Name', 'Driver DOB']\n\ttry:\n\t  table = json.loads(Driver_Details)\n\t  df = convert_to_dataframe(table)\n\t  return [columns] + df[columns].values.tolist()\n\texcept:\n\t  return Driver_Details",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360094000",
        "name": "get_driver_name_and_dob",
        "udf_type": "REFINER"
    },
    "17129": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Driver_Details",
                "value": "Driver Details"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\t\n\tcolumns = ['Driver Name', 'Licence Date']\n\ttry:\n\t  table = json.loads(Driver_Details)\n\t  df = convert_to_dataframe(table)\n\t  return [columns] + df[columns].values.tolist()\n\texcept:\n\t  return Driver_Details",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360094000",
        "name": "get_licence_tenure_and_driver_name",
        "udf_type": "REFINER"
    },
    "17130": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Country\")\n\t  return val\n\texcept:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360094000",
        "name": "get_country",
        "udf_type": "REFINER"
    },
    "17131": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"City\")\n\t  return val\n\texcept:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360094000",
        "name": "get_city",
        "udf_type": "REFINER"
    },
    "17132": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"State\")\n\t  return val\n\texcept:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360095000",
        "name": "get_state",
        "udf_type": "REFINER"
    },
    "17133": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Main Cover Type Mapped@0"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360095000",
        "name": "check_cover_type_mapped",
        "udf_type": "REFINER"
    },
    "17134": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360095000",
        "name": "get_address_line2",
        "udf_type": "REFINER"
    },
    "17135": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Address Line 3\")\n\t  return val\n\texcept:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360096000",
        "name": "get_address_line3",
        "udf_type": "REFINER"
    },
    "17136": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Postcode\")\n\t  return val\n\texcept:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360096000",
        "name": "get_postcode",
        "udf_type": "REFINER"
    },
    "17137": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360096000",
        "name": "get_address_line1",
        "udf_type": "REFINER"
    },
    "17138": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Party Address Details@0"
            }
        ],
        "code": "\timport json\n\t\n\ttry:\n\t  previous_line = eval(previous_line)\n\t  # print(type(previous_line))\n\texcept:\n\t  previous_line = json.loads(previous_line)\n\t  # print(type(previous_line))\n\t\n\treturn previous_line",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360096000",
        "name": "clean",
        "udf_type": "REFINER"
    },
    "17139": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_List",
                "value": "Excess List"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\t\n\ttable = json.loads(Excess_List)\n\t\n\tif Excess_List in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tExcess_List = convert_to_dataframe(table)\n\tunique_vals = Excess_List['Cover On Policy'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360096000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17140": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Policy Dates@0"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\tfrom datetime import datetime, timedelta\n\t\n\t# Custom function\n\tdef add_one_year_minus_one_day(date_str):\n\t    try:\n\t        input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t        try:\n\t            one_year_later = input_date.replace(year=input_date.year + 1)\n\t        except ValueError:\n\t            temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t            one_year_later = temp_date\n\t        final_date = one_year_later - timedelta(days=1)\n\t        return final_date.strftime(\"%d/%m/%Y\")\n\t    except Exception as e:\n\t        print(e)\n\t        return ''\n\t\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\t\n\ttry:\n\t    previous_line = eval(previous_line)\n\t    dates_df = convert_to_df(previous_line)\n\texcept:\n\t  try:\n\t      previous_line = json.loads(previous_line)\n\t      dates_df = convert_to_df(previous_line)\n\t  except:\n\t      return previous_line\n\t\n\ttry:\n\t\n\t    # Iterate row-wise\n\t    for index, row in dates_df.iterrows():\n\t        start_date = row['Policy Period Start Date']\n\t        \n\t        # Only update Policy Period End Date if Used Inception Date is 'Yes'\n\t        if str(row.get('Used Inception Date', '')).strip().lower() == 'yes':\n\t            dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t            print(\"Applied for \", row[\"Period\"])\n\t\n\t    # Clean up Period column after the loop\n\t    dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    # Convert to list format for output\n\t    final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    return final_df\n\t\n\t    # print(dates_df.columns)\n\t    # Iterate row-wise\n\t    # for index, row in dates_df.iterrows():\n\t    #     start_date = row['Policy Period Start Date']\n\t    #     dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t    #     dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    #     final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    # return final_df\n\t\n\texcept:\n\t    return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360097000",
        "name": "business_logic_for_policy_period_end_date",
        "udf_type": "REFINER"
    },
    "17141": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Policy_Dates",
                "value": "Policy Dates"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t    try:\n\t        Policy_Dates = eval(Policy_Dates)    \n\t    except:\n\t        Policy_Dates = json.loads(Policy_Dates)   \n\t\n\t    Policy_Dates = convert_to_df(Policy_Dates)\n\t    Policy_Dates = Policy_Dates.astype(str)\n\t    print(Policy_Dates.columns)\n\t    CCE_Policy_start_dates_List = Policy_Dates[['Policy Period Start Date']].values.tolist()\n\t    filtered_vals = [each_val for each_val in CCE_Policy_start_dates_List if each_val not in [\"N/A\",\"\",None,\"null\"]]\n\t    if len(filtered_vals)>=1:\n\t      return [[\"Policy Period Start Date\"]] + CCE_Policy_start_dates_List\n\t    else:\n\t      CCE_Period_List = Policy_Dates[['Period']].values.tolist()\n\t      return [\"Policy Period Start Date\"] + [CCE_Period_List]\n\texcept Exception as e:\n\t    print(e)\n\t    return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360097000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17142": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_List",
                "value": "Excess List"
            },
            {
                "data_type": "FIELD",
                "name": "cover_mapping",
                "value": "cover mapping"
            },
            {
                "data_type": "FIELD",
                "name": "CCE_Data",
                "value": "CCE Data"
            },
            {
                "data_type": "FIELD",
                "name": "Policy_Dates",
                "value": "Policy Dates"
            },
            {
                "data_type": "FIELD",
                "name": "Year",
                "value": "Year"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            # print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\ttry:\n\t  try:\n\t      try:\n\t          Excess_List = json.loads(Excess_List)\n\t      except:\n\t          Excess_List = eval(Excess_List)\n\t\n\t      Excess_List = convert_to_dataframe(Excess_List)\n\t      if \"Period\" in Excess_List.columns:\n\t          Excess_List[\"Period\"] = Excess_List[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  try:\n\t      try:\n\t          CCE_Data = json.loads(CCE_Data)\n\t      except:\n\t          CCE_Data = eval(CCE_Data)\n\t\n\t      CCE_Data = convert_to_dataframe(CCE_Data)\n\t      if \"Period\" in CCE_Data.columns:\n\t          CCE_Data[\"Period\"] = CCE_Data[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  try:\n\t      try:\n\t          Policy_Dates = json.loads(Policy_Dates)\n\t      except:\n\t          Policy_Dates = eval(Policy_Dates)\n\t\n\t      Policy_Dates = convert_to_dataframe(Policy_Dates)\n\t      if \"Period\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Period\"] = Policy_Dates[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      if \"Policy Period Start Date\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Policy Period Start Date\"] = Policy_Dates[\"Policy Period Start Date\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  try:\n\t      try:\n\t          Year = json.loads(Year)\n\t      except:\n\t          Year = eval(Year)\n\t      Year = convert_to_dataframe(Year)\n\t  except Exception as e:\n\t      print(e)\n\t\n\t\n\t  ## Merging the data to form final CCE table\n\t  try:\n\t      CCE_Data['Period'] = CCE_Data['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      Excess_List['Period'] = Excess_List['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      merged_df1 = pd.merge(CCE_Data, Excess_List,  on='Period', how='inner')\n\t  except:\n\t      merged_df1 = CCE_Data.copy()\n\t      \n\t      print(\"Exception in merging claims info\")\n\t  \n\t  # print(merged_df1)\n\t  \n\t  # merged_df = [merged_df1.columns.tolist()] + merged_df1.values.tolist()\n\t  # return merged_df\n\t  \n\t  try:\n\t      merged_df2 = pd.merge(merged_df1, Policy_Dates,  on='Period', how='inner')\n\t  except Exception as e:\n\t      merged_df2 = merged_df1.copy()\n\t      print(\"Exception in merging policy info\")\n\t\n\t  # merged_df = [merged_df2.columns.tolist()] + merged_df2.values.tolist()\n\t  # return merged_df\n\t  \n\t  # print(merged_df2['Period'])\n\t  # print(Insurer_Data['Period'])\n\t  \n\t  try:\n\t      merged_df3 = pd.merge(merged_df2, Insurer_Data,  on='Period', how='inner')\n\t  except:\n\t      merged_df3 = merged_df2.copy()\n\t      print(\"Exception in merging all details\")\n\t  # print(merged_df3['Period'])\n\t  try:    \n\t      merged_df = pd.merge(merged_df3, Year,  on='Policy Period Start Date', how='inner')\n\t  except:\n\t      merged_df = merged_df3.copy()\n\t      print(\"Exception in merging Year info\")\n\t  \n\t  \n\t  try:\n\t    cover_mapping = json.loads(cover_mapping)\n\t    merged_df['Cover on Policy - Mapped'] = merged_df['Cover On Policy'].map(cover_mapping)\n\t  except Exception as e:\n\t    print(e)\n\t    merged_df = merged_df.copy()\n\t\n\t  rename_dict = {\n\t      'AD Excess': 'Excess: AD',\n\t      'Theft Excess': 'Excess: Theft',\n\t      'Fire Excess': 'Excess: Fire',\n\t      'WS Excess': 'Excess: WS',\n\t      'Cover On Policy': 'Cover on Policy',\n\t      'Vehicle Years Earned': 'Vehicle Years Earned',\n\t      'Claim Count': 'Claim Count: All',\n\t      # 'no of claims windscreen': 'Claim Count: WS',\n\t      'Incurreds Paid AD WS': 'Incurreds - Paid: AD&WS',\n\t      'Incurreds Paid FT': 'Incurreds - Paid: FT',\n\t      'Incurreds Paid TP': 'Incurreds - Paid: TP',\n\t      'Incurreds Outstanding AD WS': 'Incurreds - Outstanding: AD&WS',\n\t      'Incurreds Outstanding FT': 'Incurreds - Outstanding: FT',\n\t      'Incurreds Outstanding TP': 'Incurreds - Outstanding: TP',\n\t      'Total Incurred': 'Total Incurred Paid +  Outstanding',\n\t      'Policy Period Start Date': 'Policy Year Start Date',\n\t      'Policy Period End Date': 'Policy Year End Date'\n\t  }\n\t\n\t  safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t  merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t\n\t  try:\n\t      merged_df['Claim Count: WS'] = ''\n\t  except Exception as e:\n\t    print(e)\n\t  \n\t\n\t  columns_order = [ 'Year', 'Policy Year Start Date', \\\n\t                'Policy Year End Date', \n\t                'Insurer', \n\t                'Excess: AD',\\\n\t                'Excess: Fire', \\\n\t                'Excess: Theft', \\\n\t                'Excess: WS', \\\n\t                'Cover on Policy', \\\n\t                'Cover on Policy - Mapped',\n\t                 'Vehicle Years Earned', \n\t                'Claim Count: All', \\\n\t                'Claim Count: WS', \\\n\t                'Incurreds - Paid: AD&WS', \\\n\t                'Incurreds - Paid: FT', \\\n\t                'Incurreds - Paid: TP', \n\t                'Incurreds - Outstanding: AD&WS', \\\n\t                'Incurreds - Outstanding: FT' ,\n\t                'Incurreds - Outstanding: TP', \\\n\t                'Total Incurred Paid +  Outstanding']\n\t\n\t  # existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t               \n\t  # print(merged_df.columns)\n\t  merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t  merged_df = merged_df.dropna(how='all')\n\t  \n\t  merged_df.replace('N/A', '', inplace=True)\n\t  merged_df = merged_df.fillna('')\n\t\n\t  # merged_df = merged_df[existing_columns]\n\t\n\t  for col in columns_order:\n\t    if col not in merged_df.columns:\n\t      merged_df[col] = '' \n\t\n\t  merged_df = merged_df.drop_duplicates()\n\t  if len(merged_df.columns.tolist()) > 0:\n\t\n\t      merged_df = merged_df[columns_order]\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept Exception as e:\n\t  print(e)\n\t  return [columns_order]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756387477000",
        "name": "from_CCE_Table",
        "udf_type": "REFINER"
    },
    "17143": {
        "args": [],
        "code": "\tdata = [\n\t  {\n\t    \"trade\": \"General Manufacturing\",\n\t    \"aliases\": [\"steel\", \"metal\", \"samples\", \"manufacturing of glass\", \"manufacturing of furniture\", \"manufacturing\", \"general manufacturing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Fleet - Unclassified\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Contractor\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Builders\",\n\t    \"aliases\": [\"Building Contractors\", \"house builders\", \"builders\"]\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers UK only\",\n\t    \"aliases\": [\"haulage contractors - uk only\", \"hauliers uk only\"]\n\t  },\n\t  {\n\t    \"trade\": \"Electrician\",\n\t    \"aliases\": [\"Electrical Contractors\", \"electrical installation\", \"electrical testing\", \"electrician\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plumbing & Heating Engineer\",\n\t    \"aliases\": [\"plumbing & heating contractors\", \"gas servicing\", \"boiler maintenance\", \"plumber\", \"heating engineer\"]\n\t  },\n\t  {\n\t    \"trade\": \"Engineering\",\n\t    \"aliases\": [ \"engineers\", \"mechanical engineers\", \"engineering\", \"electrical engineers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Civil engineering\",\n\t    \"aliases\": [\"civil engineering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Scaffolding Contractors\",\n\t    \"aliases\": [\"scaffolders\", \"scaffolding contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Wholesale (non food and drink)\",\n\t    \"aliases\": [\"wholesalers of electrical components\", \"building materials\", \"wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Cleaning Contractors\",\n\t    \"aliases\": [\"domestic cleaners\", \"office cleaners\", \"cleaning contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plant Hire\",\n\t    \"aliases\": [\"plant hire operator\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Wholesale\",\n\t    \"aliases\": [\"cash & carry\", \"beer & wine wholesalers\", \"food wholesalers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Retail (non food and drink)\",\n\t    \"aliases\": [\"department store\", \"shopping centre\"]\n\t  },\n\t  {\n\t    \"trade\": \"Property Owners\",\n\t    \"aliases\": [\"landlords\", \"commercial property owners\"]\n\t  },\n\t  {\n\t    \"trade\": \"Telecommunications & IT\",\n\t    \"aliases\": [\"telecommunications installation\", \"it installation\", \"it services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Double Glazing\",\n\t    \"aliases\": [\"double glazing manufacture\", \"double glazing installation\"]\n\t  },\n\t  {\n\t    \"trade\": \"Landscape Gardener\",\n\t    \"aliases\": [\"gardening\", \"landscape gardener\"]\n\t  },\n\t  {\n\t    \"trade\": \"Other Prof/Sci/Tech\",\n\t    \"aliases\": [\"laboratory\"]\n\t  },\n\t  {\n\t    \"trade\": \"Removal Contractor\",\n\t    \"aliases\": [\"removals and storage\"]\n\t  },\n\t  {\n\t    \"trade\": \"Security and investigation\",\n\t    \"aliases\": [\"private detectives\", \"security guarding\", \"security services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Builders Merchant\",\n\t    \"aliases\": [\"building supplies\", \"suppliers of aggregates\"]\n\t  },\n\t  {\n\t    \"trade\": \"Service Engineers\",\n\t    \"aliases\": [\"mechanical servicing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Asphalters/Pavers/Engineers\",\n\t    \"aliases\": [\"road maintenance\", \"road surfacing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Flooring and Carpet\",\n\t    \"aliases\": [\"carpet fitters\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure Industry\",\n\t    \"aliases\": [\"nightclub\", \"pub\", \"leisure centre\", \"gym\"]\n\t  },\n\t  {\n\t    \"trade\": \"Social Religious or Charitable\",\n\t    \"aliases\": [\"charity\"]\n\t  },\n\t  {\n\t    \"trade\": \"Business Services\",\n\t    \"aliases\": [\"document storage\", \"administration\", \"consultants\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Services\",\n\t    \"aliases\": [\"food delivery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Retail\",\n\t    \"aliases\": [\"restaurant\", \"pub\", \"takeaway\"]\n\t  },\n\t  {\n\t    \"trade\": \"Estate Agent\",\n\t    \"aliases\": [\"lettings agents\", \"estate agent\"]\n\t  },\n\t  {\n\t    \"trade\": \"Furniture Sale and Manufacture\",\n\t    \"aliases\": [\"furniture retail\", \"furniture showroom\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure (Hotel, clubs & pubs)\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Shop Fitting\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"NHS Trust\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Import/Export\",\n\t    \"aliases\": [\"Import\", \"Export\"]\n\t  },\n\t  {\n\t    \"trade\": \"Catering\",\n\t    \"aliases\": [\"licensed catering\", \"unlicensed catering\", \"outside catering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Residential Care\",\n\t    \"aliases\": [\"care homes\", \"retirement homes\"]\n\t  },\n\t  {\n\t    \"trade\": \"Manufacturing Timber/Furniture\",\n\t    \"aliases\": [\"joinery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Printers and publishers\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Demolition Contractors\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Housing Association\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Timber Merchant\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Private Ambulance Service\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers Overseas\",\n\t    \"aliases\": [\"haulage contractors - overseas\", \"hauliers overseas\"]\n\t  },\n\t  {\n\t    \"trade\": \"Textiles & Clothing\",\n\t    \"aliases\": [\"clothing manufacturing\", \"textile manufacturing\", \"clothing retail\"]\n\t  },\n\t  {\n\t    \"trade\": \"Automotive Industry\",\n\t    \"aliases\": [\"vehicle manufacturing\", \"vehicle repairs\", \"vehicle parts manufacturing\", \"vehicle parts wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Pharmaceutical\",\n\t    \"aliases\": [\"pharmacy\", \"medical laboratories\"]\n\t  },\n\t  {\n\t    \"trade\": \"Farmer\",\n\t    \"aliases\": [\"dairy farmers\", \"arable farmers\", \"livestock farmers\"]\n\t  }\n\t]\n\t\n\treturn data\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360097000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17144": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "AXA Trade Description@0"
            },
            {
                "data_type": "FIELD",
                "name": "Business_Description",
                "value": "Business Description"
            }
        ],
        "code": "\tif Business_Description:\n\t  if Business_Description == \"\":\n\t      return \"\"\n\t  else:\n\t      return previous_line\n\telse:\n\t    return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360098000",
        "name": "clean",
        "udf_type": "REFINER"
    },
    "17145": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "AXA Trade Description@1"
            },
            {
                "data_type": "FIELD",
                "name": "Trade_Descriptions",
                "value": "Trade Descriptions"
            }
        ],
        "code": "\timport json\n\t\n\tprint(repr(previous_line))\n\tif previous_line == '\"\"' or previous_line == '' or previous_line == \"\":\n\t    return \"\"\n\t\n\tif previous_line:\n\t  try:\n\t      Trade_Descriptions = eval(Trade_Descriptions)\n\t  except:\n\t      try:\n\t          Trade_Descriptions = json.loads(Trade_Descriptions)\n\t      except:\n\t          # return \"Fleet - Unclassified\"\n\t          return \"Fleet - Unclassified\"\n\t\n\t  Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t    \n\t  if previous_line.lower() in Trade_Descriptions_ls:\n\t      return previous_line\n\t  else:\n\t      # return \"Fleet - Unclassified\"\n\t      return \"Fleet - Unclassified\"\n\telse:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360098000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17146": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Incepts On@1"
            },
            {
                "data_type": "FIELD",
                "name": "Expiry_Date",
                "value": "Expiry Date"
            }
        ],
        "code": "\tfrom datetime import datetime, timedelta\n\t\n\tdef get_new_date(incepts_on, expiry_date):\n\t    # If both are empty, return ''\n\t    if not incepts_on and not expiry_date:\n\t        return ''\n\t    \n\t    # Convert to datetime objects if not empty\n\t    expiry_dt = datetime.strptime(expiry_date, \"%d/%m/%Y\") if expiry_date else ''\n\t    incepts_dt = datetime.strptime(incepts_on, \"%d/%m/%Y\") if incepts_on else ''\n\t    \n\t    # 1) If Incepts On is empty, return Expiry Date + 1 day\n\t    if not incepts_on:\n\t        return (expiry_dt + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n\t    \n\t    # 2) If Incepts On and Expiry Date are the same, return Incepts On\n\t    if expiry_date and incepts_dt == expiry_dt:\n\t        return incepts_on\n\t    \n\t    # 3) If Incepts On is non-empty, return Incepts On\n\t    return incepts_on\n\t\n\ttry:\n\t    # previous_line = str(previous_line)\n\t    # Expiry_Date = str(Expiry_Date)\n\t    \n\t    previous_line = previous_line.replace('\"\"', '')\n\t    Expiry_Date = Expiry_Date.replace('\"\"', '')\n\t\n\t    result = get_new_date(previous_line, Expiry_Date)\n\t    \n\t    return result\n\texcept Exception as e:\n\t    print(e)\n\t    return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360098000",
        "name": "form_inception_date_with_expiry_date",
        "udf_type": "REFINER"
    },
    "17147": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Main Cover Type@1"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360098000",
        "name": "check_cover_type",
        "udf_type": "REFINER"
    },
    "17148": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Incepts_On",
                "value": "Incepts On"
            }
        ],
        "code": "\tif Incepts_On :\n\t  return Incepts_On\n\telse:\n\t  return ''",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360099000",
        "name": "get_inception_date",
        "udf_type": "REFINER"
    },
    "17149": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "CCE_Data",
                "value": "CCE Data"
            },
            {
                "data_type": "FIELD",
                "name": "CCE_Table",
                "value": "CCE Table"
            }
        ],
        "code": "\t\n\timport pandas as pd\n\timport json\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t    try:\n\t        CCE_Table = eval(CCE_Table)\n\t    except:\n\t        CCE_Table = json.loads(CCE_Table)\n\t    try:\n\t        CCE_Table = convert_to_dataframe(CCE_Table)\n\t        CCE_Table_rows = CCE_Table.shape[0]\n\t        return CCE_Table_rows\n\t    except:\n\t        return 0\n\texcept:\n\t    return \"Exception in code\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360625000",
        "name": "get_years_count_from_CCE",
        "udf_type": "REFINER"
    },
    "17150": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "CCE_Data",
                "value": "CCE Data"
            },
            {
                "data_type": "FIELD",
                "name": "Policy_Dates",
                "value": "Policy Dates"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t    try:\n\t        Policy_Dates = eval(Policy_Dates)    \n\t    except:\n\t        Policy_Dates = json.loads(Policy_Dates)   \n\t\n\t    Policy_Dates = convert_to_df(Policy_Dates)\n\t    Policy_Dates = Policy_Dates.astype(str)\n\t    print(Policy_Dates.columns)\n\t    CCE_Policy_start_dates_List = Policy_Dates[['Policy Period Start Date']].values.tolist()\n\t    filtered_vals = [each_val for each_val in CCE_Policy_start_dates_List if each_val not in [\"N/A\",\"\",None,\"null\"]]\n\t    if len(filtered_vals)>=1:\n\t      return [[\"Policy Period Start Date\"]] + CCE_Policy_start_dates_List\n\t    else:\n\t      CCE_Period_List = Policy_Dates[['Period']].values.tolist()\n\t      return [\"Policy Period Start Date\"] + [CCE_Period_List]\n\texcept Exception as e:\n\t    print(e)\n\t    return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360099000",
        "name": "get_period_list",
        "udf_type": "REFINER"
    },
    "17151": {
        "args": [],
        "code": "\treturn \"New Business\"\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360099000",
        "name": "get_transaction_type",
        "udf_type": "REFINER"
    },
    "17152": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_List",
                "value": "Excess List"
            },
            {
                "data_type": "FIELD",
                "name": "CCE_Data",
                "value": "CCE Data"
            },
            {
                "data_type": "FIELD",
                "name": "Year",
                "value": "Year"
            },
            {
                "data_type": "FIELD",
                "name": "cover_mapping",
                "value": "cover mapping"
            },
            {
                "data_type": "FIELD",
                "name": "Policy_Dates",
                "value": "Policy Dates"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            # print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\ttry:\n\t  try:\n\t      Excess_List = json.loads(Excess_List)\n\t      Excess_List = convert_to_dataframe(Excess_List)\n\t      if \"Period\" in Excess_List.columns:\n\t          Excess_List[\"Period\"] = Excess_List[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t  try:\n\t      CCE_Data = json.loads(CCE_Data)\n\t      CCE_Data = convert_to_dataframe(CCE_Data)\n\t      if \"Period\" in CCE_Data.columns:\n\t          CCE_Data[\"Period\"] = CCE_Data[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t  try:\n\t      Policy_Dates = json.loads(Policy_Dates)\n\t      Policy_Dates = convert_to_dataframe(Policy_Dates)\n\t      if \"Period\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Period\"] = Policy_Dates[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      if \"Policy Period Start Date\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Policy Period Start Date\"] = Policy_Dates[\"Policy Period Start Date\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  # try:\n\t  #     Insurer_Data = json.loads(Insurer_Data)\n\t  #     Insurer_Data = convert_to_dataframe(Insurer_Data)\n\t  # except Exception as e:\n\t  #     print(e)\n\t\n\t  try:\n\t      Year = json.loads(Year)\n\t      Year = convert_to_dataframe(Year)\n\t  except Exception as e:\n\t      print(e)\n\t\n\t\n\t  ## Merging the data to form final CCE table\n\t  try:\n\t      CCE_Data['Period'] = CCE_Data['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      Excess_List['Period'] = Excess_List['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      merged_df1 = pd.merge(CCE_Data, Excess_List,  on='Period', how='inner')\n\t  except:\n\t      merged_df1 = CCE_Data.copy()\n\t      \n\t      print(\"Exception in merging claims info\")\n\t  \n\t  # print(merged_df1)\n\t  \n\t  # merged_df = [merged_df1.columns.tolist()] + merged_df1.values.tolist()\n\t  # return merged_df\n\t  \n\t  try:\n\t      merged_df2 = pd.merge(merged_df1, Policy_Dates,  on='Period', how='inner')\n\t  except Exception as e:\n\t      merged_df2 = merged_df1.copy()\n\t      print(\"Exception in merging policy info\")\n\t\n\t  \n\t\n\t  # merged_df = [merged_df2.columns.tolist()] + merged_df2.values.tolist()\n\t  # return merged_df\n\t  \n\t  # print(merged_df2['Period'])\n\t  # print(Insurer_Data['Period'])\n\t  \n\t  # try:\n\t  #     merged_df3 = pd.merge(merged_df2, Insurer_Data,  on='Period', how='inner')\n\t  # except:\n\t  #     merged_df3 = merged_df2.copy()\n\t  #     print(\"Exception in merging all details\")\n\t\n\t  \n\t  try:    \n\t      merged_df = pd.merge(merged_df2, Year,  on='Policy Period Start Date', how='inner')\n\t  except:\n\t      merged_df = merged_df2.copy()\n\t      print(\"Exception in merging Year info\")\n\t\n\t  try:\n\t      merged_df = merged_df.sort_values(\n\t      by=\"Policy Period Start Date\",  # replace with your actual column name\n\t      ascending=False\n\t      )\n\t  except:\n\t      merged_df = merged_df.copy()\n\t\n\t  try:\n\t      # Assign rank (1 = most recent, increasing for older dates)\n\t      merged_df[\"Year\"] = merged_df[\"Policy Period Start Date\"].rank(\n\t          method=\"dense\", ascending=False\n\t      ).astype(int)\n\t  except:\n\t      merged_df = merged_df.copy()\n\t\n\t\n\t  try:\n\t      merged_df = merged_df.sort_values(\n\t      by=\"Policy Period Start Date\",  # replace with your actual column name\n\t      ascending=True\n\t      )\n\t  except:\n\t      merged_df = merged_df.copy()\n\t\n\t  \n\t\n\t\n\t  \n\t  try:\n\t    cover_mapping = json.loads(cover_mapping)\n\t    merged_df['Cover on Policy - Mapped'] = merged_df['Cover On Policy'].map(cover_mapping)\n\t  except Exception as e:\n\t    print(e)\n\t    merged_df = merged_df.copy()\n\t\n\t  rename_dict = {\n\t      'AD Excess': 'Excess: AD',\n\t      'Theft Excess': 'Excess: Theft',\n\t      'Fire Excess': 'Excess: Fire',\n\t      'WS Excess': 'Excess: WS',\n\t      'Cover On Policy': 'Cover on Policy',\n\t      'Vehicle Years Earned': 'Vehicle Years Earned',\n\t      'Claim Count': 'Claim Count: All',\n\t      # 'no of claims windscreen': 'Claim Count: WS',\n\t      'Incurreds Paid AD WS': 'Incurreds - Paid: AD&WS',\n\t      'Incurreds Paid FT': 'Incurreds - Paid: FT',\n\t      'Incurreds Paid TP': 'Incurreds - Paid: TP',\n\t      'Incurreds Outstanding AD WS': 'Incurreds - Outstanding: AD&WS',\n\t      'Incurreds Outstanding FT': 'Incurreds - Outstanding: FT',\n\t      'Incurreds Outstanding TP': 'Incurreds - Outstanding: TP',\n\t      'Total Incurred': 'Total Incurred Paid +  Outstanding',\n\t      'Policy Period Start Date': 'Policy Year Start Date',\n\t      'Policy Period End Date': 'Policy Year End Date'\n\t  }\n\t\n\t  safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t  merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t\n\t  try:\n\t      merged_df['Claim Count: WS'] = ''\n\t  except Exception as e:\n\t    print(e)\n\t  \n\t\n\t  columns_order = [ 'Year', 'Policy Year Start Date', \\\n\t                'Policy Year End Date', \n\t                'Insurer', \n\t                'Excess: AD',\\\n\t                'Excess: Fire', \\\n\t                'Excess: Theft', \\\n\t                'Excess: WS', \\\n\t                'Cover on Policy', \\\n\t                'Cover on Policy - Mapped',\n\t                 'Vehicle Years Earned', \n\t                'Claim Count: All', \\\n\t                'Claim Count: WS', \\\n\t                'Incurreds - Paid: AD&WS', \\\n\t                'Incurreds - Paid: FT', \\\n\t                'Incurreds - Paid: TP', \n\t                'Incurreds - Outstanding: AD&WS', \\\n\t                'Incurreds - Outstanding: FT' ,\n\t                'Incurreds - Outstanding: TP', \\\n\t                'Total Incurred Paid +  Outstanding']\n\t\n\t  # existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t               \n\t  # print(merged_df.columns)\n\t  merged_df.replace('N/A', '', inplace=True)\n\t\n\t  merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t  merged_df = merged_df.dropna(how='all')\n\t  \n\t  merged_df = merged_df.fillna('')\n\t\n\t  # merged_df = merged_df[existing_columns]\n\t\n\t  for col in columns_order:\n\t    if col not in merged_df.columns:\n\t      merged_df[col] = '' \n\t\n\t\n\t  if len(merged_df.columns.tolist()) > 0:\n\t      merged_df = merged_df[columns_order]\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept Exception as e:\n\t  print(e)\n\t  return [columns_order]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756384884000",
        "name": "form_CCE_table",
        "udf_type": "REFINER"
    },
    "17153": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Incepts_On",
                "value": "Incepts On"
            }
        ],
        "code": "\tfrom datetime import datetime, timedelta\n\t\n\tdef add_one_year_minus_one_day(date_str):\n\t    # Parse the input date (DD/MM/YYYY)\n\t    input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t    try:\n\t        # Try to add one year directly\n\t        one_year_later = input_date.replace(year=input_date.year + 1)\n\t    except ValueError:\n\t        # Handle Feb 29 (leap year issue) and other invalid dates\n\t        temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t        one_year_later = temp_date\n\t    # Subtract one day\n\t    final_date = one_year_later - timedelta(days=1)\n\t    # Return formatted date as DD/MM/YYYY with leading zeros\n\t    return final_date.strftime(\"%d/%m/%Y\")\n\t\n\ttry:\n\t    expiry_date = add_one_year_minus_one_day(Incepts_On)\n\t    return expiry_date\n\texcept Exception as e:\n\t    return \"\"\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360100000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17154": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Expires_On",
                "value": "Expires On"
            }
        ],
        "code": "\treturn Expires_On",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360100000",
        "name": "get_expiry_date",
        "udf_type": "REFINER"
    },
    "17155": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Year@0"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tcolumns_order = [\"Policy Period Start Date\", \"Year\"]\n\t\n\tprint(previous_line)\n\ttry:\n\t    if previous_line in [\"N/A\", \"[]\", \"\",  None, \"null\"]:\n\t      return [columns_order]\n\t    try:\n\t        df = eval(previous_line)    \n\t    except:\n\t        df = json.loads(previous_line)   \n\t\n\t    df = convert_to_df(df)\n\t    df = df.astype(str)\n\t    if len(df.columns.tolist()) > 0:\n\t        return [df.columns.tolist()] + df.values.tolist()\n\t    else:\n\t        return [columns_order]\n\texcept Exception as e:\n\t    print(e)\n\t    return [columns_order]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360101000",
        "name": "refine_result",
        "udf_type": "REFINER"
    },
    "17156": {
        "args": [],
        "code": "\treturn \"Mid Market\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360101000",
        "name": "get_business_category",
        "udf_type": "REFINER"
    },
    "17157": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_List",
                "value": "Excess List"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\t\n\ttable = json.loads(Excess_List)\n\t\n\tif Excess_List in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tExcess_List = convert_to_dataframe(table)\n\tunique_vals = Excess_List['Cover On Policy'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360101000",
        "name": "get_unique_values_from_cover_vehicle",
        "udf_type": "REFINER"
    },
    "17158": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Main Cover Type Mapped@0"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360101000",
        "name": "check_cover_type_mapped",
        "udf_type": "REFINER"
    },
    "17159": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t# import json\n\t# key = \"Line_2\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360102000",
        "name": "get_address_line2",
        "udf_type": "REFINER"
    },
    "17160": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t# import json\n\t# key = \"Line_3\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Address Line 3\")\n\t  return val\n\texcept:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360102000",
        "name": "get_address_line3",
        "udf_type": "REFINER"
    },
    "17161": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t# import json\n\t# key = \"Country\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Country\")\n\t  return val\n\texcept:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360102000",
        "name": "get_address_line",
        "udf_type": "REFINER"
    },
    "17162": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t# import json\n\t# key = \"County_State\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"State\")\n\t  return val\n\texcept:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360102000",
        "name": "get_address_line",
        "udf_type": "REFINER"
    },
    "17163": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t# import json\n\t# key = \"Town_City\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"City\")\n\t  return val\n\texcept:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360103000",
        "name": "get_address_line",
        "udf_type": "REFINER"
    },
    "17164": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t# import json\n\t# key = \"County_State\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Postcode\")\n\t  return val\n\texcept:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360103000",
        "name": "get_address_line",
        "udf_type": "REFINER"
    },
    "17165": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t# import json\n\t# key = \"Line_1\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360103000",
        "name": "get_address_line1",
        "udf_type": "REFINER"
    },
    "17166": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Policy Dates@0"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\tfrom datetime import datetime, timedelta\n\t\n\t# Custom function\n\tdef add_one_year_minus_one_day(date_str):\n\t    try:\n\t        input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t        try:\n\t            one_year_later = input_date.replace(year=input_date.year + 1)\n\t        except ValueError:\n\t            temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t            one_year_later = temp_date\n\t        final_date = one_year_later - timedelta(days=1)\n\t        return final_date.strftime(\"%d/%m/%Y\")\n\t    except Exception as e:\n\t        print(e)\n\t        return ''\n\t\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\t\n\ttry:\n\t    previous_line = eval(previous_line)\n\t    dates_df = convert_to_df(previous_line)\n\texcept:\n\t  try:\n\t      previous_line = json.loads(previous_line)\n\t      dates_df = convert_to_df(previous_line)\n\t  except:\n\t      return previous_line\n\t\n\ttry:\n\t\n\t    # Iterate row-wise\n\t    for index, row in dates_df.iterrows():\n\t        start_date = row['Policy Period Start Date']\n\t        \n\t        # Only update Policy Period End Date if Used Inception Date is 'Yes'\n\t        if str(row.get('Used Inception Date', '')).strip().lower() == 'yes':\n\t            dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t            print(\"Applied for \", row[\"Period\"])\n\t\n\t    # Clean up Period column after the loop\n\t    dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    # Convert to list format for output\n\t    final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    return final_df\n\t\n\t    # print(dates_df.columns)\n\t    # Iterate row-wise\n\t    # for index, row in dates_df.iterrows():\n\t    #     start_date = row['Policy Period Start Date']\n\t    #     dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t    #     dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    #     final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    # return final_df\n\t\n\texcept:\n\t    return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360103000",
        "name": "business_logic_for_policy_inception_date",
        "udf_type": "REFINER"
    },
    "17167": {
        "args": [],
        "code": "\tdata = [\n\t  {\n\t    \"trade\": \"General Manufacturing\",\n\t    \"aliases\": [\"steel\", \"metal\", \"samples\", \"manufacturing of glass\", \"manufacturing of furniture\", \"manufacturing\", \"general manufacturing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Fleet - Unclassified\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Contractor\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Builders\",\n\t    \"aliases\": [\"Building Contractors\", \"house builders\", \"builders\"]\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers UK only\",\n\t    \"aliases\": [\"haulage contractors - uk only\", \"hauliers uk only\"]\n\t  },\n\t  {\n\t    \"trade\": \"Electrician\",\n\t    \"aliases\": [\"Electrical Contractors\", \"electrical installation\", \"electrical testing\", \"electrician\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plumbing & Heating Engineer\",\n\t    \"aliases\": [\"plumbing & heating contractors\", \"gas servicing\", \"boiler maintenance\", \"plumber\", \"heating engineer\"]\n\t  },\n\t  {\n\t    \"trade\": \"Engineering\",\n\t    \"aliases\": [ \"engineers\", \"mechanical engineers\", \"engineering\", \"electrical engineers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Civil engineering\",\n\t    \"aliases\": [\"civil engineering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Scaffolding Contractors\",\n\t    \"aliases\": [\"scaffolders\", \"scaffolding contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Wholesale (non food and drink)\",\n\t    \"aliases\": [\"wholesalers of electrical components\", \"building materials\", \"wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Cleaning Contractors\",\n\t    \"aliases\": [\"domestic cleaners\", \"office cleaners\", \"cleaning contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plant Hire\",\n\t    \"aliases\": [\"plant hire operator\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Wholesale\",\n\t    \"aliases\": [\"cash & carry\", \"beer & wine wholesalers\", \"food wholesalers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Retail (non food and drink)\",\n\t    \"aliases\": [\"department store\", \"shopping centre\"]\n\t  },\n\t  {\n\t    \"trade\": \"Property Owners\",\n\t    \"aliases\": [\"landlords\", \"commercial property owners\"]\n\t  },\n\t  {\n\t    \"trade\": \"Telecommunications & IT\",\n\t    \"aliases\": [\"telecommunications installation\", \"it installation\", \"it services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Double Glazing\",\n\t    \"aliases\": [\"double glazing manufacture\", \"double glazing installation\"]\n\t  },\n\t  {\n\t    \"trade\": \"Landscape Gardener\",\n\t    \"aliases\": [\"gardening\", \"landscape gardener\"]\n\t  },\n\t  {\n\t    \"trade\": \"Other Prof/Sci/Tech\",\n\t    \"aliases\": [\"laboratory\"]\n\t  },\n\t  {\n\t    \"trade\": \"Removal Contractor\",\n\t    \"aliases\": [\"removals and storage\"]\n\t  },\n\t  {\n\t    \"trade\": \"Security and investigation\",\n\t    \"aliases\": [\"private detectives\", \"security guarding\", \"security services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Builders Merchant\",\n\t    \"aliases\": [\"building supplies\", \"suppliers of aggregates\"]\n\t  },\n\t  {\n\t    \"trade\": \"Service Engineers\",\n\t    \"aliases\": [\"mechanical servicing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Asphalters/Pavers/Engineers\",\n\t    \"aliases\": [\"road maintenance\", \"road surfacing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Flooring and Carpet\",\n\t    \"aliases\": [\"carpet fitters\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure Industry\",\n\t    \"aliases\": [\"nightclub\", \"pub\", \"leisure centre\", \"gym\"]\n\t  },\n\t  {\n\t    \"trade\": \"Social Religious or Charitable\",\n\t    \"aliases\": [\"charity\"]\n\t  },\n\t  {\n\t    \"trade\": \"Business Services\",\n\t    \"aliases\": [\"document storage\", \"administration\", \"consultants\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Services\",\n\t    \"aliases\": [\"food delivery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Retail\",\n\t    \"aliases\": [\"restaurant\", \"pub\", \"takeaway\"]\n\t  },\n\t  {\n\t    \"trade\": \"Estate Agent\",\n\t    \"aliases\": [\"lettings agents\", \"estate agent\"]\n\t  },\n\t  {\n\t    \"trade\": \"Furniture Sale and Manufacture\",\n\t    \"aliases\": [\"furniture retail\", \"furniture showroom\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure (Hotel, clubs & pubs)\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Shop Fitting\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"NHS Trust\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Import/Export\",\n\t    \"aliases\": [\"Import\", \"Export\"]\n\t  },\n\t  {\n\t    \"trade\": \"Catering\",\n\t    \"aliases\": [\"licensed catering\", \"unlicensed catering\", \"outside catering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Residential Care\",\n\t    \"aliases\": [\"care homes\", \"retirement homes\"]\n\t  },\n\t  {\n\t    \"trade\": \"Manufacturing Timber/Furniture\",\n\t    \"aliases\": [\"joinery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Printers and publishers\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Demolition Contractors\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Housing Association\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Timber Merchant\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Private Ambulance Service\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers Overseas\",\n\t    \"aliases\": [\"haulage contractors - overseas\", \"hauliers overseas\"]\n\t  },\n\t  {\n\t    \"trade\": \"Textiles & Clothing\",\n\t    \"aliases\": [\"clothing manufacturing\", \"textile manufacturing\", \"clothing retail\"]\n\t  },\n\t  {\n\t    \"trade\": \"Automotive Industry\",\n\t    \"aliases\": [\"vehicle manufacturing\", \"vehicle repairs\", \"vehicle parts manufacturing\", \"vehicle parts wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Pharmaceutical\",\n\t    \"aliases\": [\"pharmacy\", \"medical laboratories\"]\n\t  },\n\t  {\n\t    \"trade\": \"Farmer\",\n\t    \"aliases\": [\"dairy farmers\", \"arable farmers\", \"livestock farmers\"]\n\t  }\n\t]\n\t\n\treturn data",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360104000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17168": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "AXA Trade Description@0"
            },
            {
                "data_type": "FIELD",
                "name": "Business_Description",
                "value": "Business Description"
            }
        ],
        "code": "\tprint(Business_Description)\n\tif Business_Description:\n\t  if Business_Description == \"\":\n\t      return \"\"\n\t  else:\n\t      return previous_line\n\telse:\n\t    return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360104000",
        "name": "clean",
        "udf_type": "REFINER"
    },
    "17169": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "AXA Trade Description@1"
            },
            {
                "data_type": "FIELD",
                "name": "Trade_Descriptions",
                "value": "Trade Descriptions"
            }
        ],
        "code": "\timport json\n\t\n\tprint(repr(previous_line))\n\tif previous_line == '\"\"' or previous_line == '' or previous_line == \"\":\n\t    return \"\"\n\t\n\tif previous_line:\n\t\n\t  try:\n\t      Trade_Descriptions = eval(Trade_Descriptions)\n\t  except:\n\t      try:\n\t          Trade_Descriptions = json.loads(Trade_Descriptions)\n\t      except:\n\t          # return \"Fleet - Unclassified\"\n\t          return \"Fleet - Unclassified\"\n\t\n\t  Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t    \n\t  if previous_line.lower() in Trade_Descriptions_ls:\n\t    return previous_line\n\t  else:\n\t    return \"Fleet - Unclassified\"\n\telse:\n\t  return \"\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360104000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17170": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Main Cover Type@0"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360104000",
        "name": "check_cover_type",
        "udf_type": "REFINER"
    },
    "17171": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Vehicle Schedule Data@0"
            }
        ],
        "code": "\t# import json, re\n\t# import pandas as pd\n\t\n\t# def convert_to_dataframe(data):\n\t#     if isinstance(data, list):\n\t#         if all(isinstance(item, dict) for item in data):\n\t#             # List of dictionaries\n\t#             return pd.DataFrame(data)\n\t#         elif all(isinstance(item, list) for item in data):\n\t#             # List of lists\n\t#             return pd.DataFrame(data[1:] , columns= data[0])\n\t#         else:\n\t#             raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t#     else:\n\t#         raise TypeError(\"Input data is not a list.\")\n\t\n\t# columns = ['Cover Vehicle', 'Vehicle Registration']\n\t\n\t# try:\n\t#     # print(previous_line)\n\t#     if previous_line in ['', None, 'N/A', '[]', [], 'N/A']:\n\t#         return [columns]\n\t\n\t#     df = pd.DataFrame(columns=columns)\n\t\n\t\n\t#     # ```text[]``` or ```json[]```\n\t#     matches = re.findall(r'\\w+\\[([^\\]]*)\\]', previous_line)\n\t#     result = [ [item.strip() for item in match.split(',')] for match in matches ]\n\t#     if result:\n\t#       df = convert_to_dataframe(result)\n\t    \n\t    \n\t#     if df.empty:\n\t#         # | Cover Vehicle | Vehicle Registration | \\n |:----------------|:-----------------------| \\n | FMV | RO11AYY |\n\t#         rows = [line.strip() for line in previous_line.splitlines() if previous_line.strip().startswith('|')]\n\t#         list_of_lists = [\n\t#             [cell.strip() for cell in row.split('|')[1:-1]]\n\t#             for row in rows\n\t#         ]\n\t#         if list_of_lists:\n\t#           # Remove row that contains only dashes and colons (alignment row)\n\t#             cleaned = [\n\t#                 row for row in list_of_lists\n\t#                 if not all(cell.strip().startswith(':') or set(cell.strip()) <= {'-', ':'} for cell in row)\n\t#             ]\n\t#             # print(cleaned)\n\t#             if cleaned:\n\t#                 df = convert_to_dataframe(cleaned)\n\t#                 # print(df)\n\t\n\t\n\t#     if df.empty:\n\t#       try:\n\t#         previous_line = json.loads(previous_line)\n\t#         df = convert_to_dataframe(previous_line)\n\t#       except: \n\t#         previous_line = eval(previous_line)\n\t#         df = convert_to_dataframe(previous_line)\n\t\n\t    \n\t#     df = df.replace(\"N/A\", \"\")\n\t#     df.fillna(\"\", inplace=True)\n\t#     df = df[~df.apply(lambda row: row.isna().all() or (row == '').all(), axis=1)]\n\t    \n\t#     return [df.columns.tolist()] + df.values.tolist()\n\t\n\t# except Exception as e:\n\t#     return [['Cover Vehicle', 'Vehicle Registration']]\n\t\n\timport pandas as pd\n\timport json, re, ast\n\timport traceback\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = [\"Cover Vehicle\", \"Vehicle Registration\"]\n\t\n\t\n\tdef get_df_with_regex_match(previous_line, columns_order):\n\t    parsed, inner_content = None, None\n\t    df = pd.DataFrame(columns=columns_order)\n\t    print(\"hii\")\n\t    try:\n\t        match = re.search(r\"```(?:\\w+)?\\s*([\\s\\S]*?)\\s*```\", previous_line)\n\t        print(\"match\",match)\n\t        if match:\n\t            inner_content = match.group(1).strip()\n\t    \n\t            # Step 2: Try to parse it as JSON\n\t            try:\n\t                parsed = json.loads(inner_content)\n\t                print(parsed)\n\t            except json.JSONDecodeError:\n\t                # If JSON parsing fails, fallback to literal_eval\n\t                try:\n\t                    parsed = ast.literal_eval(inner_content)\n\t                except Exception as e:\n\t                    print(\"Parsing failed:\", e)\n\t                    parsed = None\n\t          \n\t        if parsed:\n\t\n\t            df = convert_to_dataframe(parsed)\n\t            return True, df \n\t        else:\n\t            return False, df \n\t    except Exception as e:\n\t      return False, pd.DataFrame(columns=columns_order)\n\t\n\tdef parse_markdown_format(previous_line, columns_order):\n\t    try:\n\t        lines = [line.strip() for line in previous_line.strip().split('\\n') if line.strip().startswith('|') and '---' not in line]\n\t        list_of_lists = [ [cell.strip() for cell in line.strip('|').split('|')] for line in lines ]\n\t        if list_of_lists:\n\t            df = convert_to_dataframe(list_of_lists)\n\t            return True, df\n\t        else:\n\t            return False, pd.DataFrame(columns=columns_order)\n\t    except:\n\t        return False, pd.DataFrame(columns=columns_order)\n\t\n\ttry:\n\t    \n\t    df = pd.DataFrame(columns=columns_order)\n\t    print(df)\n\t    flg, df = get_df_with_regex_match(previous_line, columns_order)\n\t    \n\t    if not flg:\n\t        flg, df = parse_markdown_format(previous_line, columns_order)\n\t\n\t    if not flg:\n\t      try:\n\t          data = json.loads(previous_line)\n\t          df = convert_to_dataframe(data)\n\t      except Exception as e:\n\t          data = ast.literal_eval(previous_line)\n\t          df = convert_to_dataframe(data)\n\t      print(df)\n\t    \n\t    \n\t    df.fillna(\"\", inplace=True)\n\t    \n\t    if len(df.columns.tolist()) > 0:\n\t        df = df[columns_order]\n\t        df = [df.columns.tolist()] + df.values.tolist()\n\t        return df\n\t    else:\n\t        return [columns_order]\n\t      \n\texcept Exception as e:\n\t    return [columns_order]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360105000",
        "name": "refine_result",
        "udf_type": "REFINER"
    },
    "17172": {
        "args": [],
        "code": "\treturn \"Mid Market\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360105000",
        "name": "get_business_category",
        "udf_type": "REFINER"
    },
    "17173": {
        "args": [],
        "code": "\treturn \"New Business\"\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360105000",
        "name": "get_transaction_type",
        "udf_type": "REFINER"
    },
    "17174": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Vehicle_Schedule_Data",
                "value": "Vehicle Schedule Data"
            }
        ],
        "code": "\t# # Import Python packages\n\timport json\n\timport pandas as pd\n\t# list_of_dct = json.loads(Vehicle_Schedule_Data)\n\t# # print(list_of_dct)\n\t\n\t# try:\n\t#     unique_cover_basis_set = set(item['Cover Vehicle'] for item in list_of_dct)\n\t#     unique_cover_basis_list = list(unique_cover_basis_set)\n\t#     unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t#     return unique_cover_basis_list_without_na\n\t\n\t# except Exception as e:\n\t#     # print(\"in ex\", e)\n\t#     # print(type(list_of_dct))\n\t#     unique_first_col = list(set(row[0] for row in list_of_dct))\n\t#     if 'Cover Vehicle' in unique_first_col:\n\t#         unique_first_col.remove('Cover Vehicle')\n\t\n\t#     return unique_first_col\n\t    \n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tVehicle_Schedule_Data = json.loads(Vehicle_Schedule_Data)\n\tprint(type(Vehicle_Schedule_Data))\n\t\n\tif Vehicle_Schedule_Data in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tVehicle_Schedule_Data = convert_to_dataframe(Vehicle_Schedule_Data)\n\tunique_vals = Vehicle_Schedule_Data['Cover Vehicle'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360105000",
        "name": "get_unique_cover_values",
        "udf_type": "REFINER"
    },
    "17175": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Vehicle_Schedule_Data",
                "value": "Vehicle Schedule Data"
            },
            {
                "data_type": "FIELD",
                "name": "Unique__Values__Cover__Basis",
                "value": "Unique_Values_Cover_Basis"
            },
            {
                "data_type": "FIELD",
                "name": "Cover_Basis_Mapping",
                "value": "Cover Basis Mapping"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\t# columns_order = [ 'Vehicle Registration', \\\n\t#                 'Cover - Vehicle', \\\n\t#                 'Cover - Vehicle - Mapped']\n\t\n\tcolumns_order = ['Effective From', \\\n\t                  'Effective To', \\\n\t                'Vehicle Registration', \\\n\t                'Cover - Vehicle', \\\n\t                'Cover - Vehicle - Mapped']\n\t\n\t\n\tif Vehicle_Schedule_Data == \"[]\":\n\t    return [columns_order]\n\ttry:\n\t    vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t    vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t\n\t    vehicle_num_and_cover_type_table_df.rename(columns={\n\t        'Cover Vehicle' : 'Cover - Vehicle',\n\t        'Vehicle Registration Number' : 'Vehicle Registration' \n\t      }, inplace=True)\n\t\n\t    try:\n\t        Unique__Values__Cover__Basis = json.loads(Unique__Values__Cover__Basis)\n\t    except:\n\t        Unique__Values__Cover__Basis = eval(Unique__Values__Cover__Basis)\n\t\n\t\n\t    if Unique__Values__Cover__Basis:\n\t        try:\n\t            Cover_Basis_Mapping_dct = json.loads(Cover_Basis_Mapping)\n\t            vehicle_num_and_cover_type_table_df['Cover - Vehicle - Mapped'] = vehicle_num_and_cover_type_table_df['Cover - Vehicle'].replace(Cover_Basis_Mapping_dct)\n\t        except:\n\t            vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.copy()\n\t\n\t    invalid_values = [\"\", \"n/a\", \"null\", \"none\", \"nan\"]\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t   \n\t\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.replace('N/A', '')\n\t    vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t\n\t    for col in columns_order:\n\t      if col not in vehicle_num_and_cover_type_table_df.columns:\n\t        vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t    if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t        vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t        return vehicle_num_and_cover_type_table_df\n\t    else:\n\t        return [columns_order]\n\texcept:\n\t    try:\n\t        vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t        vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t        try:\n\t            vehicle_num_and_cover_type_table_df.rename(columns={\n\t                'Cover Vehicle' : 'Cover - Vehicle',\n\t                'Vehicle Registration Number' : 'Vehicle Registration' \n\t              }, inplace=True)\n\t\n\t        except:\n\t            print()\n\t\n\t        for col in columns_order:\n\t          if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t        invalid_values = [\"\", \"n/a\", \"N/A\", \"null\", \"None\", \"nan\"]\n\t\n\t        # Filter out rows with invalid vehicle registrations\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t\n\t\n\t        vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t\n\t        if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t            vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t            vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t            return vehicle_num_and_cover_type_table_df\n\t        else:\n\t            return [columns_order]\n\t    except:\n\t        return [columns_order]\n\t    return [columns_order]",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360106000",
        "name": "form_vehicle_schedule_table",
        "udf_type": "REFINER"
    },
    "17176": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Vehicle_Schedule_Table",
                "value": "Vehicle Schedule Table"
            }
        ],
        "code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame( data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\ttry:\n\t  if Vehicle_Schedule_Table == \"[]\" or Vehicle_Schedule_Table == 'n/a':\n\t    return \"n/a\"\n\t  else:\n\t    Vehicle_Schedule_Table = json.loads(Vehicle_Schedule_Table)\n\t    Vehicle_Schedule_Table = convert_to_dataframe(Vehicle_Schedule_Table)\n\t    return len(Vehicle_Schedule_Table['Vehicle Registration'])\n\texcept:\n\t  return \"0\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360106000",
        "name": "get_number_of_notifiable_vehicles",
        "udf_type": "REFINER"
    },
    "17177": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Number_of_Notifiable_Vehicles",
                "value": "Number of Notifiable Vehicles"
            }
        ],
        "code": "\t\n\t\n\t# def determine_offering_type(vehicle_count, premium_cleaned=None):\n\t\n\t#     if vehicle_count is None:\n\t#         return \"\"\n\t#     if vehicle_count <= 19:\n\t#         if premium_cleaned is None or premium_cleaned < 10000:\n\t#             return \"Mini Fleet\"\n\t#     elif 20 <= vehicle_count <= 149:\n\t#         if premium_cleaned is None or 10000 <= premium_cleaned < 250000:\n\t#             # print(\"yes\")\n\t#             return \"Vantage Fleet\"\n\t#     elif vehicle_count >= 150:\n\t#         if premium_cleaned is None or premium_cleaned >= 250000:\n\t#             return \"Mid Corp\"\n\t\n\t#     return \"\"\n\t\n\t# try:\n\t  \n\t#     Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\t#     offering_type = determine_offering_type( Number_of_Notifiable_Vehicles )\n\t#     print(offering_type)\n\t#     return offering_type\n\t\n\t# except Exception as e:\n\t#   print(e)\n\t#   return \"\"\n\t\n\t \n\tdef clean_premium(premium_value):\n\t    if premium_value is None:\n\t        return None\n\t    if isinstance(premium_value, (int, float)):\n\t        return float(premium_value)\n\t    try:\n\t        # Remove currency symbols, commas, and whitespace\n\t        # print(premium_value)\n\t        # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t        cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t        print(cleaned)\n\t        return float(cleaned)\n\t    except Exception:\n\t        return None\n\t\n\tdef determine_offering_type(vehicle_count, premium_cleaned=None):    \n\t\n\t    if vehicle_count == 0:\n\t        if premium_cleaned is None or premium_cleaned < 10000:\n\t            return \"Mini Fleet\"\n\t        \n\t        elif 10000 <= premium_cleaned < 250000:\n\t            return \"Vantage Fleet\"\n\t        \n\t        elif premium_cleaned >= 250000:\n\t            return \"Mid Corp\"\n\t\n\t    if vehicle_count <= 19:\n\t            return \"Mini Fleet\"\n\t            \n\t    elif 20 <= vehicle_count <= 149:\n\t            return \"Vantage Fleet\"\n\t        \n\t    elif vehicle_count >= 150:\n\t            return \"Mid Corp\"\n\t\n\t\n\t\n\ttry:\n\t    Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\texcept:\n\t    Number_of_Notifiable_Vehicles = 0\n\t\n\t\n\toffering_type = determine_offering_type( Number_of_Notifiable_Vehicles)\n\treturn offering_type",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360106000",
        "name": "get_offering_type",
        "udf_type": "REFINER"
    },
    "17178": {
        "args": [
            {
                "data_type": "LINE",
                "name": "previous_line",
                "value": "Main Cover Type Mapped@0"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360107000",
        "name": "check_cover_type_mapped",
        "udf_type": "REFINER"
    },
    "17179": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360107000",
        "name": "get_addressline2",
        "udf_type": "REFINER"
    },
    "17180": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 3\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360107000",
        "name": "unnamed_custom_function",
        "udf_type": "REFINER"
    },
    "17181": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Country\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360107000",
        "name": "get_country",
        "udf_type": "REFINER"
    },
    "17182": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"State\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360108000",
        "name": "get_state",
        "udf_type": "REFINER"
    },
    "17183": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"City\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360108000",
        "name": "get_city",
        "udf_type": "REFINER"
    },
    "17184": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Postcode\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360108000",
        "name": "get_postcode",
        "udf_type": "REFINER"
    },
    "17185": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Details",
                "value": "Party Address Details"
            }
        ],
        "code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360108000",
        "name": "get_address_line1",
        "udf_type": "REFINER"
    },
    "17186": {
        "args": [],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the desired output\n\treturn \"Yes\"",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360109000",
        "name": "is_haulage_fact_finder_received",
        "udf_type": "REFINER"
    },
    "17187": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "CCE_Table",
                "value": "CCE Table"
            },
            {
                "data_type": "FIELD",
                "name": "Incepts_On",
                "value": "Incepts On"
            }
        ],
        "code": "\timport pandas as pd\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t\tCCE_Table = json.loads(CCE_Table)\n\texcept:\n\t\ttry:\n\t\t\t\tCCE_Table = eval(CCE_Table)\n\t\texcept:\n\t\t\t\treturn None\n\t\n\ttry:\n\t    CCE_Table = convert_to_df(CCE_Table)\n\t\n\t    # Convert the column to datetime\n\t    CCE_Table['Policy Year Start Date'] = pd.to_datetime(CCE_Table['Policy Year Start Date'], format='%d/%m/%Y')\n\t\n\t    incepts_on_date = pd.to_datetime(Incepts_On, format='%d/%m/%Y')\n\t\n\t    # Compare day and month of inception date with policy period start date\n\t    all_match = CCE_Table['Policy Year Start Date'].apply(\n\t        lambda x: x.day == incepts_on_date.day and x.month == incepts_on_date.month\n\t    ).all()\n\t\n\t    # Final result\n\t    result = None if all_match else \"Inception date doesn't matches with Policy Period Start Dates\"\n\t\n\t    return result\n\t\n\texcept:\n\t    return None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360115000",
        "name": "compare_inception_date_with_policy_period_start_date",
        "udf_type": "VALIDATIONS"
    },
    "17188": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Expires_On",
                "value": "Expires On"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Expires_On:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Expires_On = str(Expires_On).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\n\t\tif contains_only_quotes(Expires_On):\n\t\t\treturn None\n\t\tif not isinstance(Expires_On,str):\n\t\t\treturn \"Expires On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Expires_On):\n\t\t\treturn \"Expires On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Expires_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360115000",
        "name": "validate_expires_on",
        "udf_type": "VALIDATIONS"
    },
    "17189": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_Accident_Damage",
                "value": "Excess Type Accident Damage"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Accident_Damage:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Accident_Damage = str(Excess_Type_Accident_Damage).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Accident_Damage)\n\t  except Exception:\n\t    return \"Invalid Excess Type Accident Damage\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360116000",
        "name": "validate_excess_type_accident_damage",
        "udf_type": "VALIDATIONS"
    },
    "17190": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_Fire",
                "value": "Excess Type Fire"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Fire:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Fire = str(Excess_Type_Fire).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Fire)\n\t  except Exception:\n\t    return \"Invalid Excess Type Fire Price\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360116000",
        "name": "validate_excess_type_fire",
        "udf_type": "VALIDATIONS"
    },
    "17191": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_Theft",
                "value": "Excess Type Theft"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Theft:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Theft = str(Excess_Type_Theft).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Theft)\n\t  except Exception:\n\t    return \"Invalid Excess Type Theft Price\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360116000",
        "name": "validate_excess_type_theft",
        "udf_type": "VALIDATIONS"
    },
    "17192": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_WS",
                "value": "Excess Type WS"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_WS:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_WS = str(Excess_Type_WS).strip()\n\t    clean_and_validate_price_loose(Excess_Type_WS)\n\t  except Exception:\n\t    return \"Invalid Excess Type WS Price\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360117000",
        "name": "validate_excess_type_ws",
        "udf_type": "VALIDATIONS"
    },
    "17193": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Number_of_Notifiable_Vehicles",
                "value": "Number of Notifiable Vehicles"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\tif Number_of_Notifiable_Vehicles:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t\tNumber_of_Notifiable_Vehicles = str(Number_of_Notifiable_Vehicles).strip()\n\t\texcept:\n\t\t\treturn None\n\t\ttry:\n\t\t\tn = int (Number_of_Notifiable_Vehicles)\n\t\t\tif n < 0:\n\t\t\t\treturn \"Number of Notifiable Vehicles must be a non-negative interger.\"\n\t\t\treturn None\n\t\texcept(ValueError, TypeError):\n\t\t\treturn \"Number of Notifiable Vehicles must be a number.\"\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360117000",
        "name": "validate_number_of_notifiable_vehicles",
        "udf_type": "VALIDATIONS"
    },
    "17194": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Postcode",
                "value": "Party Address Postcode"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360117000",
        "name": "validate_party_address_postcode",
        "udf_type": "VALIDATIONS"
    },
    "17195": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Target_Price",
                "value": "Target Price"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_target_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Target_Price:\n\t\t# Return None to indicate validation passed\n\t  try:\n\t      Target_Price = str(Target_Price).strip()\n\t  except:\n\t      None\n\t  try:\n\t      clean_and_validate_target_price_loose(Target_Price)\n\t  except Exception:\n\t      print(traceback.format_exc())\n\t      return \"Invalid Target Price\"\n\telse:\n\t      return None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360117000",
        "name": "validate_target_price",
        "udf_type": "VALIDATIONS"
    },
    "17196": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Agency_Enquiry_Reference",
                "value": "Agency Enquiry Reference"
            }
        ],
        "code": "\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Agency_Enquiry_Reference:\n\t\t# Return None to indicate validation passed\n\t\tif not isinstance(Agency_Enquiry_Reference, str):\n\t\t\treturn \"Agency Enquiry Reference must be a string\"\n\t\tAgency_Enquiry_Reference = Agency_Enquiry_Reference.strip()\n\t\tif not Agency_Enquiry_Reference:\n\t\t\treturn \"Agency Enquiry Reference cannot be empty or whitespace.\"\n\t\tif contains_only_quotes(Agency_Enquiry_Reference):\n\t\t\t\treturn None\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360118000",
        "name": "validate_agency_enquiry_reference",
        "udf_type": "VALIDATIONS"
    },
    "17197": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Broker_Deadline",
                "value": "Broker Deadline"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Broker_Deadline:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Broker_Deadline = str(Broker_Deadline).strip()\n\t\texcept:\n\t\t\treturn None\n\t\n\t\tif contains_only_quotes(Broker_Deadline):\n\t\t\treturn None\n\t\tif not isinstance(Broker_Deadline,str):\n\t\t\treturn \"Broker Deadline must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Broker_Deadline):\n\t\t\treturn \"Broker Deadline must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Broker_Deadline,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360118000",
        "name": "validate_broker_deadline",
        "udf_type": "VALIDATIONS"
    },
    "17198": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "CCE_Table",
                "value": "CCE Table"
            }
        ],
        "code": "\timport pandas as pd\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t  CCE_Table = str(CCE_Table).strip()\n\texcept:\n\t  return None\n\ttry:\n\t\tCCE_Table = json.loads(CCE_Table)\n\texcept:\n\t\ttry:\n\t\t\t\tCCE_Table = eval(CCE_Table)\n\t\texcept:\n\t\t\t\treturn None\n\t\n\ttry:\n\t    CCE_Table = convert_to_df(CCE_Table)\n\t\n\t    # Convert the column to datetime\n\t    CCE_Table['Policy Year Start Date'] = pd.to_datetime(CCE_Table['Policy Year Start Date'], format='%d/%m/%Y')\n\t\n\t    incepts_on_date = pd.to_datetime(Incepts_On, format='%d/%m/%Y')\n\t\n\t    # Compare day and month of inception date with policy period start date\n\t    all_match = CCE_Table['Policy Year Start Date'].apply(\n\t        lambda x: x.day == incepts_on_date.day and x.month == incepts_on_date.month\n\t    ).all()\n\t\n\t    # Final result\n\t    result = None if all_match else \"Inception date doesn't matches with Policy Period Start Dates\"\n\t\n\t    return result\n\t\n\texcept:\n\t    return None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360118000",
        "name": "compare_inception_date_with_policy_period_start_date",
        "udf_type": "VALIDATIONS"
    },
    "17199": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Date_Established",
                "value": "Date Established"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Date_Established:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Date_Established = str(Date_Established).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\t\n\t\tif contains_only_quotes(Date_Established):\n\t\t\treturn None\n\t\tif not isinstance(Date_Established,str):\n\t\t\treturn \"Date Established must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Date_Established):\n\t\t\treturn \"Date Established must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Date_Established,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360119000",
        "name": "validate_date_established",
        "udf_type": "VALIDATIONS"
    },
    "17200": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Incepts_On",
                "value": "Incepts On"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Incepts_On:\n\t\ttry:\n\t\t\tIncepts_On = str(Incepts_On).strip()\n\t\texcept:\n\t\t\treturn None \n\t\t# Return None to indicate validation passed\n\t\t# Incepts_On = Incepts_On.strip()\n\t\tif contains_only_quotes(Incepts_On):\n\t\t\treturn None\n\t\tif not isinstance(Incepts_On,str):\n\t\t\treturn \"Incepts On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Incepts_On):\n\t\t\treturn \"Incepts On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Incepts_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360119000",
        "name": "validate_incepts_on",
        "udf_type": "VALIDATIONS"
    },
    "17201": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Expires_On",
                "value": "Expires On"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Expires_On:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Expires_On = str(Expires_On).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\n\t\tif contains_only_quotes(Expires_On):\n\t\t\treturn None\n\t\tif not isinstance(Expires_On,str):\n\t\t\treturn \"Expires On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Expires_On):\n\t\t\treturn \"Expires On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Expires_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360119000",
        "name": "validate_expires_on",
        "udf_type": "VALIDATIONS"
    },
    "17202": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Number_of_Notifiable_Vehicles",
                "value": "Number of Notifiable Vehicles"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\tif Number_of_Notifiable_Vehicles:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t\tNumber_of_Notifiable_Vehicles = str(Number_of_Notifiable_Vehicles).strip()\n\t\texcept:\n\t\t\treturn None\n\t\ttry:\n\t\t\tn = int (Number_of_Notifiable_Vehicles)\n\t\t\tif n < 0:\n\t\t\t\treturn \"Number of Notifiable Vehicles must be a non-negative interger.\"\n\t\t\treturn None\n\t\texcept(ValueError, TypeError):\n\t\t\treturn \"Number of Notifiable Vehicles must be a number.\"\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360119000",
        "name": "validate_number_of_notifiable_vehicles",
        "udf_type": "VALIDATIONS"
    },
    "17203": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_Accident_Damage",
                "value": "Excess Type Accident Damage"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Accident_Damage:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Accident_Damage = str(Excess_Type_Accident_Damage).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Accident_Damage)\n\t  except Exception:\n\t    return \"Invalid Excess Type Accident Damage\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360120000",
        "name": "validate_excess_type_accident_damage",
        "udf_type": "VALIDATIONS"
    },
    "17204": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_Fire",
                "value": "Excess Type Fire"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Fire:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Fire = str(Excess_Type_Fire).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Fire)\n\t  except Exception:\n\t    return \"Invalid Excess Type Fire Price\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360120000",
        "name": "validate_excess_type_fire",
        "udf_type": "VALIDATIONS"
    },
    "17205": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_Theft",
                "value": "Excess Type Theft"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Theft:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Theft = str(Excess_Type_Theft).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Theft)\n\t  except Exception:\n\t    return \"Invalid Excess Type Theft Price\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360120000",
        "name": "validate_excess_type_theft",
        "udf_type": "VALIDATIONS"
    },
    "17206": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_WS",
                "value": "Excess Type WS"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_WS:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_WS = str(Excess_Type_WS).strip()\n\t    clean_and_validate_price_loose(Excess_Type_WS)\n\t  except Exception:\n\t    return \"Invalid Excess Type WS Price\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360121000",
        "name": "validate_excess_type_ws",
        "udf_type": "VALIDATIONS"
    },
    "17207": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Incepts_On",
                "value": "Incepts On"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Incepts_On:\n\t\ttry:\n\t\t\tIncepts_On = str(Incepts_On).strip()\n\t\texcept:\n\t\t\treturn None \n\t\t# Return None to indicate validation passed\n\t\t# Incepts_On = Incepts_On.strip()\n\t\tif contains_only_quotes(Incepts_On):\n\t\t\treturn None\n\t\tif not isinstance(Incepts_On,str):\n\t\t\treturn \"Incepts On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Incepts_On):\n\t\t\treturn \"Incepts On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Incepts_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360121000",
        "name": "validate_incepts_on",
        "udf_type": "VALIDATIONS"
    },
    "17208": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Expires_On",
                "value": "Expires On"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Expires_On:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Expires_On = str(Expires_On).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\n\t\tif contains_only_quotes(Expires_On):\n\t\t\treturn None\n\t\tif not isinstance(Expires_On,str):\n\t\t\treturn \"Expires On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Expires_On):\n\t\t\treturn \"Expires On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Expires_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360121000",
        "name": "validate_expires_on",
        "udf_type": "VALIDATIONS"
    },
    "17209": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_Theft_CCE",
                "value": "Excess Type Theft CCE"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Theft_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Theft_CCE = str(Excess_Type_Theft_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Theft_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type Theft Price\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360122000",
        "name": "validate_excess_type_theft",
        "udf_type": "VALIDATIONS"
    },
    "17210": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_Accident_Damage_CCE",
                "value": "Excess Type Accident Damage CCE"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Accident_Damage_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Accident_Damage_CCE = str(Excess_Type_Accident_Damage_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Accident_Damage_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type Accident Damage\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360122000",
        "name": "validate_excess_type_accident_damage",
        "udf_type": "VALIDATIONS"
    },
    "17211": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_Fire_CCE",
                "value": "Excess Type Fire CCE"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Fire_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Fire_CCE = str(Excess_Type_Fire_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Fire_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type Fire Price\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360122000",
        "name": "validate_excess_type_fire",
        "udf_type": "VALIDATIONS"
    },
    "17212": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Excess_Type_WS_CCE",
                "value": "Excess Type WS CCE"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_WS_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_WS_CCE = str(Excess_Type_WS_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_WS_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type WS Price\"\n\telse:\n\t  return None\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360123000",
        "name": "validate_excess_type_ws",
        "udf_type": "VALIDATIONS"
    },
    "17213": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Postcode",
                "value": "Party Address Postcode"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360123000",
        "name": "validate_party_address_postcode",
        "udf_type": "VALIDATIONS"
    },
    "17214": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Postcode",
                "value": "Party Address Postcode"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360123000",
        "name": "validate_party_address_postcode",
        "udf_type": "VALIDATIONS"
    },
    "17215": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Number_of_Notifiable_Vehicles",
                "value": "Number of Notifiable Vehicles"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\tif Number_of_Notifiable_Vehicles:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t\tNumber_of_Notifiable_Vehicles = str(Number_of_Notifiable_Vehicles).strip()\n\t\texcept:\n\t\t\treturn None\n\t\ttry:\n\t\t\tn = int (Number_of_Notifiable_Vehicles)\n\t\t\tif n < 0:\n\t\t\t\treturn \"Number of Notifiable Vehicles must be a non-negative interger.\"\n\t\t\treturn None\n\t\texcept(ValueError, TypeError):\n\t\t\treturn \"Number of Notifiable Vehicles must be a number.\"\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360123000",
        "name": "validate_number_of_notifiable_vehicles",
        "udf_type": "VALIDATIONS"
    },
    "17216": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Party_Address_Postcode",
                "value": "Party Address Postcode"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360124000",
        "name": "validate_party_address_postcode",
        "udf_type": "VALIDATIONS"
    },
    "17217": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Target_Price",
                "value": "Target Price"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_target_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Target_Price:\n\t\t# Return None to indicate validation passed\n\t  try:\n\t      Target_Price = str(Target_Price).strip()\n\t  except:\n\t      None\n\t  try:\n\t      clean_and_validate_target_price_loose(Target_Price)\n\t  except Exception:\n\t      print(traceback.format_exc())\n\t      return \"Invalid Target Price\"\n\telse:\n\t      return None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360125000",
        "name": "validate_target_price",
        "udf_type": "VALIDATIONS"
    },
    "17218": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Agency_Enquiry_Reference",
                "value": "Agency Enquiry Reference"
            }
        ],
        "code": "\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Agency_Enquiry_Reference:\n\t\t# Return None to indicate validation passed\n\t\tif not isinstance(Agency_Enquiry_Reference, str):\n\t\t\treturn \"Agency Enquiry Reference must be a string\"\n\t\tAgency_Enquiry_Reference = Agency_Enquiry_Reference.strip()\n\t\tif not Agency_Enquiry_Reference:\n\t\t\treturn \"Agency Enquiry Reference cannot be empty or whitespace.\"\n\t\tif contains_only_quotes(Agency_Enquiry_Reference):\n\t\t\t\treturn None\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360125000",
        "name": "validate_agency_enquiry_reference",
        "udf_type": "VALIDATIONS"
    },
    "17219": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Broker_Deadline",
                "value": "Broker Deadline"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Broker_Deadline:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Broker_Deadline = str(Broker_Deadline).strip()\n\t\texcept:\n\t\t\treturn None\n\t\n\t\tif contains_only_quotes(Broker_Deadline):\n\t\t\treturn None\n\t\tif not isinstance(Broker_Deadline,str):\n\t\t\treturn \"Broker Deadline must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Broker_Deadline):\n\t\t\treturn \"Broker Deadline must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Broker_Deadline,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360126000",
        "name": "validate_broker_deadline",
        "udf_type": "VALIDATIONS"
    },
    "17220": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "CCE_Table",
                "value": "CCE Table"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_numeric_columns_old(df, columns_to_check):\n\t\n\t    invalid_columns = []\n\t    \n\t    for col in columns_to_check:\n\t        # Treat empty strings as NaN\n\t        # series = df[col].replace(\"\", np.nan)\n\t        \n\t        # Convert to numeric (non-numeric -> NaN)\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        \n\t        # Compare only non-empty rows\n\t        mask_non_empty = series.notna()\n\t        non_numeric_mask = mask_non_empty & numeric_check.isna()\n\t\n\t        if non_numeric_mask.any():\n\t            # Store non-numeric values for this column\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t\n\t    # print(invalid_columns)\n\t\n\t    # Return None if all columns are valid\n\t    if len(invalid_columns)==0:\n\t        return None\n\t    # else:\n\t    #     print(str(invalid_columns) + \" has non numeric columns\")\n\t    #     return str(invalid_columns) + \"has non numeric columns\"\n\t    elif len(invalid_columns) == 1:\n\t        return f\"The column '{invalid_columns[0]}' contains non-numeric values.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_columns)\n\t        return f\"The columns {cols_str} contain non-numeric values.\"\n\t\n\t\n\t  \n\tdef validate_numeric_columns_old_2(df, columns_to_check):\n\t    invalid_columns = {}\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns[col] = invalid_values\n\t    if not invalid_columns:\n\t        return None\n\t    else:\n\t        return str(invalid_columns) + \" has non-numeric values\"\n\t\n\tdef validate_numeric_columns(df, columns_to_check):\n\t    invalid_columns = []\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            # invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t    if not invalid_columns:\n\t        return None\n\t    # else:\n\t    #     return str(invalid_columns) + \" has non-numeric values\"\n\t    elif len(invalid_columns) == 1:\n\t        return f\"The column '{invalid_columns[0]}' contains non-numeric values.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_columns)\n\t        return f\"The columns {cols_str} contain non-numeric values.\"\n\t\n\t# Columns you want to check\n\tcolumns_to_check = [\n\t\n\t'Excess: AD', \n\t'Excess: Fire', \n\t'Excess: Theft', \n\t'Excess: WS',\n\t\n\t'Incurreds - Paid: AD&WS',\n\t'Incurreds - Paid: FT', \n\t'Incurreds - Paid: TP',\n\t'Incurreds - Outstanding: AD&WS',\n\t'Incurreds - Outstanding: FT',\n\t'Incurreds - Outstanding: TP', \n\t'Total Incurred Paid +  Outstanding',\n\t\n\t'Vehicle Years Earned',\n\t'Claim Count: All',\n\t]\n\t\n\t\n\ttry:\n\t    try:\n\t        CCE_Table = json.loads(str(CCE_Table).strip())\n\t    except:\n\t        try:\n\t            CCE_Table = eval(str(CCE_Table).strip())\n\t        except:\n\t            return None\n\t    CCE_Table = convert_to_df( CCE_Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t\n\tresult = validate_numeric_columns( CCE_Table, columns_to_check )\n\tprint(result)\n\treturn result",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360126000",
        "name": "numeric_validations_for_CCE_Table",
        "udf_type": "VALIDATIONS"
    },
    "17221": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "CCE_Table",
                "value": "CCE Table"
            }
        ],
        "code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Policy Year Start Date',\n\t  'Policy Year End Date'\n\t]\n\tTable = CCE_Table\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360126000",
        "name": "data_validations",
        "udf_type": "VALIDATIONS"
    },
    "17222": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "CCE_Table",
                "value": "CCE Table"
            }
        ],
        "code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Policy Year Start Date',\n\t  'Policy Year End Date'\n\t]\n\tTable = CCE_Table\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360127000",
        "name": "date_validations",
        "udf_type": "VALIDATIONS"
    },
    "17223": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Vehicle_Schedule_Table",
                "value": "Vehicle Schedule Table"
            }
        ],
        "code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Effective From',\n\t  'Effective To'\n\t]\n\tTable = Vehicle_Schedule_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360127000",
        "name": "date_validations",
        "udf_type": "VALIDATIONS"
    },
    "17224": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Date_Established",
                "value": "Date Established"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Date_Established:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Date_Established = str(Date_Established).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\t\n\t\tif contains_only_quotes(Date_Established):\n\t\t\treturn None\n\t\tif not isinstance(Date_Established,str):\n\t\t\treturn \"Date Established must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Date_Established):\n\t\t\treturn \"Date Established must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Date_Established,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360127000",
        "name": "validate_date_established",
        "udf_type": "VALIDATIONS"
    },
    "17225": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Vehicle_Schedule_Table",
                "value": "Vehicle Schedule Table"
            }
        ],
        "code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t    \n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Effective From',\n\t  'Effective To'\n\t]\n\tTable = Vehicle_Schedule_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360128000",
        "name": "date_validations",
        "udf_type": "VALIDATIONS"
    },
    "17226": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Driver_Party_Table",
                "value": "Driver Party Table"
            }
        ],
        "code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Driver D.O.B',\n\t]\n\tTable = Driver_Party_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360128000",
        "name": "date_validations",
        "udf_type": "VALIDATIONS"
    },
    "17227": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Driver_Party_Table",
                "value": "Driver Party Table"
            }
        ],
        "code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    # Beautify the list of columns\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Driver D.O.B',\n\t]\n\tTable = Driver_Party_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360128000",
        "name": "date_validations",
        "udf_type": "VALIDATIONS"
    },
    "17228": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "CCE_Table",
                "value": "CCE Table"
            }
        ],
        "code": "\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_numeric_columns_old(df, columns_to_check):\n\t\n\t    invalid_columns = []\n\t    \n\t    for col in columns_to_check:\n\t        # Treat empty strings as NaN\n\t        # series = df[col].replace(\"\", np.nan)\n\t        \n\t        # Convert to numeric (non-numeric -> NaN)\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        \n\t        # Compare only non-empty rows\n\t        mask_non_empty = series.notna()\n\t        non_numeric_mask = mask_non_empty & numeric_check.isna()\n\t\n\t        if non_numeric_mask.any():\n\t            # Store non-numeric values for this column\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t\n\t    # print(invalid_columns)\n\t\n\t    # Return None if all columns are valid\n\t    if len(invalid_columns)==0:\n\t        return None\n\t    else:\n\t        print(str(invalid_columns) + \" has non numeric columns\")\n\t        return str(invalid_columns) + \"has non numeric columns\"\n\t\n\t  \n\tdef validate_numeric_columns_old(df, columns_to_check):\n\t    invalid_columns = {}\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns[col] = invalid_values\n\t    if not invalid_columns:\n\t        return None\n\t    else:\n\t        return str(invalid_columns) + \" has non-numeric values\"\n\t\n\tdef validate_numeric_columns(df, columns_to_check):\n\t    invalid_columns = []\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            # invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t    if not invalid_columns:\n\t        return None\n\t    # else:\n\t    #     return str(invalid_columns) + \" has non-numeric values\"\n\t    elif len(invalid_columns) == 1:\n\t        return f\"The column '{invalid_columns[0]}' contains non-numeric values.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_columns)\n\t        return f\"The columns {cols_str} contain non-numeric values.\"\n\t\n\t\n\t# Columns you want to check\n\tcolumns_to_check = [\n\t\n\t'Excess: AD', \n\t'Excess: Fire', \n\t'Excess: Theft', \n\t'Excess: WS',\n\t\n\t'Incurreds - Paid: AD&WS',\n\t'Incurreds - Paid: FT', \n\t'Incurreds - Paid: TP',\n\t'Incurreds - Outstanding: AD&WS',\n\t'Incurreds - Outstanding: FT',\n\t'Incurreds - Outstanding: TP', \n\t'Total Incurred Paid +  Outstanding',\n\t\n\t'Vehicle Years Earned',\n\t'Claim Count: All',\n\t]\n\t\n\t\n\ttry:\n\t    try:\n\t        CCE_Table = json.loads(str(CCE_Table).strip())\n\t    except:\n\t        try:\n\t            CCE_Table = eval(str(CCE_Table).strip())\n\t        except:\n\t            return None\n\t    CCE_Table = convert_to_df( CCE_Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t\n\tresult = validate_numeric_columns( CCE_Table, columns_to_check )\n\tprint(result)\n\treturn result",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360129000",
        "name": "numeric_validations_for_CCE_Table",
        "udf_type": "VALIDATIONS"
    },
    "17229": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Risk_Postcode",
                "value": "Risk Postcode"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360129000",
        "name": "validate_post_code",
        "udf_type": "VALIDATIONS"
    },
    "17230": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Risk_Postcode",
                "value": "Risk Postcode"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360129000",
        "name": "validate_post_code",
        "udf_type": "VALIDATIONS"
    },
    "17231": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Risk_Postcode",
                "value": "Risk Postcode"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360129000",
        "name": "validate_post_code",
        "udf_type": "VALIDATIONS"
    },
    "17232": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Risk_Postcode",
                "value": "Risk Postcode"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360130000",
        "name": "validate_post_code",
        "udf_type": "VALIDATIONS"
    },
    "17233": {
        "args": [
            {
                "data_type": "FIELD",
                "name": "Incepts_On",
                "value": "Incepts On"
            }
        ],
        "code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Incepts_On:\n\t\ttry:\n\t\t\tIncepts_On = str(Incepts_On).strip()\n\t\texcept:\n\t\t\treturn None \n\t\t# Return None to indicate validation passed\n\t\t# Incepts_On = Incepts_On.strip()\n\t\tif contains_only_quotes(Incepts_On):\n\t\t\treturn None\n\t\tif not isinstance(Incepts_On,str):\n\t\t\treturn \"Incepts On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Incepts_On):\n\t\t\treturn \"Incepts On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Incepts_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
        "docstring": null,
        "lambda_end_of_life": null,
        "lambda_id": null,
        "lambda_udf_id": null,
        "last_updated_at": "1756360130000",
        "name": "validate_incepts_on",
        "udf_type": "VALIDATIONS"
    }
}