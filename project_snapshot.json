{
    "project_data": {
        "createdAtMs": "1756360072000",
        "creationBase": "PROJECT",
        "dataMountPath": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98",
        "description": "",
        "extractionMode": "CLASSIFICATION",
        "id": "0198ef37-d998-7864-878d-539c9c7ddd98",
        "llm": "gpt-3.5-turbo",
        "name": "Fleet Master Build Aug28 - V0.0.12",
        "owner": "eae40c82-c4f6-41b8-a4f7-15e321256cf6",
        "ownerPublicId": "balusa.prasad.ctr_instabase.com",
        "projectMountPath": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98",
        "repoId": "f23b2ea0-089c-4bd8-981f-95c4fb9b71de",
        "schemaVersion": "1",
        "settings": {
            "advanced_ocr": {
                "isChangeLanguageType": false,
                "isSelectCheckboxes": false,
                "isSelectFileAsImage": false,
                "isSelectTables": true,
                "languageType": "standard",
                "nativeExcelProcessing": true,
                "pagesText": "",
                "splitByPage": false,
                "version": "v1"
            }
        },
        "type": "BUILD",
        "updatedAtMs": "1756360072000",
        "workspace": "dev-si-fleet"
    },
    "project_schema": {
        "case_fields": [],
        "classes": {
            "Broker Presentation": {
                "class_id": "13544",
                "description": "Classify the document as a Broker Presentation if it is a formal, polished report prepared by an insurance broker for underwriting or renewal. It often includes: **Underwriting reports**, \"Fleet rated commercial motor\", \"Motor fleet factfinder\" documents also if they include executive summaries, insured\u2019s business overview, coverage terms, renewal requests, claims history or \u201cClaims Experience,\u201d client details, and general information. Look for professionally formatted tables, structured sections, broker branding, and especially the presence of the word \u201cPresentation\u201d, \"Market Presentation\" or phrases like \u201cPrepared for Underwriters.\u201d\nDo NOT classify as Broker Presentation if the document is a questionnaire or form (e.g., contains multiple questions, blank fields, or checkboxes), is conversational or resembles an email (e.g., \u201cFrom:\u201d, \u201cHi\u201d, \u201cRegards\u201d), or is purely for data collection like Fact Finders.\nEmphasize: Presentation-style, summary reports created by brokers\u2014not informal messages or raw data forms.",
                "fields": [
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Target Price \u2014 the premium or price the client or broker expects to pay for the insurance policy. sometimes it is implied from previous year\u2019s premium. Just return target price,\nIf no relevant price reference is available, return \"N/A\"",
                        "field_id": "193456",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Extract only the final target premium value.\nIf no valid target premium value is found, return \"\".\nDo not include any label text like \"target premium\" or \"client is looking for\".\nReturn only the clean value."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Target Price@1"
                                    }
                                ],
                                "function_id": 17104,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Target Price",
                        "position": 0,
                        "prompt": "Step 1 \u2013 Identify Base Premium:\nLook for phrases like \u201crenewal terms received\u201d, \u201ctarget at the moment\u201d, \u201cwe would like terms\u201d, \u201clooking for a quote\u201d \u2192 mark as Target_Premium.\n\nstrictly Ignore values with \u201clast year was\u201d, \u201cexpiry premium\u201d,\"last year paid\", \u201ccurrently paying\u201d unless a % change is also mentioned.\n\nIf % change (e.g., \u201c10% increase\u201d) is mentioned with a past premium \u2192 calculate: base \u00b1 % change in Step3\n\nIf two premiums from different insurers \u2192 use the lower one.\nIf no valid target \u2192 return \" \".\n\nStep 2 \u2013 Adjust for IPT:\nIf \u201cincluding IPT\u201d \u2192 divide by 1.12.\nIf \u201c+ IPT\u201d, \u201cexcluding IPT\u201d, or \u201cplus IPT\u201d \u2192 use as-is.\nIgnore commission.\n\nStep 3 \u2013 Apply % Change:\nIf phrases like \u201c10% increase\u201d or \"additional premium\" or \u201cindication is\u2026\u201d appear \u2192 apply change.\n\nOutput:\nCalculated Target Premium: \u00a3[value]\n\nlets think step by step",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Identify the Agency Enquiry Reference from the broker presentation, which is a code or ID used for tracking the insurance submission. Return only the value. If unavailable, return 'N/A'.",
                        "field_id": "193457",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "DEFAULT",
                        "name": "Agency Enquiry Reference",
                        "position": 1,
                        "prompt": "Task: Extract the Agency Enquiry Reference number from the given document.\n\nFollow these Instructions carefully to extract the Agency Enquiry Reference number:\n\n#Instructions:\n\n1. Read the broker presentation carefully.\n\n2. Look for a value labeled \"Agency Enquiry Reference\" or anything similar (e.g., \"Agency Ref\", \"Enquiry ID\", \"Reference Code\", \"Request for quotation\").\n\n3. This is usually a code or ID used to track the insurance submission.\n\n4. If you find it, extract only the value (not the label or surrounding text).\n\n5. If it\u2019s not available anywhere in the document, return an empty string (\"\").",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Identify the Broker Deadline from the broker presentation. This refers to the latest date by which the broker expects a quote or response. Return only the value. If unavailable, return 'N/A'.",
                        "field_id": "193458",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Just return only date without extra information"
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Broker Deadline@1"
                                    }
                                ],
                                "function_id": 17105,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Broker Deadline",
                        "position": 2,
                        "prompt": "Extract the Broker Deadline from the email content.\u2028This refers to the date by which a quote is expected from the insurer.\n\nInstructions\nStep 1: Search for relevant deadline phrases\nIdentify dates associated with the following phrases:\nQuote required by\nDeadline\nTarget date\nQuote deadline\nQuote expected by\nNeeded by\nTerms no later than\n\nStep 2: Handle date formats\nIf day, month, and year are present \u2192 return in dd/mm/yyyy format.\u2028Example: \u201cQuote required by 10 January 2025\u201d \u2192 10/01/2025\nIf only day and month are present (no year) \u2192 return in dd/mm format.\u2028Example: \u201cQuote expected by 19 Jan\u201d \u2192 19/01\nDo not infer the year from the email\u2019s received date.\n\nStep 3: Exclude irrelevant dates\nIgnore dates associated with:\nPolicy renewal\nEffective\nExpiry\nDue on\n\nOutput Format\nBroker Deadline Date : <deadline date or \"\">\nFull date: Broker Deadline Date : 10/01/2025\nPartial date: Broker Deadline Date : 10/01\nNo valid date: Broker Deadline Date : \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193459",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17106,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Product",
                        "position": 3,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the name of the insured, which may be referred to as the policyholder, client name, company name, full client name, or trading name. Return only the name of the person or business insured. If unavailable, return 'N/A'.",
                        "field_id": "193460",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "DEFAULT",
                        "name": "Insured",
                        "position": 4,
                        "prompt": "Extract the name of the insured, which may be referred to as the policyholder, client name, company name, full client name, or trading name. Return only the name of the person or business insured. If unavailable, return ''.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Business Description of the company from the broker presentation. Look for sections titled 'Company Details,' 'General Information,' or similar. Return only the content that explains **Business Description** the company's activities or business nature. If no relevant information is found, return:\nBusiness Description: \"N/A\"\nOtherwise, return in the following format:\nBusiness Description: \"<extracted text>\"",
                        "field_id": "193461",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean **Business Description**.\n\n- Preserve the complete business activity description as written.\n- Remove any preceding labels such as \"Trade:\", \"Business Description:\", or \"Nature of Business:\"\n- Remove trailing punctuation, line breaks, or filler text.\n\nIf no business description is found, return \"\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Business Description",
                        "position": 5,
                        "prompt": "Extract the Business Description of the company from the broker presentation. Look for sections titled **Company Details** , **General Information** or similar. Return only the content that explains **Business Description** or  company's activities or business nature. If no relevant information is found, return:\nBusiness Description: \"\"\nOtherwise, return in the following format:\nBusiness Description: \"<extracted text>\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Agency Name from the broker presentation document.\n\nInstructions:\n\nLook for the name of the brokerage firm or agency responsible for the presentation.\n\nIt is usually found:\n\nOn the cover page, header, or footer of the presentation.\n\nNear the broker contact information or under sections like \u201cOur Team\u201d, \u201cBroker Details\u201d, or \u201cPresented by\u201d.\n\nAlongside the agency logo or in the email signature block if present.\n\nThe agency name is usually a company name (e.g., Marsh Ltd., Gallagher Insurance, Towergate Partnership).\n\nDo not return personal names of broker representatives.\n\nOutput:\nReturn the full Agency Name as a string. If no valid agency name is found, return \"N/A\"",
                        "field_id": "193462",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean **Agency Name** or **Broker Name**.\n\nIf no agency or broker name is found, return \"\"."
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Agency Name",
                        "position": 6,
                        "prompt": "Find the Agency Name in the broker presentation document.\n\nSteps:\n\nStep 1: Look for any prominently displayed branding, logos, or company names on the cover page or header.If an agency name is identified, extract it in the format <Agency Name> : <Value>.\n\nStep 2: If the Agency Name not found in Step 1, identify the  risk management or Insurance brokers or Insurer  that prepared the document. And then Extract Broker name in this format <Broker Name>: <Value>\n\nStep 3: Do not return the client or insured's name.\n\nLet's think step by step",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Company House Reference of the insured. This is a unique identifier assigned by Companies House in the UK \u2014 typically an 8-character code, though it can be shorter. Return only value, If no such reference is found , return \"\"N/A\"\"\"",
                        "field_id": "193463",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Just return the final output company registration number of the insured, otherwise return an empty string: \"\" without any additional text."
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Company House Reference Party",
                        "position": 7,
                        "prompt": "You are expert in analysing vehicle insurance data. Your task is to extract the **company registration** number of the insured/client from the Broker presentation document.\n\nInstructions:\n\nStep-1 : Extract the company registration number of the insured/client, which is typically 8 characters long but can be fewer.\n\nStep-2 : Look for key phrases indicating a registration number, such as:\n\n\u201cCompany Number\u201d\n\n\u201cCompany Registration Number\u201d\n\n\u201cRegistered Number\u201d\n\nor any similar variation\n\nExclusions & Constraints:\n\nStep-3 : Do not extract the company number found in the broker's details or footer.\n\nStep-4 : Only extract the number if it refers specifically to the insured/client.\n\nStep-5 : If no such registration number is found for the client, return an empty string: \"\".\n\nJust return the final output company registration number of the insured, otherwise return an empty string: \"\" without any additional text.\n\nLet's think step by step.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Date Established of the insured \u2014 this refers to the year or full date when the business was founded or incorporated. Return only the value,\nIf the establishment date cannot be found, return \"\"N/A\"\".",
                        "field_id": "193464",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "You are an expert in document analysis and date extraction.\nYour task is to extract and normalize a date from the given text.\nInstructions:\nInterpret all date formats strictly as day/month/year (DD/MM/YYYY), regardless of how the date appears in the text. The first number is always the day, second is the month, third is the year.\nNormalize and return the date in the format: DD/MM/YYYY (zero-padded).\nIf only a year is present (e.g. \"2020\"), return it as 01/01/YYYY.\nIf a month and year are present (e.g. \"March 2020\" or \"03/2020\"), return it as 01/MM/YYYY.\nIf a full day, month, and year are present, return the exact date in DD/MM/YYYY format.\nReturn only the extracted value (no label or explanation in output).\nIf no valid establishment date is found, return exactly: \"\" (empty string, without quotes).\nExamples:\nExample 1:\nText: The business was founded in 2021.\nOutput: 01/01/2021\nExample 2:\nText: Established: March 2020.\nOutput: 01/03/2020\nExample 3:\nText: Policy effective from 5/11/2023 for one year.\nOutput: 05/11/2023\nExample 4:\nText: Driver's licence issue date: 14-6-90, valid for 10 years.\nOutput: 14/06/1990\nExample 5:\nText: Vehicle registration expires on 01.12.2025.\nOutput: 01/12/2025\nExample 6:\nText: Renewal date: \"\"\nOutput: \"\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Date Established",
                        "position": 8,
                        "prompt": "Extract the Date Established of the insured \u2014 this refers to the year or full date when the business was founded or incorporated. Look for similar phrases such as:\n\nYear business established\nEstablished Year\n\nReturn only the extracted value.\nIf no establishment date is found, return \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Insured Address from the document.\n\nIt is usually found under the Client Details and General Information or Client Profile section.\n\nIgnore other addresses like broker, branch, or claimant addresses.\n\nReturn the full address as one line. If not found, return \"N/A\"",
                        "field_id": "193465",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Clean and return **only the address details**. This means:\n- Preserve the physical address lines.\n- Format it as a single clean string separated by commas.\n- If no valid address is present, return \"\"."
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address",
                        "position": 9,
                        "prompt": "Extract the Party Address (also known as the Insured Address/ Correspondence Address / Trading Address) from the document.\n\nInstructions:\nIt is often found under the below section:\n**Client Details and General Information**, **Client Profile**, **Company Details**\n\nIgnore unrelated addresses (e.g., branch, broker, or claimant addresses).\n\nReturn the complete address exactly as a string from the document, without omitting or reformatting any part.\n\nOutput:\n\nReturn the full Party Address as a string. If no valid address is found, return \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Inception Date \u2014 the date the insurance policy is set to begin.\nIt may also be referred to as \"Effective Date\", \"Start Date\", or \"Incepts On\".\nDo not return the Renewal Date.\nIf no relevant start date is found, return \"N/A\".",
                        "field_id": "193466",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Check Format: If date is already in the dd/mm/yyyy format, return it as is.\nStandardize Date: If the date is in a different valid format (e.g., yyyy-mm-dd, mm/dd/yyyy, dd-mm-yyyy, etc.), convert it to dd/mm/yyyy. return cleaned date without any extra text and quotations.\nInvalid Dates: If input_date is missing, empty, or not a valid date, return an empty string \"\""
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Incepts On",
                        "position": 10,
                        "prompt": "The inception date may be referred to (case-insensitive) using the following labels only , followed by a date:\nPolicy Inception Date, Inception Date, Effective Date, Start Date, Period of Insurance, Renewal Date, Renewal On, or Incepts On.\n\nApply the following rules strictly:\n\nDo not extract any general or standalone date \u2014 only extract dates that immediately follow a valid label\n\nIf multiple \"Period of Insurance\" date ranges appear, extract the start date from the range that is nearest to the heading **\"Motor Fleet\"** (or a similar heading). If no such heading is found, default to the latest date range.\n\nIf multiple valid labels are present prioritize the Policy Inception Date.\nExtract only one date per document \u2014 the most relevant based on the above priority.\n\nDo not extract dates labeled as Survey Date, Quotation Date, Deadline Date, Next Renewal Date, Policy Expiry Date, or any general Date without a valid label.\n\nIf the date appears without a year (e.g., only DD/MM), return it in DD/MM format only with out extra text.\n\nIf the date includes day, month, and year, return in DD/MM/YYYY ormat only with out extra text.\n\nIf no valid inception-related date is found, return an empty string: \"\".",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193467",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Incepts_On",
                                        "value": "Incepts On"
                                    }
                                ],
                                "function_id": 17107,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Expires On",
                        "position": 11,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Main Cover Type from the input. This refers to the level of motor insurance cover requested or provided (e.g., 'Comprehensive', 'Third Party Only', 'TPFT'). This information is usually found under headers like 'Cover Basis', 'Cover', or similar. Return only the cover type. If no valid cover type is found, return 'N/A'",
                        "field_id": "193468",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "If excess values are given for each vehicle group, return the most common cover type value.\n\nIf excess values are given for different periods, return the most recent period cover value .\n\nIf a single value is extracted (not linked to group or period), return just the cover type ."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Main Cover Type@1"
                                    }
                                ],
                                "function_id": 17108,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Main Cover Type",
                        "position": 12,
                        "prompt": "Extract the Cover basis (also referred to as Cover Type) from the document using the following rules:\n\n**Valid_cover_types** includes: \n- \"Comprehensive(COMP)\" (also treat \"Comp\", \"COMP\", \"Comprehensive\" as equivalent to this)\n- \"Third Party Fire and Theft(TPFT)\" (include variations like \"TPFT\", \"Third Party Fire & Theft\")\n- \"Third Party Only(TPO)\" (include \"TPO\", \"Third Party\")\n- \"Laid up ADFT\"\n- \"Laid up FT\"\n\nIf a \"Claims Experience\" section is found, extract the Cover Type from the most recent period mentioned in that section.\n\u2023 Format: <period range>:<cover type>\n\nIf the section is not found, but cover types are listed for each vehicle group, extract them.\n\u2023 Format: <vehicle group>:<cover type> (comma-separated if multiple)\n\nIf neither is found, but a standalone Cover Type is mentioned (not in a section or group), extract the most recent or most relevant one.\n\u2023 Format: AD Cover:<cover type>\n\nReturn only the final output. No explanations.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193469",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address",
                                        "value": "Party Address"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "insured_address: {{ \\ Party Address \\}}\n\nExtract the post code from the insured_address. The post code is typically an alphanumeric value located at the end of the address.\n\nIf no post code is found, return \"\"."
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Risk Postcode",
                        "position": 13,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Accidental Damage Excess value (also referred to as ADFT or AD) from the document using the following rules:\n\nIf a \"Claims Experience\" section is found, extract the Accidental Damage Excess value from the most recent period mentioned in that section.\n\u2023 Format: <period range>:<excess value>\n\nIf the section is not found, but excess values are listed for each vehicle group, extract them.\n\u2023 Format: <vehicle group>:<excess value> (comma-separated if multiple)\n\nIf neither is found, but a standalone Accidental Damage Excess value is mentioned (not in a section or group), extract the most recent or most relevant one.\n\u2023 Format: AD Excess:<excess value>\n\n\nReturn only the final output. No explanations.",
                        "field_id": "193470",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "If excess values are given for each vehicle group, return the most common value as:\nExcess Value: <value>\n\nIf excess values are given for different periods, return the most recent period and its value as:\n<period range>:<value>\n\nIf a single excess value is extracted (not linked to group or period), return it directly as:\nExcess Value: <value>"
                            },
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Just return the numerical value if present, otherwise return \"0\""
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Excess Type Accident Damage",
                        "position": 14,
                        "prompt": "Extract the Accidental Damage Excess value (also referred to as ADFT or AD) from the document using the following steps: \n\nSteps :\nStep 1: Check for accidental damage excess values, also known as ADFT or AD, which are listed for each vehicle group. Extract them in this format: <vehicle group>:<excess value> (comma-separated if there are multiple).\n\nStep 2: If excess values not found in Step 1, but a standalone Accidental Damage Excess value is mentioned (not in a section or group), extract the most recent or most relevant one in this Format: AD Excess:<excess value>\n\nLet's think step by step",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Fire & theft excess (\u00a3)  value (also referred to as ADFT or FT) from the document using the following rules:\n\nIf a \"Claims Experience\" section is found, extract the Fire & theft excess Excess value from the most recent period mentioned in that section.\n\u2023 Format: <period range>:<excess value>\n\nIf the section is not found, but Fire & theft excess values are listed for each vehicle group, extract them.\n\u2023 Format: <vehicle group>:<excess value> (comma-separated if multiple)\n\nIf neither is found, but a standalone Fire & theft excess value is mentioned (not in a section or group), extract the most recent or most relevant one.\n\u2023 Format: FT Excess:<excess value>\n\n\nReturn only the final output. No explanations.",
                        "field_id": "193471",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "If excess values are given for each vehicle group, return the most common value as:\nExcess Value: <value>\n\nIf excess values are given for different periods, return the most recent period and its value as:\n<period range>:<value>\n\nIf a single excess value is extracted (not linked to group or period), return it directly as:\nExcess Value: <value>"
                            },
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Just return the numerical value if present, otherwise return \"0\""
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Excess Type Fire",
                        "position": 15,
                        "prompt": "Extract the Fire excess (\u00a3) value (also referred to as ADFT or FT) from the document using the following steps: \n\nSteps :\nStep 1: Check for Fire & theft excess values (also referred to as ADFT or FT) which are listed for each vehicle group, extract them in this Format: <vehicle group>:<excess value> (comma-separated if multiple)\n\nStep 2:  If excess values not found in Step 1, but a standalone Fire & theft excess value (also referred to as ADFT or FT) is mentioned (not in a section or group), extract the most recent or most relevant one in this Format: FT Excess:<excess value>\n\nLet's think step by step",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Theft excess (\u00a3)  value (also referred to as ADFT or FT) from the document using the following rules:\n\nIf a \"Claims Experience\" section is found, extract the Fire & theft excess Excess value from the most recent period mentioned in that section.\n\u2023 Format: <period range>:<excess value>\n\nIf the section is not found, but Fire & Theft excess  values are listed for each vehicle group, extract them.\n\u2023 Format: <vehicle group>:<excess value> (comma-separated if multiple)\n\nIf neither is found, but a standalone Theft Excess value is mentioned (not in a section or group), extract the most recent or most relevant one.\n\u2023 Format: Theft Excess:<excess value>\n\n\nReturn only the final output. No explanations.",
                        "field_id": "193472",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "If excess values are given for each vehicle group, return the most common value as:\nExcess Value: <value>\n\nIf excess values are given for different periods, return the most recent period and its value as:\n<period range>:<value>\n\nIf a single excess value is extracted (not linked to group or period), return it directly as:\nExcess Value: <value>"
                            },
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Just return the numerical value if present, otherwise return \"0\""
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Excess Type Theft",
                        "position": 16,
                        "prompt": "Extract the Theft excess (\u00a3) value (also referred to as ADFT or FT) from the document using the following steps: \n\nSteps :\nStep 1: Check for Fire & theft excess values (also referred to as ADFT or FT) which are listed for each vehicle group, extract them in this Format: <vehicle group>:<excess value> (comma-separated if multiple)\n\nStep 2:  If excess values not found in Step 1, but a standalone Fire & theft excess value (also referred to as ADFT or FT) is mentioned (not in a section or group), extract the most recent or most relevant one in this Format: FT Excess:<excess value>\n\nLet's think step by step",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the **Windscreen excess (\u00a3)** value (also referred to as ADFTWS or WS) from the document using the following rules:\n\nIf a \"Claims Experience\" section is found, extract the Windscreen excess value from the most recent period mentioned in that section.\n\u2023 Format: <period range>:<excess value>\n\nIf the section is not found, but Windscreen excess values are listed for each vehicle group, extract them.\n\u2023 Format: <vehicle group>:<excess value> (comma-separated if multiple)\n\nIf neither is found, but a standalone Windscreen Excess value is mentioned (not in a section or group), extract the most recent or most relevant one.\n\u2023 Format:WS Excess:<excess value>\n\n\nReturn only the final output. No explanations.",
                        "field_id": "193473",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "If excess values are given for each vehicle group, return the most common value as:\nExcess Value: <value>\n\nIf excess values are given for different periods, return the most recent period and its value as:\n<period range>:<value>\n\nIf a single excess value is extracted (not linked to group or period), return it directly as:\nExcess Value: <value>"
                            },
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Just return the numerical value if present, otherwise return \"0\""
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Excess Type WS",
                        "position": 17,
                        "prompt": "Extract the **Windscreen excess (\u00a3)** value (also referred to as ADFTWS or WS or Glass Excess) from the document using the following steps:\n\nSteps :\nStep 1: Check for Windscreen excess values (also referred to as ADFTWS or WS or Glass Excess) which are listed for each vehicle group, extract them in this Format: <vehicle group>:<excess value> (comma-separated if multiple)\n\nStep 2:  If excess values not found in Step 1, but a standalone Windscreen Excess value (also referred to as ADFTWS or WS or Glass Excess) is mentioned (not in a section or group), extract the most recent or most relevant one in this Format:WS Excess:<excess value>\n\nLet's think step by step",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Agency Address, which refers to the full address of the insurance agency or broker involved in the policy.\nIf no address details are found, return \"N/A\".",
                        "field_id": "193474",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Clean and return **only the address details**. This means:\n- Preserve the physical address lines.\n- Format it as a single clean string separated by commas.\n- If no valid address is present, return \"\"."
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Agency address",
                        "position": 18,
                        "prompt": "You are an expert in Insurance underwriting. Your task is to extract the correct Agency Address from the OCR-processed Broker Presentation.\n\nFollow the instructions carefully:\n\n1) The Agency Address refers to the address of the insurance broker, intermediary, or agent responsible for managing or placing the insurance policy.\n\n2) This address is typically associated with organisations such as Gallagher, UBT, Arthur J. Gallagher, etc.\n\n3) Look for addresses that are directly linked to broker personnel (e.g., account managers, account directors, insurance advisors) or company names known to be brokers/intermediaries.\n\n4) Also consider any address that follows the keywords \"Produced by\" or \"Prepared by\" on the first page of the document.\n\n5) Exclude all addresses related to the insured party. For clarity, this includes any address listed as or associated with:\n\nPolicyholder\n\nClient (business or trading)\n\nInsured\n\nCorrespondence\n\nReturn the complete address exactly as it appears in the document, without omitting or reformatting any part.\n\nIf no agency/broker address is identified, return an empty string: \"\"\n\nOutput Format:\nA single string containing the agency\u2019s physical address, or \"\" if not found.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193475",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Incepts_On",
                                        "value": "Incepts On"
                                    }
                                ],
                                "function_id": 17109,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Effective From",
                        "position": 19,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193476",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Expires_On",
                                        "value": "Expires On"
                                    }
                                ],
                                "function_id": 17110,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Effective To",
                        "position": 20,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the value associated with \"Holding Broker\" from the given text. Only return the value if it is explicitly labeled as \"Holding Broker\" (case-insensitive). Do not infer or guess the broker name if the label is not clearly mentioned. Return \"N/A\" if \"Holding Broker\" is not found",
                        "field_id": "193477",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Holding Broker@0"
                                    }
                                ],
                                "function_id": 17111,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Holding Broker",
                        "position": 21,
                        "prompt": "Task: Extract the value associated with the Holding Broker from the given text.\n\nRules:\n1. Only extract the value if it is explicitly labeled as one of the following (case-insensitive):\n   - \"Holding Broker\"\n   - \"Current Broker\"\n\n2. Return the value exactly as written after the label (e.g., after \"Holding Broker:\").\n\n3. Do not infer, guess, or extract any value unless one of the exact labels above is found.\n\n4. If none of the specified labels are present, return an empty string \"\".\n\n5. Do not Extract Holding Broker from the label like Broker name, Holding Insurers.\n\nOutput format:\n\nAlways return result as JSON:\n\n{\n  \"holding_broker\": \"<name or blank>\",\n  \"source_text\": \"<the phrase that triggered your decision>\"\n}",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193478",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Agency_Name",
                                        "value": "Agency Name"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Holding_Broker",
                                        "value": "Holding Broker"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "holding_broker: \\ Holding Broker \\\nagency_name: \\ Agency Name \\\n\nComparare `holding_broker` and `agency_name` to determine if they represent the same organization.\n\nRules:\n1. If `holding_broker` is an empty string, return: {\"holding_broker\": \"\", \"basis\": \"Holding broker is empty\"}.\n\n2. If the two names refer to the same or a similar entity \u2014 for example, when they differ only by abbreviations, suffix variations (e.g., \"Ltd\" vs \"Limited\", \"Corp\" vs \"Corporation\"), or extra generic words (e.g., \"Group\", \"Financial\") \u2014 return:\n{\"holding_broker\": \"Yes\", \"basis\": \"Names are similar after normalization\"}\n\n3. If they are different entities, return: {\"holding_broker\": \"No\", \"basis\": \"Names are different after normalization\"}.\n\nSteps:\n- Normalize both names: lowercase, remove legal suffixes (like ltd, corp, inc), and special characters.\n- Then compare using fuzzy similarity or normalized string match.\n\nReturn output strictly in JSON format with keys:\n- `holding_broker`: \"Yes\", \"No\", or \"\"\n- `basis`: explanation of the decision"
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Is Holding Broker@0"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Holding_Broker",
                                        "value": "Holding Broker"
                                    }
                                ],
                                "function_id": 17112,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Is Holding Broker",
                        "position": 22,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "OBJECT_LIST",
                        "description": "Your task is to scan every page of the Broker presentation thoroughly and extract all driver-related details or related terms like Primary Driver, Secondary Driver, Additional Driver, Listed Driver, Young Driver, Convicted Driver etc.\n\nAnalyze the details under all headers sections and extract all the **Driver** related details.\n\nInstructions:\nStep 1: Identify Driver Sections\n-->Analyze the context under each heading and check if driver related information(Ex : Driver names, Driver claims) is present.\nStep 2: Extract the following information for each driver\n-->Driver Name \u2013 Full name of the driver (Ex:'Williams').\n-->Driver Date of Birth (DOB) \u2013 The driver's date of birth in the format it appears.\n-->Driver Licence Date \u2013 Date when the license obtained. This is not the same as \"Date of conviction\".\n-->Driver Claims - Extract the claim for the driver if mentioned.\nStep 3: Match and Group Details\nEnsure that for every driver found, the corresponding Name, DOB, and Licence Date are correctly grouped.\nLet's think step by step.",
                        "field_id": "193479",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Driver Details@0"
                                    }
                                ],
                                "function_id": 17113,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Driver Details",
                        "position": 24,
                        "prompt": "Driver Details",
                        "prompt_schema": [
                            {
                                "description": "Extract only the valid driver names, do not extract irrelevant text as driver's name. Extract all driver names from the entire document.",
                                "name": "Driver Name"
                            },
                            {
                                "description": "-->If the date is already in dd/mm/yyyy format, keep it as is.\n-->If only the year is provided (e.g., 1990), default to 01/01/yyyy.\n-->If month and year are provided (e.g., March 1990), default to 01/mm/yyyy.\n-->Ensure all output DOBs are returned in dd/mm/yyyy format.",
                                "name": "Driver DOB"
                            },
                            {
                                "description": "1. If the date is already in **dd/mm/yyyy** format, keep it unchanged.\n2. If only the **year** is given (e.g., \"1990\"), return the DOB as **01/01/1990**.\n3. If **month and year** are provided (e.g., \"March 1990\" or \"03/1990\"), return the DOB as **01/03/1990**.\n4. If the date is written in a textual format like \"2nd March 1990\" or \"March 2, 1990\", convert it to **02/03/1990**.\n5. Return all DOBs in **dd/mm/yyyy** format with **leading zeros** for day and month if needed.\n\nExamples:\n- \"1990\" \u2192 \"01/01/1990\"\n- \"March 1990\" \u2192 \"01/03/1990\"\n- \"2nd March 1990\" \u2192 \"02/03/1990\"\n- \"02/03/1990\" \u2192 \"02/03/1990\"\n- \"Mar 1990\" \u2192 \"01/03/1990\"",
                                "name": "Licence Date"
                            },
                            {
                                "description": "Extract all the conviction codes for each driver, such as 'SP-30', 'IN 10', or 'TS10' and give them as a comma separated list.",
                                "name": "Conviction Code"
                            },
                            {
                                "description": "Extract the claim details of the drivers if they are mentioned. Do not consider Reason for referral or notice as Driver claims.",
                                "name": "Driver Claims"
                            }
                        ],
                        "prompt_type": "basic"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193480",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Todays_Date",
                                        "value": "Todays Date"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Name_And_Driver_DOB",
                                        "value": "Driver Name And Driver DOB"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "You are given the  \\ Driver Name And Driver DOB \\ table containing two columns: 'Driver Name' and 'Driver DOB'.\n\nYour task is to calculate the age of each driver using the Driver DOB and \\ Todays Date \\\n\nThe age should be calculated in complete years.\nIf Driver DOB is missing, empty, or invalid, the corresponding Age must be left blank (empty string).\n\nReturn the output strictly as a table with exactly two columns:\nOutput Format:\n[[\"Driver Name\", \"Age\"],[],[]]\nStrictly return a **table formatted output** in a structured list of list format as specified above."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Driver Age@0"
                                    }
                                ],
                                "function_id": 17114,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Driver Age",
                        "position": 26,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193481",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17115,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Todays Date",
                        "position": 23,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193482",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Todays_Date",
                                        "value": "Todays Date"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Name_And_Licence_Date_Info",
                                        "value": "Driver Name And Licence Date Info"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "You are given the \\ Driver Name And Licence Date Info \\ table containing two columns: 'Driver Name' and 'Driver Licence date'.\n\nYour task is to calculate the Licence tenure of each driver using the \"Licence date\" and \"\\ Todays Date \\\"\n\nThe Licence Tenure should be calculated in complete years.\nIf Licence date is missing, empty, or invalid, the corresponding Licence Tenure must be left blank (empty string).\n\nReturn the output strictly as a table with exactly two columns:\nOutput Format:\n[[\"Driver Name\", \"Licence Tenure\"],[],[]]\nStrictly return a **table formatted output** in a structured list of list format as specified above."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Driver Licence Tenure@0"
                                    }
                                ],
                                "function_id": 17116,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Driver Licence Tenure",
                        "position": 28,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Scan each and every page of the document and extract cover basis type along with the alphanumeric vehicle registration numbers which includes unregistered vehicle numbers as well present under the vehicle group.\n\nOutput Format : \n\nEx : 1) Vehicle group Name : <Vehicle group Name>\n          -- Cover Basis : <Cover Basis Type>\n          -- Registration Numbers : <Alphanumeric                      registration numbers or N/A>",
                        "field_id": "193483",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "ADVANCED",
                        "name": "Vehicle Category Info and Registration Numbers",
                        "position": 32,
                        "prompt": "Your task is to thoroughly scan each and every page of the document and extract structured information for each vehicle group.\n\nEach vehicle group will have:\n\n--> A Vehicle Group Name\n--> A Cover Basis (e.g., Comprehensive, TPO, TPFT, etc.)\n--> One or more Vehicle Registration Numbers, which may include:\n        -->Alphanumeric registration numbers (e.g., \"TN01AB1234\", \"MH12XY9999\")\n        -->Unregistered vehicles (e.g., \u201cUNREGISTERED VEHICLE\u201d, \u201cTBA\u201d, \u201cN/A\u201d, etc.)\n        -->Include Serial/chassis number \n        -->Include the vehicle registration number for excluded drivers.\n--> Keep duplicate Vehicle Registration numbers as well. Do not change the order of Vehicle Registrations\u2014maintain the same order as in the document.\n\nOutput Format : \n\nEx : 1) Vehicle group Name : <Vehicle group Name>\n          -- Cover Basis : <Cover Basis or N/A>\n          -- Registration Numbers : <Alphanumeric registration numbers or N/A>",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193484",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Category_Info_and_Registration_Numbers",
                                        "value": "Vehicle Category Info and Registration Numbers"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Extract a table with two columns: ['Vehicle Registration', 'Cover Basis'] from \\ Vehicle Category Info and Registration Numbers \\\n\t1\tFind valid vehicle registration numbers and their cover types.\n\t2\tConvert all vehicle registration numbers to UPPERCASE. Keep Cover Basis as is.\n\t3\tIf cover type is missing for a valid reg number, use an empty string (\"\").\n\t4\tRemove rows where vehicle registration is: \"To be confirmed (TBC)\", \"TO BE ADVISED from\", \"UNREG\" or Similar meaning, or not a vehicle registration number,  \"N/A\", Empty (\"\").\n\t5\tIf 'Cover Basis' is \"N/A\", replace it with an empty string (\"\").\nReturn format:\u2028[[\"Vehicle Registration\", \"Cover Basis\"], [\"AB12 XYZ\", \"TPO\"]]\u2028If no valid rows, return [] only."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Vehicle Registration Numbers And Cover Basis@0"
                                    }
                                ],
                                "function_id": 17117,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Vehicle Registration Numbers And Cover Basis",
                        "position": 33,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193485",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Age",
                                        "value": "Driver Age"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Licence_Tenure",
                                        "value": "Driver Licence Tenure"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Details",
                                        "value": "Driver Details"
                                    }
                                ],
                                "function_id": 17118,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Driver Merged Data",
                        "position": 29,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193486",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Merged_Data",
                                        "value": "Driver Merged Data"
                                    }
                                ],
                                "function_id": 17119,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Driver Type",
                        "position": 30,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193487",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Registration_Numbers_And_Cover_Basis",
                                        "value": "Vehicle Registration Numbers And Cover Basis"
                                    }
                                ],
                                "function_id": 17120,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Unique_Values_Cover_Basis",
                        "position": 34,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193488",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Unique__Values__Cover__Basis",
                                        "value": "Unique_Values_Cover_Basis"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "For each value in the list {{\\ Unique_Values_Cover_Basis \\}}, generate a dictionary mapping where each value is normalised to one of the standard cover basis categories. You may receive values in full form (e.g., \"Third Party, Fire and Theft\"), as acronyms (e.g., \"TPFT\", \"TPO\", \"Comp\"), or as variations of known phrases (e.g., \"Comprehensive Cover\", \"Laid Up - Fire &amp; Theft\"). Use the following mapping rules: \"Third Party, Fire and Theft\" or similar \u2192 TPFT, \"Comprehensive\" or similar \u2192 Comp, \"Third Party Only\" or similar \u2192 TPO, \"Laid Up (Accidental Damage, Fire, and Theft)\" or similar \u2192 Laid Up (ADFT), \"Laid Up (Fire and Theft)\" , LUFT or similar \u2192 Laid up (FT Only). Direct acronyms like TPO, TPFT, Comp should map to themselves exclude LUFT, Since LUFT -&gt; Laid up (FT Only). If a value is irrelevant or does not match any known category, map it to an empty string \"\" Return the output as a dictionary like this: {\u201coriginal value\u201d: mapped category, }"
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Cover Basis Mapping@0"
                                    }
                                ],
                                "function_id": 17121,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Cover Basis Mapping",
                        "position": 35,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193489",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Registration_Numbers_And_Cover_Basis",
                                        "value": "Vehicle Registration Numbers And Cover Basis"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Unique__Values__Cover__Basis",
                                        "value": "Unique_Values_Cover_Basis"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Cover_Basis_Mapping",
                                        "value": "Cover Basis Mapping"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Effective_From",
                                        "value": "Effective From"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Effective_To",
                                        "value": "Effective To"
                                    }
                                ],
                                "function_id": 17122,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Vehicle Schedule Table",
                        "position": 36,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193490",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Schedule_Table",
                                        "value": "Vehicle Schedule Table"
                                    }
                                ],
                                "function_id": 17123,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Number of Notifiable Vehicles",
                        "position": 37,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193491",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Target_Price",
                                        "value": "Target Price"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Number_of_Notifiable_Vehicles",
                                        "value": "Number of Notifiable Vehicles"
                                    }
                                ],
                                "function_id": 17124,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Offering Type BP",
                        "position": 38,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193492",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17125,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Transaction Type",
                        "position": 40,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Agency Contact \u2014 this is the name of the person who sent the email.\n\nInstructions:\n\nLook for the sender\u2019s name in the email body, especially near the signature section at the end of the email.\n\nReturn only the full name of the person (e.g., John Smith).\n\nDo not return any email addresses, phone numbers, or job titles.\n\nIf no name is found, return \"N/A\"",
                        "field_id": "193493",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "DEFAULT",
                        "name": "Agency Contact",
                        "position": 39,
                        "prompt": "Extract the Agency Contact \u2014 the name of the person who submitted the broker presentation.\n\nInstructions:\n\nSearch the presentation for the sender\u2019s full name, typically found on the cover page, in a \u201cPrepared by\u201d \u201cContact\u201d, \"presenated by\", \"Produced by\" or \u201cSubmitted by\u201d section, or near the footer/header. Look for labels like Contact, Broker Contact, Point of Contact, or Prepared by.\n\nReturn only the full name (e.g., \"John Smith\" or \"Emma-Jane Clarke\").\n\nIf only a first or last name is present, return it as-is.\n\nIf no valid name is found, return \"\".",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193494",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Merged_Data",
                                        "value": "Driver Merged Data"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Type",
                                        "value": "Driver Type"
                                    }
                                ],
                                "function_id": 17126,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Driver Party Table",
                        "position": 31,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193495",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17127,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Business Category",
                        "position": 41,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193496",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Details",
                                        "value": "Driver Details"
                                    }
                                ],
                                "function_id": 17128,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Driver Name And Driver DOB",
                        "position": 25,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193497",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Details",
                                        "value": "Driver Details"
                                    }
                                ],
                                "function_id": 17129,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Driver Name And Licence Date Info",
                        "position": 27,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193498",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17130,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Country",
                        "position": 47,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193499",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17131,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address City",
                        "position": 49,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193500",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17132,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address State",
                        "position": 48,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193501",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Main_Cover_Type",
                                        "value": "Main Cover Type"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Main_cover_type : {{ \\ Main Cover Type \\ }}\n\nValid Normalized Types:\n\n1) COMP \u2192 includes: \"Comp\", \"COMP\", \"Comprehensive\", \"Comprehensive Cover\"\n\n2) TPFT \u2192 includes: \"TPFT\", \"Third Party Fire and Theft\", \"Third Party, Fire and Theft\", \"Third Party Fire &amp; Theft\"\n\n3) TPO \u2192 includes: \"TPO\", \"Third Party Only\", \"Third Party\"\n\n4) Laid up (ADFT) \u2192 includes: \"Laid Up (Accidental Damage, Fire, and Theft)\", \"Laid Up ADFT\", \"Laid-Up ADFT\"\n\n5) Laid up (FT Only) \u2192 includes: \"Laid Up (Fire and Theft)\", \"LUFT\", \"Laid-Up FT\"\n\nInstructions:\n\nMatch the Main_cover_type with the above Valid Normalized types (case-insensitive, ignore minor formatting differences).\n\nIf matched, return the normalized type exactly as listed above.\n\nIf no valid match, return \"\"."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Main Cover Type Mapped@0"
                                    }
                                ],
                                "function_id": 17133,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Main Cover Type Mapped",
                        "position": 42,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193502",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17134,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 2",
                        "position": 45,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193503",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17135,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 3",
                        "position": 46,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193504",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17136,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Postcode",
                        "position": 50,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193505",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17137,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 1",
                        "position": 44,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193506",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address",
                                        "value": "Party Address"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "You are an intelligent address extraction assistant.\n\nGiven a full address string {{\\ Party Address \\}}, extract the following components:\nAddress Line 1: Only the building number/name and street/road name. (Exclude landmarks, estates, zones, or locality info.)\nAddress Line 2: Estate, locality, landmark, or additional location detail\nAddress Line 3: Optional detail like block, zone, village, or area\nCity: Town or city name\nState: The state, province, or region\nCountry: Country name\nPostcode: postal code (UK format)\n\nExtraction Rules:\nDo not merge unrelated fields across lines\nDo not confuse postcode with city/state.\nReturn empty strings if any field is missing for that component \n{\n  \"Address Line 1\": \"\",\n  \"Address Line 2\": \"\",\n  \"Address Line 3\": \"\",\n  \"City\": \"\",\n  \"State\": \"\",\n  \"Country\": \"\",\n  \"Postcode\": \"\"\n}"
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Party Address Details@0"
                                    }
                                ],
                                "function_id": 17138,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Details",
                        "position": 43,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "OBJECT_LIST",
                        "description": "Task: Extract Excess Details and Cover Information for each valid Period from the document.\n\nInstructions:\nIdentify and extract rows where Period represents a valid policy period \u2014 typically expressed as either a range of years (e.g., \u201c2020\u20132021\u201d) or a date range (e.g., \u201c01/01/2021 \u2013 31/12/2021\u201d).\n\nField Extraction:\nLook for the following fields (column names may vary slightly):\n* Period\n* Accident Damage (AD) Excess \u2013 may appear as ADFT, ADFTWS, or similar variants\n* Fire Excess  \n* Theft Excess\n* WS Excess - ADFTWS, WS\n* Cover in Period\n\nInclusion Criteria:\n* Include a row only if both Period and Cover in Period are present.\n* Do not exclude rows if any of the excess fields are missing \u2014 for such cases, output those fields as empty strings ('').\n\nData Cleaning:\n* Remove any currency symbols (e.g., \u00a3) from the excess values.\n* Remove any thousands separators (e.g., 1,000 \u2192 1000) if present.",
                        "field_id": "193507",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "ADVANCED",
                        "name": "Excess List",
                        "position": 51,
                        "prompt": "Excess List",
                        "prompt_schema": [
                            {
                                "description": "",
                                "name": "Period"
                            },
                            {
                                "description": "If an ADFT (Accidental Damage, Fire, and Theft) excess is available, return only that value. If an ADFTWS (Accidental Damage, Fire, and Theft and WS) excess is available, return only that value.\n\nIf ADFT is not found, return the AD ( Accident and Damage) excess value.\n\nIf neither is available, return '0'",
                                "name": "AD Excess"
                            },
                            {
                                "description": "If an ADFT (Accidental Damage, Fire, and Theft) excess is available, return only that value.  If an ADFTWS (Accidental Damage, Fire, and Theft and WS) excess is available, return only that value.\n\nIf ADFT is not found, return the FT (Fire & Theft) excess value.\n\nIf neither is available, return '0'",
                                "name": "Fire Excess"
                            },
                            {
                                "description": "If an ADFT (Accidental Damage, Fire, and Theft) excess is available, return only that value.      If an ADFTWS (Accidental Damage, Fire, and Theft and WS) excess is available, return only that value.\n\nIf ADFT is not found, return the FT (Fire & Theft) excess value.\n\nIf neither is available, return '0'",
                                "name": "Theft Excess"
                            },
                            {
                                "description": "Wind screen excess value , If an ADFTWS (Accidental Damage, Fire, and Theft and WS) excess is available, return only that value. If an WS (wind storm) excess is available, return only that value. If data is not available, return '0'",
                                "name": "WS Excess"
                            },
                            {
                                "description": "The cover type may appear as one of the following:\n\nComprehensive or Comp\nThird Party\nTPO (Third Party Only)\nTPFT (Third Party, Fire and Theft)\n\nReturn the exact label as it appears in the document.\n\nIf multiple cover types are mentioned, return all distinct types.\n\nIf no cover type is found, or if you see some relevent text like \"Not Insured\", return \"\", don't return irrelevent data",
                                "name": "Cover On Policy"
                            }
                        ],
                        "prompt_type": "basic"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193508",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Excess_List",
                                        "value": "Excess List"
                                    }
                                ],
                                "function_id": 17139,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Unique Values in Cover On Policy",
                        "position": 52,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193509",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Unique_Values_in_Cover_On_Policy",
                                        "value": "Unique Values in Cover On Policy"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "For each row in the list {{ \\ Unique Values in Cover On Policy \\ }}, generate a dictionary mapping with the original value mapped to a category based on cover types.\n\n**Valid_cover_types**: \n- \"Comprehensive(COMP)\" (also treat \"Comp\", \"COMP\", \"Comprehensive\" as equivalent to this)\n- \"Third Party Fire and Theft(TPFT)\" (include variations like \"TPFT\", \"Third Party Fire &amp; Theft\")\n- \"Third Party Only(TPO)\" (include \"TPO\", \"Third Party\")\n- \"Laid up ADFT\"\n- \"Laid up FT\"\n- \"Mixed Cover\"\n\n**Mapping Rules**:\n1) If all values in a row (after normalization) are Comprehensive(COMP) \u2192 map to \"All Comprehensive\"\n(This must include variations like \u201cComp\u201d, \u201cCOMP\u201d, \u201cComprehensive\u201d as meaning \"Comprehensive(COMP)\")\n\n2) If all values in a row are Third Party Fire and Theft(TPFT) \u2192 map to \"All Third Party Fire and Theft\"\n\n3) If values are Third Party Only(TPO) \u2192 map to \"All Third Party Only\"\n\n4) If the row contains Comprehensive(COMP) (or its variations) along with other valid cover types \u2192 map to \"Mixed cover including Comprehensive\"\n\n5) If the row contains more than one valid cover type present in Valid_cover_types excluding Comprehensive(COMP) \u2192 map to \"Mixed cover excluding Comprehensive\"\n\n6) If any value in the row is not part of the Valid_cover_types or their known variations, map the entire row to \"\" (empty string). For example, the text \"Insurance company\" is not present in Valid_cover_types, so map it to \"\" (empty string).\n\nExample Output Format :\n\nInput : [\n             \"TPO, TPFT\",\n             \"Comp, TPFT\",\n             \"Comp, Comp\", \n              \"Any Insurance company\"\n             ]\n\nOutput : \n {\n\"TPO, TPFT\": \"Mixed cover excluding Comprehensive\",\n\"Comp, TPFT\": \"Mixed cover including Comprehensive\",\n\"Comp, Comp\": \"All Comprehensive\",\n \"Any Insurance company\": \"\" \n}"
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "cover mapping",
                        "position": 53,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "OBJECT_LIST",
                        "description": "Task: Extract insurance data for each valid Period from the CCE document.\n\nInstructions:\nIdentify and extract rows where Period represents a valid policy period \u2014 typically expressed as either a range of years (e.g., \u201c2020\u20132021\u201d) or a date range (e.g., \u201c01/01/2021 \u2013 31/12/2021\u201d).\n\nGuidelines:\nOnly extract rows where Period matches a valid policy period format (year range or date range).\nIgnore summary rows, totals, or headings that don\u2019t match this format.\nSkip rows where the Period is valid but no relevant data is associated (i.e., all required fields are blank, zero, or marked as N/A).\n\nClean numeric fields by:\nRemoving currency symbols (e.g., \u00a3)\nRemoving thousands separators (e.g., commas)\n\nIf a single field in the document combines multiple categories (e.g.:\n\u201cIncurreds - Paid: AD & WS and FT\u201d\n\u201cIncurreds - Outstanding: AD & WS and FT\u201d),\nthen duplicate the value into each corresponding field in the output (e.g., populate both AD & WS and FT fields with the same value).",
                        "field_id": "193510",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "ADVANCED",
                        "name": "CCE Data",
                        "position": 54,
                        "prompt": "CCE Data",
                        "prompt_schema": [
                            {
                                "description": "",
                                "name": "Period"
                            },
                            {
                                "description": "",
                                "name": "Vehicle Years Earned"
                            },
                            {
                                "description": "This may be labeled as Total no. of accidents & claims",
                                "name": "Claim Count"
                            },
                            {
                                "description": "These may be labeled under \"Claims paid (incl. payments on o/s claims\" and it is claims paid related to Accidental Damage and Windscreen (AD WS)",
                                "name": "Incurreds Paid AD WS"
                            },
                            {
                                "description": "Extract the claims paid amount related to Fire and Theft (FT) for each year from the CCE table.\n\nIf there is a column labeled \"AD including FT\" or similar, use that value \u2014 it already includes FT.\n\nIf there is a column labeled ADFT (Accidental Damage, Fire & Theft), use that.\n\nIf neither of the above is found, then look for and use the FT value. It might be written as:\nFT\nF&T\nF.&T.\nF&T (in AD).                                                                             \nAD including FT",
                                "name": "Incurreds Paid FT"
                            },
                            {
                                "description": "claims paid related to Third Party (TP), which may appear as TP or \"Claims Paid TP\"",
                                "name": "Incurreds Paid TP"
                            },
                            {
                                "description": "claims outstanding incurred amount related to Non-Owned (NO) claims, Look for column headers labeled as \"NO\", \"No\",  \"Non-Owned\", or similar",
                                "name": "Incurreds Outstanding NO"
                            },
                            {
                                "description": "outstanding claims related to Accidental Damage and Windscreen (AD WS)",
                                "name": "Incurreds Outstanding AD WS"
                            },
                            {
                                "description": "outstanding claims related to Fire and Theft, which may appear as FT or F&T",
                                "name": "Incurreds Outstanding FT"
                            },
                            {
                                "description": "outstanding claims related to Third Party (TP)",
                                "name": "Incurreds Outstanding TP"
                            },
                            {
                                "description": "total incurred amount for each year , which may appear as \"Total Paid and Outstanding\"",
                                "name": "Total Incurred"
                            },
                            {
                                "description": "If the text contains phrases like \"Not Insured with\", leave the insurer value empty for that row.\n\nIf no insurer is mentioned for a given period, use the default insurer mentioned in the header of that page\n\nOtherwise, extract the insurer name mentioned for that period.",
                                "name": "Insurer"
                            }
                        ],
                        "prompt_type": "basic"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193511",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Incepts_On",
                                        "value": "Incepts On"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "CCE_Data",
                                        "value": "CCE Data"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "CCE_Data: { \\ CCE Data \\}\u2028\nPolicy_Inception_Date: { \\ Incepts On \\}\n\nEach row in CCE_Data has a **Period** value. It can be a full date range 01/01/2023 - 31/12/2023, a year range 2020-2021 or 05/2020 - 06/2020 or a single year 2020.\n\nIf only years are present, use the day and month from the Policy_Inception_Date to build full dates. if month and year is present use date from the Policy_Inception_date to build full dates.\u2028\nFor example, if Policy_Inception_Date is 15/06/2019 and Period is 2020-2021, the result should be 15/06/2020 and 15/06/2021.\n\nIf only 2020 is given, return 15/06/2020 as the start date and 15/06/2021 (exactly one year ahead as end date) . If the Period has full dates, keep them unchanged.\n\nIf Period is 11/2020 - 10/2021 use the date from the Policy_Inception_Date and result should be 15/11/2020 and 15/10/2021.\n\nIf Period is 24/10/17-23/10/18 then the result should be 24/10/2017 and 24/10/2018\n\nReturn a table with:\u2028Period, Policy Period Start Date, Policy Period End Date, Used Inception Date (Yes/No) \u2014 where \u201cUsed Inception Date\u201d indicates whether the Policy_Inception_Date was used to construct the full dates or not."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Policy Dates@0"
                                    }
                                ],
                                "function_id": 17140,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Policy Dates",
                        "position": 55,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193512",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Policy_Dates",
                                        "value": "Policy Dates"
                                    }
                                ],
                                "function_id": 17141,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Period",
                        "position": 56,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193513",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Excess_List",
                                        "value": "Excess List"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "cover_mapping",
                                        "value": "cover mapping"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "CCE_Data",
                                        "value": "CCE Data"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Policy_Dates",
                                        "value": "Policy Dates"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Year",
                                        "value": "Year"
                                    }
                                ],
                                "function_id": 17142,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "CCE Table",
                        "position": 58,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193514",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Period",
                                        "value": "Period"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "claims_period: {{ \\ Period \\}}\nYou are given a list called claim_periods.\n\nYou are given a list called claim_periods.\nEach item in the list is a period, which may be:\nin full year format like \"31/01/2023\"\nOr in year format like \"2023\"\n\nYour task is:\n1. Extract the **starting year** for each row as mentioned.\n2. Sort the periods by starting year in **descending** order.\n3. Give a **Year** to each period starting from 1 (most recent = 1).\n\nIgnore any periods that don\u2019t contain a valid year.\nReturn the final output as a table with two columns: 'Policy Period Start Date' , 'Year'"
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Year",
                        "position": 57,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193515",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17143,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Trade Descriptions",
                        "position": 59,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193516",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Business_Description",
                                        "value": "Business Description"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Trade_Descriptions",
                                        "value": "Trade Descriptions"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "You are a domain expert in classifying business descriptions.\n\nYou are given:\n\nTrades_List: { \\ Trade Descriptions \\} \nBusiness_Description: { \\ Business Description \\}\n\nInstructions:\nFor the business description provided:\nCompare it against both the trade names and their aliases in Trades_List.\nDetermine the closest and similar matching **trade**.\nIf no suitable match is found, return \"Fleet - Unclassified\"."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "AXA Trade Description@0"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Business_Description",
                                        "value": "Business Description"
                                    }
                                ],
                                "function_id": 17144,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "AXA Trade Description@1"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Trade_Descriptions",
                                        "value": "Trade Descriptions"
                                    }
                                ],
                                "function_id": 17145,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "AXA Trade Description",
                        "position": 60,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    }
                ]
            },
            "Claims  Listing": {
                "class_id": "13548",
                "description": "Classify a document as Claims Listing if it has headers such as **Claims Listing**, **Claims Breakdown**, **Claims Experience Report** which includes headers such as **Claim Number**, **Claim Reference number** \n\nAlso consider any document with just driver details or **Policy schedule** and renewal quotations also.",
                "fields": []
            },
            "Confirmed Claims Experience": {
                "class_id": "13545",
                "description": "The document contains headings like \u201cMotor Fleet Claims Experience\u201d, \u201cMotor Fleet Experience Form\u201d, or \u201cConfirmed Claims Experience\u201d.\n\nIt includes data of Insured, Address of insured, Policy Number.\n\n\nIt includes a summary table with headers like:\n\n**Period**, **Vehicle Years**, **Number of Claims**, **Claims Paid**, **Claims Outstanding**, **Total Paid and Outstanding**",
                "fields": [
                    {
                        "data_type": "TEXT",
                        "description": "Extract the name of the insured. This could be called policyholder, client name, company name, full client name, or trading name. Just return the name of the person or business that the insurance is for. It often appears near the top of the email or in the subject line",
                        "field_id": "193517",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "DEFAULT",
                        "name": "Insured",
                        "position": 0,
                        "prompt": "Extract the name of the insured. This may appear under fields such as:\nPolicyholder, Insured Name, Name of Insured, Client Name, Company Name, Full Client Name, or Trading Name.\n\nJust return the name of the person or business the insurance is for.\nIf not found, return \"\" with no additional text.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "You are an expert in processing commercial fleet insurance submissions.\n\nYour task is to extract the **Business Description**, also referred to as the **Trade**, from the email subject or body. This describes the core nature of the client's business activities.\n\nThe business description often appears after labels like:\n- \"Trade:\"\n- \"Business Description:\"\n- \"Nature of Business:\"\n- \"Client is involved in\"\n- \"Their operations include\"\n\nExample:\n\"The Provision of Construction Services in the Development of General Contracting, Design & Build Contract Management, Maintenance and Refurbishment; Property Owners and Developers, Servicing of own vehicles\"\n\nExtract the full descriptive text that outlines the business operations of the insured.",
                        "field_id": "193518",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Just return the value for Business description without any additional text.\n\nIf no business description is found, return \"\"."
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Business Description",
                        "position": 1,
                        "prompt": "Extract the Business Description of the company from the document.\nLook for text following labels or titles such as:\n\n**Occupation**,\n**Business**\n**Business Description**,\n**Nature of Business**,\n**Trade**\n\nStrictly extract the business description from the provided labels only.\n\nDo not extract the Policyholder name or Insured name as Business description.\n\nIf no relevant information is found, return:\nBusiness Description: \"\"\n\nIf found, return in this format:\nBusiness Description: \"<extracted text>\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Insured\u2019s Address \u2014 the business or registered address of the party being covered.\n\nInstructions:\n\nSearch for address details labeled under headings such as \u201cInsured Address,\u201d \u201cAddress of Insured,\u201d or simply \u201cAddress.\u201d This is typically the location of the insured entity.\n\nReturn the full address as a single string (e.g., \"123 High Street, London, W1A 1AA\").\n\nDo not return any unrelated addresses (e.g., broker address, risk location).\n\nIf multiple addresses are listed, choose the one most clearly linked to the insured party.\n\nIf no valid address is found, return \"\"",
                        "field_id": "193519",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Clean and return **only the address details**. This means:\n- Preserve the physical address lines.\n- Format it as a single clean string separated by commas.\n- If no valid address is present, return \"\"."
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address",
                        "position": 2,
                        "prompt": "Extract the Insured Address \u2014 the business or registered address of the party being covered.\n\nInstructions:\n\nSearch for address details labeled under headings such as \u201cInsured Address,\u201d \u201cAddress of Insured,\u201d or simply \u201cAddress.\u201d This is typically the location of the insured entity.\n\nReturn the full address exactly as it appears in the document, without omitting or reformatting any part. Do not return any unrelated addresses (e.g., broker address, risk location).\n\nIf multiple addresses are listed, choose the one most clearly linked to the insured party.\n\nIf no valid address is found, return \"\" without any additional text",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the date labeled as \"Renewal Date\" for the insurance policy. If not found, return \"N/A\".",
                        "field_id": "193520",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Check Format: If date is already in the dd/mm/yyyy format, return it as is.\nStandardize Date: If the date is in a different format (e.g., yyyy-mm-dd, mm/dd/yyyy, dd-mm-yyyy, etc.), convert it to dd/mm/yyyy.\nInvalid Dates: If input_date is missing, empty, or not a valid date, return an empty string \"\""
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Incepts On@1"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Expiry_Date",
                                        "value": "Expiry Date"
                                    }
                                ],
                                "function_id": 17146,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Incepts On",
                        "position": 4,
                        "prompt": "Extract the Renewal Date from the given text using the following rules:\n\nStep 1: Look for \"Renewal Date\" , \"Policy Inception Date\", \"Inception date\"(case-insensitive)\n\u2013 Extract only the date next to these labels.\n\u2013 Accept formats like DD/MM/YYYY, DD-MM-YYYY, or textual (03 March 25 \u2192 03/03/2025)\n\u2013 If the year is two digits (e.g., 25), expand it to 2025\n- If only **Expiry Date** is found (without mention of Renewal), then do not treat it as a Renewal Date. In this case, return \"\".\n\nStep 2: If multiple renewal dates are present, return only the latest one\n\nStep 3: If not found, search for the combined label \"Renewal Date / Expiry Date\"\n\u2013 Extract only the first date as the Renewal Date\n\nStep 4: just return only valid date in dd/mm/yyy format only without extra text, If no valid renewal label is found, return: \"\"\n\n\nLets think step by step",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Main Cover Type from the input.\nThis refers to the level of motor insurance cover requested or provided (e.g., \"\"Comprehensive\"\", \"\"Third Party Only\"\", \"\"TPFT\"\").\nIf no valid cover type is found, return \"\"N/A\"\".",
                        "field_id": "193521",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "If the cover type got multiple values as comma separated, then return the first one, otherwise return the cover type as is without any additional text."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Main Cover Type@1"
                                    }
                                ],
                                "function_id": 17147,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Main Cover Type",
                        "position": 7,
                        "prompt": "Task: Extract the \"Main Cover Type\" from the document.\n\n**Valid cover types include**:\n--> \"Comprehensive\" (also matches: \"COMP\", \"Comp\", \"Comprehensive(COMP)\", etc.)\n--> \"Third Party Only\" (also matches: \"TPO\", \"Third Party\", \"Third Party Only(TPO)\", etc.)\n--> \"Third Party Fire and Theft\" (also matches: \"TPFT\", \"Third Party Fire & Theft\", \"Third Party Fire and Theft(TPFT)\", etc.)\n--> \"Laid Up Accidental Damage and Theft(LU ADFT)\"\n--> \"Laid Up Fire and Theft(LUFT)\"\n--> If the exact term \"Mixed Cover\" appears, treat it as a distinct cover type\n--> Combination of cover types(For eg: \"Comp, TPO\", \"TPO, TPFT\", \"Comp, Comp\", \"TPO,TPFT,Comp\") etc\n\nInstructions:\n1) Identify the most recent year mentioned in the excess amounts table.\n2) Search for the main cover type (motor insurance level) associated with that year.\n3) If the most recent year does not contain a valid cover type, then search for the next most recent year and repeat until a valid cover type is found.\n4) If no valid cover type is found in any year, return \"\".\n\nOutput: Return the cover type or combination of cover type mentioned in the document as is without any additional text. If not found, return: \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193522",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address",
                                        "value": "Party Address"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "insured_address: {{ \\ Party Address \\}}\n\nExtract the post code from the insured_address. The post code is typically an alphanumeric value located at the end of the address.\n\nIf no post code is found, return \"\"."
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Risk Postcode",
                        "position": 8,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193523",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Incepts_On",
                                        "value": "Incepts On"
                                    }
                                ],
                                "function_id": 17148,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Effective From",
                        "position": 6,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "OBJECT_LIST",
                        "description": "Task: Extract Excess Details and Cover Information for each valid Period from the document.\n\nInstructions:\nIdentify and extract rows where Period represents a valid policy period \u2014 typically expressed as either a range of years (e.g., \u201c2020\u20132021\u201d) or a date range (e.g., \u201c01/01/2021 \u2013 31/12/2021\u201d).\n\nField Extraction:\nLook for the following fields (column names may vary slightly):\n* Period\n* Accident Damage (AD) Excess \u2013 may appear as ADFT, ADFTWS, or similar variants\n* Fire Excess\n* Theft Excess\n* Wind Screen (WS) Excess\n* Cover in Period\n\nInclusion Criteria:\n* Include a row only if both Period and Cover in Period are present.\n* Do not exclude rows if any of the excess fields are missing \u2014 for such cases, output those fields as empty strings ('').\n\nData Cleaning:\n* Remove any currency symbols (e.g., \u00a3) from the excess values.\n* Remove any thousands separators (e.g., 1,000 \u2192 1000) if present.",
                        "field_id": "193524",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "ADVANCED",
                        "name": "Excess List",
                        "position": 9,
                        "prompt": "Excess List",
                        "prompt_schema": [
                            {
                                "description": "",
                                "name": "Period"
                            },
                            {
                                "description": "Extract only the numeric value. If an ADFT (Accidental Damage, Fire, and Theft) excess is available, return only that value. If an ADFTWS (Accidental Damage, Fire, and Theft and WS) excess is available, return only the numeric value.\n\nIf ADFT is not found, return the AD ( Accident and Damage) excess value.\n\nIf neither is available, return '0'",
                                "name": "AD Excess"
                            },
                            {
                                "description": "If an ADFT (Accidental Damage, Fire, and Theft) excess is available, return only that value.  If an ADFTWS (Accidental Damage, Fire, and Theft and WS) excess is available, return only the numeric value.\n\nIf ADFT is not found, return the FT (Fire & Theft) excess value.\n\nIf neither is available, return '0'",
                                "name": "Fire Excess"
                            },
                            {
                                "description": "If an ADFT (Accidental Damage, Fire, and Theft) excess is available, return only that value.      If an ADFTWS (Accidental Damage, Fire, and Theft and WS) excess is available, return only the numeric value.\n\nIf ADFT is not found, return the FT (Fire & Theft) excess value.\n\nIf neither is available, return '0'",
                                "name": "Theft Excess"
                            },
                            {
                                "description": "Wind storm excess value , If an ADFTWS (Accidental Damage, Fire, and Theft and WS) excess is available, return only that value. If an WS (wind storm) excess is available, return only the numeric value. If data is available, return '0'",
                                "name": "WS Excess"
                            },
                            {
                                "description": "Extract only the valid cover type which may appear as one of the following:\n**Comprehensive** or **Comp**\n**Third Party**\n**TPO (Third Party Only)**\n**TPFT (Third Party, Fire and Theft)** or **TPFT**\n\nIf multiple cover types are mentioned, return all distinct types.\nIf no cover type is found, or if you see some relevant text like \"Not Insured\", return \"\", don't return irrelevant data.",
                                "name": "Cover On Policy"
                            }
                        ],
                        "prompt_type": "basic"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193525",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Excess_List",
                                        "value": "Excess List"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the {{ \\ Excess List \\ }} use the 'Period' column to identify the recent year and return the 'AD Excess' value corresponding to it. The final output should be a well-formatted table with two columns: 'Period' column which is extactly there in the table and 'Excess Value', showing only the most recent year's value"
                            },
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean numeric data associated with **Excess Value**.\nIf no Excess Value is found, return \"0\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Excess Type Accident Damage CCE",
                        "position": 12,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "OBJECT_LIST",
                        "description": "Task: Extract insurance data for each valid Period from the CCE document.\n\nInstructions:\nIdentify and extract rows where Period represents a valid policy period \u2014 typically expressed as either a range of years (e.g., \u201c2020\u20132021\u201d, \"2021\", \"23/24\", \"21-22\") or a date range (e.g., \u201c01/01/2021 \u2013 31/12/2021\u201d).\n\nGuidelines:\nOnly extract rows where Period matches a valid policy period format (year range or date range).\nIgnore summary rows, totals, or headings that don\u2019t match this format.\nSkip rows where the Period is valid but no relevant data is associated (i.e., all required fields are blank, zero, or marked as N/A).\n\nClean numeric fields by:\nRemoving currency symbols (e.g., \u00a3)\nRemoving thousands separators (e.g., commas)\n\nIf a single field in the document combines multiple categories (e.g.:\n\u201cIncurreds - Paid: AD & WS and FT\u201d\n\u201cIncurreds - Outstanding: AD & WS and FT\u201d),\nthen duplicate the value into each corresponding field in the output (e.g., populate both AD & WS and FT fields with the same value).",
                        "field_id": "193526",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "ADVANCED",
                        "name": "CCE Data",
                        "position": 16,
                        "prompt": "CCE Data",
                        "prompt_schema": [
                            {
                                "description": "'Period' or 'Policy period'",
                                "name": "Period"
                            },
                            {
                                "description": "This refers to vehicle years, not the number of vehicles, it is always a numeric value",
                                "name": "Vehicle Years Earned"
                            },
                            {
                                "description": "This may be labeled as Total no. of accidents & claims",
                                "name": "Claim Count"
                            },
                            {
                                "description": "These may be labeled under \"Claims paid (incl. payments on o/s claims\" and it is claims paid related to Accidental Damage and Windscreen (AD WS)",
                                "name": "Incurreds Paid AD WS"
                            },
                            {
                                "description": "Extract the claims paid amount related to Fire and Theft (FT) for each year from the CCE table.\n\nIf there is a column labeled \"AD including FT\" or similar, use that value \u2014 it already includes FT.\n\nIf there is a column labeled ADFT (Accidental Damage, Fire & Theft), use that.\n\nIf neither of the above is found, then look for and use the FT value. It might be written as:\nFT\nF&T\nF.&T.\nF&T (in AD).                                                                                                                             ADFT&W/S                                                                             \nAD including FT",
                                "name": "Incurreds Paid FT"
                            },
                            {
                                "description": "claims paid related to Third Party (TP)",
                                "name": "Incurreds Paid TP"
                            },
                            {
                                "description": "claims outstanding incurred amount related to Non-Owned (NO) claims, Look for column headers labeled as \"NO\", \"No\",  \"Non-Owned\", or similar",
                                "name": "Incurreds Outstanding NO"
                            },
                            {
                                "description": "outstanding claims related to Accidental Damage and Windscreen (AD WS)",
                                "name": "Incurreds Outstanding AD WS"
                            },
                            {
                                "description": "outstanding claims related to Fire and Theft, which may appear as FT or F&T",
                                "name": "Incurreds Outstanding FT"
                            },
                            {
                                "description": "outstanding claims related to Third Party (TP)",
                                "name": "Incurreds Outstanding TP"
                            },
                            {
                                "description": "total incurred amount for each year",
                                "name": "Total Incurred"
                            },
                            {
                                "description": "If the text contains phrases like \"Not Insured with\", leave the insurer value empty for that row.\n\nIf no insurer is mentioned for a given period, use the default insurer mentioned in the header of that page\n\nOtherwise, extract the insurer name mentioned for that period.",
                                "name": "Insurer"
                            }
                        ],
                        "prompt_type": "basic"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193527",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Excess_List",
                                        "value": "Excess List"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the ** \\ Excess List \\** use the 'Period' column to identify the recent year and return the recent 'Fire Excess' value corresponding to it. The final output should be a well-formatted table with two columns: 'Period' column which is extactly there in the table and 'Excess Value', showing only the most recent year's value"
                            },
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean numeric data associated with **Excess Value**.\nIf no Excess Value is found, return \"0\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Excess Type Fire CCE",
                        "position": 13,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193528",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Excess_List",
                                        "value": "Excess List"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the \\ Excess List \\ use the 'Period' column to identify the recent year and return the 'Theft Excess' value corresponding to it. The final output should be a well-formatted table with two columns: 'Period' column which is extactly there in the table and 'Excess Value', showing only the most recent year's value"
                            },
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean numeric data associated with **Excess Value**.\nIf no Excess Value is found, return \"0\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Excess Type Theft CCE",
                        "position": 14,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193529",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Excess_List",
                                        "value": "Excess List"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the \\ Excess List \\ use the 'Period' column to identify the recent year and return the 'WS Excess' value corresponding to it. The final output should be a well-formatted table with two columns: 'Period' column which is extactly there in the table and 'Excess value', showing only the most recent year's value"
                            },
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean numeric data associated with **Excess value**.\nIf no Excess Value is found, return \"0\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Excess Type WS CCE",
                        "position": 15,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193530",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "CCE_Data",
                                        "value": "CCE Data"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "CCE_Table",
                                        "value": "CCE Table"
                                    }
                                ],
                                "function_id": 17149,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Number Of Years Claims Experience",
                        "position": 23,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193531",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "CCE_Data",
                                        "value": "CCE Data"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Policy_Dates",
                                        "value": "Policy Dates"
                                    }
                                ],
                                "function_id": 17150,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Period",
                        "position": 18,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193532",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17151,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Transaction Type",
                        "position": 20,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193533",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Excess_List",
                                        "value": "Excess List"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "CCE_Data",
                                        "value": "CCE Data"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Year",
                                        "value": "Year"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "cover_mapping",
                                        "value": "cover mapping"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Policy_Dates",
                                        "value": "Policy Dates"
                                    }
                                ],
                                "function_id": 17152,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "CCE Table",
                        "position": 22,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193534",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Incepts_On",
                                        "value": "Incepts On"
                                    }
                                ],
                                "function_id": 17153,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Expires On",
                        "position": 5,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193535",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Expires_On",
                                        "value": "Expires On"
                                    }
                                ],
                                "function_id": 17154,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Effective To",
                        "position": 21,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193536",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Period",
                                        "value": "Period"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "claims_period: {{ \\ Period \\}}\nYou are given a list called claim_periods.\n\nYou are given a list called claim_periods.\nEach item in the list is a period, which may be:\nin full year format like \"31/01/2023\"\nOr in year format like \"2023\"\n\nYour task is:\n1. Extract the **starting year** for each row as mentioned.\n2. Sort the periods by starting year in **descending** order.\n3. Give a **Year** to each period starting from 1 (most recent = 1).\n\nIgnore any periods that don\u2019t contain a valid year.\nReturn the final output as a table with two columns: 'Policy Period Start Date' , 'Year'"
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Year@0"
                                    }
                                ],
                                "function_id": 17155,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Year",
                        "position": 19,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193537",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17156,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Business Category",
                        "position": 24,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193538",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Excess_List",
                                        "value": "Excess List"
                                    }
                                ],
                                "function_id": 17157,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Unique Values in Cover On Policy",
                        "position": 10,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193539",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Unique_Values_in_Cover_On_Policy",
                                        "value": "Unique Values in Cover On Policy"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "For each row in the list {{ \\ Unique Values in Cover On Policy \\ }}, generate a dictionary mapping with the original value mapped to a category based on cover types.\n\n**Valid_cover_types**: \n- \"Comprehensive(COMP)\" (also treat \"Comp\", \"COMP\", \"Comprehensive\" as equivalent to this)\n- \"Third Party Fire and Theft(TPFT)\" (include variations like \"TPFT\", \"Third Party Fire &amp; Theft\")\n- \"Third Party Only(TPO)\" (include \"TPO\", \"Third Party\")\n- \"Laid up ADFT\"\n- \"Laid up FT\"\n- \"Mixed Cover\"\n\n**Mapping Rules**:\n1) If all values in a row (after normalization) are Comprehensive(COMP) \u2192 map to \"All Comprehensive\"\n(This must include variations like \u201cComp\u201d, \u201cCOMP\u201d, \u201cComprehensive\u201d as meaning \"Comprehensive(COMP)\")\n\n2) If all values in a row are Third Party Fire and Theft(TPFT) \u2192 map to \"All Third Party Fire and Theft\"\n\n3) If values are Third Party Only(TPO) \u2192 map to \"All Third Party Only\"\n\n4) If the row contains Comprehensive(COMP) (or its variations) along with other valid cover types \u2192 map to \"Mixed cover including Comprehensive\"\n\n5) If the row contains more than one valid cover type present in Valid_cover_types excluding Comprehensive(COMP) \u2192 map to \"Mixed cover excluding Comprehensive\"\n\n6) If any value in the row is not part of the Valid_cover_types or their known variations, map the entire row to \"\" (empty string). For example, the text \"Insurance company\" is not present in Valid_cover_types, so map it to \"\" (empty string).\n\nExample Output Format :\n\nInput : [\n             \"TPO, TPFT\",\n             \"Comp, TPFT\",\n             \"Comp, Comp\", \n              \"Any Insurance company\"\n             ]\n\nOutput : \n {\n\"TPO, TPFT\": \"Mixed cover excluding Comprehensive\",\n\"Comp, TPFT\": \"Mixed cover including Comprehensive\",\n\"Comp, Comp\": \"All Comprehensive\",\n \"Any Insurance company\": \"\" \n}"
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "cover mapping",
                        "position": 11,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193540",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Main_Cover_Type",
                                        "value": "Main Cover Type"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Main_cover_type : {{ \\ Main Cover Type \\ }}\n\nValid Normalized Types:\n\n1) COMP \u2192 includes: \"Comp\", \"COMP\", \"Comprehensive\", \"Comprehensive Cover\"\n\n2) TPFT \u2192 includes: \"TPFT\", \"Third Party Fire and Theft\", \"Third Party, Fire and Theft\", \"Third Party Fire &amp; Theft\"\n\n3) TPO \u2192 includes: \"TPO\", \"Third Party Only\", \"Third Party\"\n\n4) Laid up (ADFT) \u2192 includes: \"Laid Up (Accidental Damage, Fire, and Theft)\", \"Laid Up ADFT\", \"Laid-Up ADFT\"\n\n5) Laid up (FT Only) \u2192 includes: \"Laid Up (Fire and Theft)\", \"LUFT\", \"Laid-Up FT\"\n\nInstructions:\n\nMatch the Main_cover_type with the above Valid Normalized types (case-insensitive, ignore minor formatting differences).\n\nIf matched, return the normalized type exactly as listed above.\n\nIf no valid match, return \"\"."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Main Cover Type Mapped@0"
                                    }
                                ],
                                "function_id": 17158,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Main Cover Type Mapped",
                        "position": 25,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193541",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17159,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 2",
                        "position": 28,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193542",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17160,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 3",
                        "position": 29,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193543",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17161,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Country",
                        "position": 30,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193544",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17162,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address State",
                        "position": 31,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193545",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17163,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address City",
                        "position": 33,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193546",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17164,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Postcode",
                        "position": 32,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193547",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17165,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 1",
                        "position": 27,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193548",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address",
                                        "value": "Party Address"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "You are an intelligent address extraction assistant.\n\nGiven a full address string {{ \\ Party Address \\}}, extract the following components:\nAddress Line 1: Only the building number/name and street/road name. (Exclude landmarks, estates, zones, or locality info.)\nAddress Line 2: Estate, locality, landmark, or additional location detail\nAddress Line 3: Optional detail like block, zone, village, or area\nCity: Town or city name\nState: The state, province, or region\nCountry: Country name\nPostcode: postal code (UK format)\n\nExtraction Rules:\nDo not merge unrelated fields across lines\nDo not confuse postcode with city/state.\nReturn empty strings if any field is missing for that component \n{\n  \"Address Line 1\": \"\",\n  \"Address Line 2\": \"\",\n  \"Address Line 3\": \"\",\n  \"City\": \"\",\n  \"State\": \"\",\n  \"Country\": \"\",\n  \"Postcode\": \"\"\n}"
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Details",
                        "position": 26,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Step 1: Search for a date that is explicitly labeled just with the keyword **Expiry Date**.\n\u2013 If found, extract only the date value next to it.\n\u2013 Ignore any extra text, prefixes, or suffixes. Return the date in its original format.\n\nStep2 : Strictly do not extract Renewal Date and other dates as expiry date\n\nStep 3: If none of the above labels are found, return \"\".\n\nOutput Format:\nReturn only the date value (e.g., 12/05/2024)\nIf not available based on the rules above, return \"\".\n\nLet's think step by step.",
                        "field_id": "193549",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Check Format: If date is already in the dd/mm/yyyy format, return it as is.\nStandardize Date: If the date is in a different format (e.g., yyyy-mm-dd, mm/dd/yyyy, dd-mm-yyyy, etc.), convert it to dd/mm/yyyy.\nInvalid Dates: If input_date is missing, empty, or not a valid date, return an empty string \"\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Expiry Date",
                        "position": 3,
                        "prompt": "Step 1: Search for a date that is explicitly labeled just with the keyword **Expiry Date**.\n\u2013 If found, extract only the date value next to it.\n\u2013 Ignore any extra text, prefixes, or suffixes. Return the date in its original format.\n\nStep2 : Strictly do not extract **Renewal Date** and other dates as expiry date\n\nStep 3: If none of the above labels are found, return \"\".\n\nOutput Format:\nReturn only the date value (e.g., 12/05/2024)\nIf not available based on the rules above, return \"\".\n\nLet's think step by step.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193550",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Incepts_On",
                                        "value": "Incepts On"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "CCE_Data",
                                        "value": "CCE Data"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "CCE_Data: { \\ CCE Data \\}\u2028\nPolicy_Inception_Date: { \\ Incepts On \\}\n\nEach row in CCE_Data has a **Period** value. It can be a full date range 01/01/2023 - 31/12/2023, a year range 2020-2021 or 05/2020 - 06/2020 or a single year 2020.\n\nIf only years are present, use the day and month from the Policy_Inception_Date to build full dates. if month and year is present use date from the Policy_Inception_date to build full dates.\u2028\nFor example, if Policy_Inception_Date is 15/06/2019 and Period is 2020-2021, the result should be 15/06/2020 and 15/06/2021.\n\nIf only 2020 is given, return 15/06/2020 as the start date and 15/06/2021 (exactly one year ahead as end date) . If the Period has full dates, keep them unchanged.\n\nIf Period is 11/2020 - 10/2021 use the date from the Policy_Inception_Date and result should be 15/11/2020 and 15/10/2021.\n\nIf Period is 24/10/17-23/10/18 then the result should be 24/10/2017 and 24/10/2018\n\nReturn a table with:\u2028Period, Policy Period Start Date, Policy Period End Date, Used Inception Date (Yes/No) \u2014 where \u201cUsed Inception Date\u201d indicates whether the Policy_Inception_Date was used to construct the full dates or not."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Policy Dates@0"
                                    }
                                ],
                                "function_id": 17166,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Policy Dates",
                        "position": 17,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193551",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17167,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Trade Descriptions",
                        "position": 34,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193552",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Business_Description",
                                        "value": "Business Description"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Trade_Descriptions",
                                        "value": "Trade Descriptions"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "You are a domain expert in classifying business descriptions.\n\nYou are given:\n\nTrades_List: { \\ Trade Descriptions \\} \nBusiness_Description: { \\ Business Description \\}\n\nInstructions:\nFor the business description provided:\nCompare it against both the trade names and their aliases in Trades_List.\nDetermine the closest and similar matching **trade**.\nIf no suitable match is found, return \"Fleet - Unclassified\"."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "AXA Trade Description@0"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Business_Description",
                                        "value": "Business Description"
                                    }
                                ],
                                "function_id": 17168,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "AXA Trade Description@1"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Trade_Descriptions",
                                        "value": "Trade Descriptions"
                                    }
                                ],
                                "function_id": 17169,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "AXA Trade Description",
                        "position": 35,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    }
                ]
            },
            "Email": {
                "class_id": "13542",
                "description": "Classify a document as an email if it shows typical email structure and language. \n\nStrictly check for the file extension, it should be \".eml\" or \".msg\" only.\n\nLook for:\nHeader fields usually found at the beginning of emails, such as:\nFrom, To, Subject, Cc, Date, or Sent.\nEmail-specific language, especially near the end of the document:\nClosings like \u201cBest regards\u201d, \u201cKind regards\u201d, \u201cThanks\u201d, \u201cSincerely\u201d, \u201cWarm regards\u201d, or similar phrases followed by a name.\nInformal or conversational tone often used in emails, including greetings like:\n\u201cHi [Name]\u201d, \u201cHello\u201d, \u201cDear [Name]\u201d.\nOptional metadata or footer elements:\nEmail signatures, disclaimers, or \u201cPlease consider the environment\u2026\u201d type footers.\nClassify the document as \"email\" only if at least two of the above characteristics are present",
                "fields": [
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Target Price \u2014 the premium or price the client or broker expects to pay for the insurance policy. sometimes it is implied from previous year\u2019s premium.\nIf no relevant price reference is available, return \"N/A\"",
                        "field_id": "193401",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Extract only the final target premium value.\nIf no valid target premium value is found, return \"\".\nDo not include any label text like \"target premium\" or \"client is looking for\".\nReturn only the clean value."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Target Price@1"
                                    }
                                ],
                                "function_id": 17068,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Target Price",
                        "position": 0,
                        "prompt": "Step 1 \u2013 Identify Base Premium:\nLook for phrases like \u201crenewal terms received\u201d, \u201ctarget at the moment\u201d, \u201cwe would like terms\u201d, \u201clooking for a quote\u201d \u2192 mark as Target_Premium.\n\nstrictly Ignore values with \u201clast year was\u201d, \u201cexpiry premium\u201d,\"last year paid\", \u201ccurrently paying\u201d unless a % change is also mentioned.\n\nIf % change (e.g., \u201c10% increase\u201d) is mentioned with a past premium \u2192 calculate: base \u00b1 % change in Step3\n\nIf two premiums from different insurers \u2192 use the lower one.\nIf no valid target \u2192 return \" \".\n\nStep 2 \u2013 Adjust for IPT:\nIf \u201cincluding IPT\u201d \u2192 divide by 1.12.\nIf \u201c+ IPT\u201d, \u201cexcluding IPT\u201d, or \u201cplus IPT\u201d \u2192 use as-is.\nIgnore commission.\n\nStep 3 \u2013 Apply % Change:\nIf phrases like \u201c10% increase\u201d or \"additional premium\" or \u201cindication is\u2026\u201d appear \u2192 apply change.\n\nOutput:\nCalculated Target Premium: \u00a3[value]\n\nlets think step by step",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "You are an expert in processing commercial fleet insurance submissions.\n\nYou are working with the subject line and body of an email related to a submission. Your task is to extract the **Agency Enquiry Reference**, also referred to as **Quote Ref**, **Quote Reference**, or **RFQ**.\n\nThis reference number may appear:\n- In the subject line (e.g. \"Quote Ref: 746134380\")\n- In the body of the email (e.g. \"Please use RFQ - 156802033 for your response\")\n\nLook for patterns like:\n- \"Quote Ref\"\n- \"Quote Reference\"\n- \"QuoteRef\"\n- \"RFQ\"\n- \"Ref\"\n\nExtract the full text containing the reference, including any prefix like \"RFQ -\".",
                        "field_id": "193402",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Extract only the clean Agency Enquiry Reference value.\n\nKeep the full reference if it is in a format like:\n\n\"RFQ - 156802033\"\n\n\"746134380\"\n\n\"[Quote Ref: 746134380]\"\n\nRemove any surrounding labels, brackets, or filler text.\nReturn only the clean reference string. If no valid reference is found, return \"\"."
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Agency Enquiry Reference",
                        "position": 1,
                        "prompt": "You are working with the subject line and body of an email related to a submission. Your task is to extract the Agency Enquiry Reference, also known as Quote Ref, Quote Reference, RFQ, or Ref. It is a unique numeric or alphanumeric ID provided by the broker to track the quote.\n\nLook for below similar phrases:\nQuote Ref: 745134320\nRFQ-156802033\nRef 947152\nExtract the full expression, including the prefix (e.g., \"RFQ-156802033\" or \"Quote \nRef: 746134380\").\n\nExclude:\nUnlabeled strings like Q009258247-1169877\nSystem IDs starting with Q00, P00\nPlain numbers with no label\nIf multiple references are present, return all. If none, return an empty string \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "You are an expert in processing commercial fleet insurance submissions.\n\nYour task is to extract the **Broker Deadline**, also referred to as **Target Date**, **Quote Required By**, or simply **Deadline**, from the email subject or body.\n\nLook for expressions such as:\n- \"Quote required by 10th January 2025\"\n- \"Deadline: 10th January 2025\"\n- \"Target date 10th January 2025\"\n\nIdentify phrases indicating a deadline and extract the associated date that follows.",
                        "field_id": "193403",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Just return only date without extra information"
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Broker Deadline@1"
                                    }
                                ],
                                "function_id": 17069,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Broker Deadline",
                        "position": 2,
                        "prompt": "Extract the Broker Deadline from the email content.\u2028This refers to the date by which a quote is expected from the insurer.\n\nInstructions\nStep 1: Search for relevant deadline phrases\nIdentify dates associated with the following phrases:\nQuote required by\nDeadline\nTarget date\nQuote deadline\nQuote expected by\nNeeded by\nTerms no later than\n\nStep 2: Handle date formats\nIf day, month, and year are present \u2192 return in dd/mm/yyyy format.\u2028Example: \u201cQuote required by 10 January 2025\u201d \u2192 10/01/2025\nIf only day and month are present (no year) \u2192 return in dd/mm format.\u2028Example: \u201cQuote expected by 19 Jan\u201d \u2192 19/01\nDo not infer the year from the email\u2019s received date.\n\nStep 3: Exclude irrelevant dates\nIgnore dates associated with:\nPolicy renewal\nEffective\nExpiry\nDue on\n\nOutput Format\nBroker Deadline Date : <deadline date or \"\">\nFull date: Broker Deadline Date : 10/01/2025\nPartial date: Broker Deadline Date : 10/01\nNo valid date: Broker Deadline Date : \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193404",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17070,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Product",
                        "position": 3,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "You are an expert in processing commercial fleet insurance submissions.\n\nYour task is to extract the **Insured Name**, which may also be referred to as:\n\n- Policyholder\n- Client Name\n- Full Client Name\n- Trading Name\n- Company Name\n\nThe insured name typically refers to the legal entity or business applying for cover. It can appear in the email subject or body and is usually a company or trading name.\n\nExamples:\n- \"ABC (Holdings) Ltd\"\n- \"XYZ Holdings\"\n\nLook for phrases such as:\n- \"Client Name: ABC (Holdings) Ltd\"\n- \"Company: XYZ Holdings\"\n- \"Policyholder \u2013 ABC Ltd\"\n\nExtract only the entity name that represents the client or policyholder.",
                        "field_id": "193405",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Return only the clean **Insured Name**.\n\n- Preserve full legal or trading names as written, such as:\n  - \"ABC (Holdings) Ltd\"\n  - \"XYZ Holdings\"\n- Remove any preceding labels such as \"Client Name:\", \"Company:\", or \"Policyholder \u2013\"\n- Remove any trailing punctuation or filler text\n\nIf no insured name is found, return \"\"."
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Insured",
                        "position": 4,
                        "prompt": "You are an expert in processing commercial fleet insurance submissions.\n\nYour task is to extract the **Insured Name**, It often appears on the top of the email or in the **Subject:** line. which may also be referred to as:\n\n- Policyholder\n- Client Name\n- Full Client Name\n- Trading Name\n- Company Name\n\nThe insured name typically refers to the legal entity or business applying for cover. It can appear in the email subject or body and is usually a company or trading name.\n\nExamples:\n- Subject: ABC Holdings Ltd  \u2192 Extract ABC Holdings Ltd\n- Subject: [EXTERNAL] XYZ Holdings \u2192Extract XYZ Holdings\n\nLook for phrases such as:\n- \"Client Name: ABC (Holdings) Ltd\"\n- \"Company: XYZ Holdings\"\n- \"Policyholder \u2013 ABC Ltd\"\n\nExtract only the entity name that represents the client or policyholder.\n\nOutput Format:\n{\n   \"Insured Name\" : \"<extracted insured name>\"\n}",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Find and return the business description or trade of the insured. This is usually a short phrase describing what the company does \u2014 like haulage, construction, taxi service, or logistics",
                        "field_id": "193406",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean **Business Description**.\n\n- Preserve the complete business activity description as written.\n- Remove any preceding labels such as \"Trade:\", \"Business Description:\", or \"Nature of Business:\"\n- Remove trailing punctuation, line breaks, or filler text.\n\nIf no business description is found, return \"\""
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Business Description",
                        "position": 5,
                        "prompt": "Extract the Business Description of the company from the docuemnt.\nLook for text following keywords or titles such as:\n\nOccupation\nBusiness\nBusiness Description\nNature of Business\nTrade\n\nOnly extract the content that describes the core business activity or operations. Strictly do not include the insured name, Name of Insured, contact info, address, or unrelated labels as business description .\n\nIf no relevant information is found, return:\nBusiness Description: \"\"\n\nIf found, return in this format:\nBusiness Description: \"<extracted text>\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "You are an expert in processing commercial fleet insurance submissions.\n\nYour task is to extract the **Agency Name**, also referred to as the **Broker Name**, from the email content. This is the name of the brokerage firm or intermediary who has sent the submission.\n\nThis name is typically found in:\n- The email signature\n- The sender\u2019s details in the body\n- Footer or sign-off sections of the email\n\nCommon patterns:\n- \"Kind regards, John Smith, Butterworth Spengler\"\n- \"Thomas Carroll \u2013 Fleet Division\"\n- \"Best, Rachel | Broker at XYZ Brokers\"\n\nExtract only the name of the broker agency, not the individual contact person.",
                        "field_id": "193407",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean **Agency Name** or **Broker Name**.\n\nIf no agency or broker name is found, return \"\"."
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Agency Name",
                        "position": 10,
                        "prompt": "Extract the complete Broker or Agency Name from the email content. This is the name of the brokerage firm or intermediary that sent the submission.\n\nLook for the name in:\n\nEmail signature or sign-off lines (e.g., \u201cKind regards, John \u2013 ABC Brokers\u201d)\n\nSender\u2019s details in body (e.g., \u201c| Broker at XYZ Insurance Services\u201d)\n\nFooter/legal lines\n\nAlways extract the full official name, including suffixes like Ltd, LLP, & Co., Insurance Group, etc.\n\nIf it says \u201ca part of\u201d or \u201csubsidiary of\u201d a group, extract the parent agency name.\n\nIgnore person names, internal teams (e.g., \u201cFleet Division\u201d), or job titles.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Companies House Reference of the insured, an alphanumeric code up to 8 characters, often referred to as \"Company registration number\" or \"Company No.\" Return only the value. If not found, return \"N/A\".",
                        "field_id": "193408",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Just return the final output company registration number of the insured, otherwise return an empty string: \"\" without any additional text."
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Company House Reference Party",
                        "position": 6,
                        "prompt": "You are expert in analysing vehicle insurance data. Your task is to extract the **company registration** number of the insured/client from the Email body.\n\nInstructions:\n\nStep-1 : Identify the Insured/client.\n\nStep-2 : Extract the company registration number of the insured/client extracted from step-1, which is typically 8 characters long but can be fewer.\n\nStep-3 : Look for key phrases indicating a registration number, such as:\n\n\u201cCompany Number\u201d\n\n\u201cCompany Registration Number\u201d\n\n\u201cRegistered Number\u201d\n\nor any similar variation\n\nExclusions & Constraints:\n\nStep-4 : Do not extract the company number found in the broker's details or footer.\n\nStep-5 : Only extract the number if it refers specifically to the insured/client.\n\nStep-6 : If no such registration number is found for the client, return an empty string: \"\".\n\nLet's think step by step.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "You are an expert in processing commercial fleet insurance submissions.\n\nYour task is to extract the **Date Established** for the insured. This may also be referred to as:\n\n- \"Insured \u2013 Date Established\"\n- \"Year Business Established\"\n- \"Established\"\n- \"Date of Incorporation\"\n- \"Trading Since\"\n\nThis information typically appears in the body of the email, submission form, or company profile section.\n\nLook for phrases like:\n- \"Date established: 15th March 2010\"\n- \"Established in 2005\"\n- \"Trading since January 1998\"\n- \"Incorporated on 07/06/2003\"\n\nExtract the full date or year that indicates when the insured began operations.",
                        "field_id": "193409",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "You are an expert in document analysis and date extraction.\nYour task is to extract and normalize a date from the given text.\nInstructions:\nInterpret all date formats strictly as day/month/year (DD/MM/YYYY), regardless of how the date appears in the text. The first number is always the day, second is the month, third is the year.\nNormalize and return the date in the format: DD/MM/YYYY (zero-padded).\nIf only a year is present (e.g. \"2020\"), return it as 01/01/YYYY.\nIf a month and year are present (e.g. \"March 2020\" or \"03/2020\"), return it as 01/MM/YYYY.\nIf a full day, month, and year are present, return the exact date in DD/MM/YYYY format.\nReturn only the extracted value (no label or explanation in output).\nIf no valid establishment date is found, return exactly: \"\" (empty string, without quotes).\nExamples:\nExample 1:\nText: The business was founded in 2021.\nOutput: 01/01/2021\nExample 2:\nText: Established: March 2020.\nOutput: 01/03/2020\nExample 3:\nText: Policy effective from 5/11/2023 for one year.\nOutput: 05/11/2023\nExample 4:\nText: Driver's licence issue date: 14-6-90, valid for 10 years.\nOutput: 14/06/1990\nExample 5:\nText: Vehicle registration expires on 01.12.2025.\nOutput: 01/12/2025\nExample 6:\nText: Renewal date: \"\"\nOutput: \"\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Date Established",
                        "position": 7,
                        "prompt": "You are an expert in processing commercial fleet insurance submissions.\n\nYour task is to extract the **Date Established** for the insured. This may also be referred to as:\n\n- \"Insured \u2013 Date Established\"\n- \"Year Business Established\"\n- \"Established\"\n- \"Date of Incorporation\"\n- \"Trading Since\"\n\nThis information typically appears in the body of the email, submission form, or company profile section.\n\nLook for phrases like:\n- \"Date established: 15th March 2010\"\n- \"Established in 2005\"\n- \"Trading since January 1998\"\n- \"Incorporated on 07/06/2003\"\n\nExtract the full date or year that indicates when the insured began operations.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Party Address (also known as the Insured \u2013 Correspondence Address) from the document.\n\nInstructions:\n\nThis is the physical or mailing address of the insured entity.\n\nIt may appear in the email body, contact details section, or signature block.\n\nLook for lines or phrases like:\n\"Correspondence Address: 8 Griffin Road\"\n\"Insured Address \u2013 1 Main Street\"\n\"Address: Head Office \u2013 22 King Street\"\n\nIf multiple addresses are found, prioritise the one labeled as \"Head Office\" or similar (e.g., \u201cHO Address\u201d).\n\nExtract the complete line(s) representing the address.\n\nIgnore unrelated addresses (e.g., branch, broker, or claimant addresses).\n\nOutput:\n\nReturn the full Party Address as a string. If no valid address is found, return \"N/A\"",
                        "field_id": "193410",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Clean and return **only the address details**. This means:\n- Preserve the physical address lines.\n- Format it as a single clean string separated by commas.\n- If no valid address is present, return \"\"."
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Party Address",
                        "position": 8,
                        "prompt": "Extract the full insured party\u2019s address from the given text.\n\nThe insured is the client or policyholder, not the insurer or the broker/intermediary.\n\nOnly return addresses clearly associated with the insured party\u2019s name (e.g., Hewer Group Ltd, Multi Turn Ltd, etc.).\n\nDo not return any address linked to brokers, agents, or insurers (e.g., Gallagher, AJG, UBT, AXA, Marsh, etc.).\n\nIf multiple addresses appear, return the one labeled \"Head Office\", \"HO Address\", or \"Registered Office\", if available.\n\nReturn the full address exactly as it appears in the document without omitting or reformatting any part, as a single string.\n\nIf no valid insured-related address is found, return \"\".",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the value associated with \"Holding Broker\" from the given text. Only return the value if it is explicitly labeled as \"Holding Broker\" (case-insensitive). Do not infer or guess the broker name if the label is not clearly mentioned. Return \"N/A\" if \"Holding Broker\" is not found",
                        "field_id": "193411",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "ADVANCED",
                        "name": "Holding Broker",
                        "position": 12,
                        "prompt": "Your task is to extract the Holding Broker from the given text.\n\nOnly extract the value if the term \"Holding Broker\" is explicitly mentioned (case-insensitive).\n\nAccept direct patterns such as:\n\n\"Holding Broker: [Name]\"\n\"We are the holding broker\"\n\"Acting as the holding broker\"\n\"[Name] is the holding broker\"\n\nDo not infer or assume the broker name based on context, sender domain, or other parties mentioned in the text.\n\nIf the statement is like \u201cWe are the holding broker,\u201d return the organization or person making the statement only if they are clearly identified nearby (e.g., in a signature block).\n\nIf the term \"Holding Broker\" is not present, or the associated name is unclear, return an empty string \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract just the Inception Date \u2014 the date the insurance policy is set to begin.\nIt may also be referred to as \"Effective Date\", \"Start Date\", or \"Incepts On\".\nDo not return the Renewal Date.\nIf no relevant start date is found, return \"N/A\".",
                        "field_id": "193412",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Check Format: If date is already in the dd/mm/yyyy format, return it as is.\nStandardize Date: If the date is in a different valid format (e.g., yyyy-mm-dd, mm/dd/yyyy, dd-mm-yyyy, etc.), convert it to dd/mm/yyyy. return the cleaned date without any quotes\nInvalid Dates: If date is missing, empty, or not a valid date, return an empty string \"\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Incepts On",
                        "position": 13,
                        "prompt": "From the provided reference text, extract the Inception Date of the insurance policy.\n\nTreat the following labels as valid indicators of inception (case-insensitive):\nInception Date, Policy Inception Date, Effective Date, Start Date, Period of Insurance, Renewal Date, Renewal On,**RNL **, Incepts On, **due**, **Due Date**, **Renewal Due**, **Presentation Due**, **Renewal Presentation due**\n\nExtract a date only if it appears alongside one of these valid labels.\n\nIf the date includes day, month, and year, return it in DD/MM/YYYY format.\n\nIf the date includes only day and month, append the year from the email sent date and return in DD/MM/YYYY format.\n\nDo not extract any date based solely on the email sent date.\n\nIf no valid label or date is present, return an empty string: \"\".\n\nOutput only the extracted date \u2014 no extra text, quotes, or explanations.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193413",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Incepts_On",
                                        "value": "Incepts On"
                                    }
                                ],
                                "function_id": 17071,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Expires On",
                        "position": 14,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Main Cover Type from the input.\nThis refers to the level of motor insurance cover requested or provided (e.g., \"Comprehensive\", \"Third Party Only\", \"TPFT\").\nIf no valid cover type is found, return \"N/A\".",
                        "field_id": "193414",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean **Main Cover Type**.\nIf no cover type is found, return \"\""
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Main Cover Type@1"
                                    }
                                ],
                                "function_id": 17072,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Main Cover Type",
                        "position": 15,
                        "prompt": "Extract the Main Cover Type from the document. This refers to the level of motor insurance cover requested or provided.\n\nInstructions:\n\nLook for mentions of possible cover types which includes:\n\"Comprehensive\"(\"Comp\"),  \"Third Party Only\"(\"TPO\"), \"TPFT\" (\"Third Party, Fire & Theft\"), \"Laid Up Accidental Damage and Theft\"(Laid Up ADFT) or \"Laid Up Fire and Theft\"(\"Laid Up FT\").\n\nIdentify all occurrences of these terms throughout the document.\nExtract the most frequently occurring cover type.\n\nIf no valid cover type is found, return \"\".\n\nReturn only the exact cover type value, no extra text or explanation.\n\nOutput format:\n\nMain Cover Type: \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193415",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address",
                                        "value": "Party Address"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "insured_address: {{ \\ Party Address \\}}\n\nExtract the post code from the insured_address. The post code is typically an alphanumeric value located at the end of the address.\n\nIf no post code is found, return \"\"."
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Risk Postcode",
                        "position": 16,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193416",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Today_date",
                                        "value": "Today date"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Details",
                                        "value": "Driver Details"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "driver_details: {{ \\ Driver Details \\}}\n\ntoday_date: {{ \\ Today date \\}}\n\n\nThe driver_details table contains a column 'Driver Name' and 'Licence Date'.\n\nCalculate each driver's licence tenure in complete years using 'Licence Date' and today_date.\n\nIf 'Licence Date' is missing or empty in a row, return that row with the Licence Tenure as empty.\n\nReturn the final output as a table with two columns: 'Driver Name' , \u2018Licence Tenure'"
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Driver Licence Tenure@0"
                                    }
                                ],
                                "function_id": 17073,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Driver Licence Tenure",
                        "position": 26,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Agency Address from the provided text.\n\nDefinition:\n\nThe Agency Address refers to the address of the insurance broker, intermediary, or agent responsible for managing or placing the insurance policy.\n\nThis address is typically associated with organisations such as Gallagher, UBT, Arthur J. Gallagher, etc.\n\nInstructions:\n\nLook for addresses that are directly linked to broker personnel (e.g., account managers, account directors, insurance advisors) or company names known to be brokers/intermediaries.\n\nExclude any addresses related to the insured party (e.g., the policyholder or client like Hewer Group Ltd or Multi Turn Ltd).\n\nReturn the complete line(s) representing the address, usually found near the broker's name, company name, or email signature.\n\nIf multiple broker addresses are present, select the one associated with the contact sending or managing the case.\n\nIf no agency/broker address is identified, return \"N/A\".\n\nOutput:\n\nA single string containing the agency\u2019s physical address, or \"N/A\" if not found.",
                        "field_id": "193417",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Clean and return **only the address details**. This means:\n- Preserve the physical address lines.\n- Format it as a single clean string separated by commas.\n- If no valid address is present, return \"\"."
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Agency address",
                        "position": 9,
                        "prompt": "You are an expert in Insurance underwriting. Your task is to extract the correct Agency address from the OCR-processed Email.\n\nFollow the given instructions before extracting the address.\n\nInstructions:\n\n1. The Agency Address refers to the address of the insurance broker, intermediary, or agent responsible for managing or placing the insurance policy.This address is typically associated with organisations such as Gallagher, UBT, Arthur J. Gallagher, etc.\n\n2. Look for addresses that are directly linked to broker personnel (e.g., account managers, account directors, insurance advisors) or company names known to be brokers/intermediaries.\n\n3. Exclude any addresses related to the insured party (e.g., the policyholder or client like Hewer Group Ltd or Multi Turn Ltd).\n\n4. Return the complete line(s) representing the address, usually found near the broker's name, company name, or email signature.\n\n5. If multiple broker addresses are present, select the one associated with the contact sending or managing the case.\n\n6. Return the complete physical address exactly as it is displayed in the document, without omitting or reformatting any part.\n\n7. If no agency/broker address is identified, return \"\".\n\nOutput Format:\n\nA single string containing the agency\u2019s **physical address**, or \"\" if not found.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193418",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Incepts_On",
                                        "value": "Incepts On"
                                    }
                                ],
                                "function_id": 17074,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Effective From",
                        "position": 17,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193419",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Expires_On",
                                        "value": "Expires On"
                                    }
                                ],
                                "function_id": 17075,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Effective To",
                        "position": 18,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193420",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Is_Holding_Broker_Details",
                                        "value": "Is Holding Broker Details"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Is_Holding_Broker_From_Context",
                                        "value": "Is Holding Broker From Context"
                                    }
                                ],
                                "function_id": 17076,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Is Holding Broker",
                        "position": 21,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193421",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17077,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Transaction Type",
                        "position": 22,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Agency Contact \u2014 this is the name of the person who sent the email or prepared the Broker Presentation.\n\nInstructions:\n\nLook for the sender\u2019s name in the email body, especially near the signature section at the end of the email.\n\nIf the name is mentioned in the Broker Presentation, usually on the first or last page, extract it from there.\n\nReturn only the full name of the person (e.g., John Smith).\n\nDo not return any email addresses, phone numbers, or job titles.\n\nIf no name is found, return \"N/A\"",
                        "field_id": "193422",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "DEFAULT",
                        "name": "Agency Contact",
                        "position": 11,
                        "prompt": "Extract the Agency Contact \u2014 this is the name of the person who sent the original enquiry from the agency (broker/intermediary) to the insurer.\n\nInstructions:\n\nLook for the first email in the thread that is sent from a broker or intermediary (not the insurer).\n\nIdentify the sender\u2019s full name from that email, typically found near the signature block (e.g., at the bottom).\n\nReturn only the sender\u2019s full name .\n\nDo not return email addresses, phone numbers, or job titles.\n\nIf no agency contact name is found, return \"\".",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Task : Extract all driver-specific details from the Email body.\n\nFor each driver, extract the following fields. If a field is missing or not clearly present, use an empty string '':\n\nDriver Name \u2013 The full name of the driver.\nDriver DOB \u2013 The date of birth of the driver.\nLicence Date \u2013 The date when the driver obtained their license.\nConviction code \u2013 The conviction code(s), if any, associated with this driver(For example 'SP30', 'IN10' etc).\nDriver claim \u2013 Claim information related to this driver, if any.\n\nInstructions:\n--> Ensure that data is correctly attributed to the corresponding driver.\n--> If driver name is not available, eliminate that row.\n\nReturn the tabular format output as specified below without any additional text.\nExample Output format:\n[\n  [\"Driver Name\", \"Driver DOB\u201d, \"Licence Date\", \"Conviction code\", \"Driver Claims\"],\n  [\"<Driver Name>\", \"<Driver DOB>\", \"<Licence Date>\", \"<Conviction Code>\", \"<Driver Claims>\"],\n]\n\nLet's think step by step.",
                        "field_id": "193423",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Driver Details@0"
                                    }
                                ],
                                "function_id": 17078,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Driver Details",
                        "position": 24,
                        "prompt": "Task : Extract all driver-specific details from the Email body.\n\nFor each driver, extract the following fields. If a field is missing or not clearly present, use an empty string '':\n\nDriver Name \u2013 The full name of the driver(For example : \"Williams\").\nDriver DOB \u2013 The date of birth of the driver.\nLicence Date \u2013 The date when the driver obtained their license.\nConviction code \u2013 The conviction code(s), if any, associated with this driver(For example 'SP30', 'IN10' etc).\nDriver claim \u2013 Claim information related to this driver, if any.\n\nInstructions:\nEnsure that data is correctly attributed to the corresponding driver.\nIf driver name is not available, eliminate that row.\nDate Normalization Rules (for both DOB and Licence Date):\nIf the date is in dd/mm/yyyy format, keep it as is.\nIf only year \u2192 01/01/yyyy\nIf month+year \u2192 01/mm/yyyy\n\nOutput all dates as dd/mm/yyyy\nReturn the tabular format output as specified below without any additional text.\nExample Output format:\n[ [\"Driver Name\", \"Driver DOB\u201d, \"Licence Date\", \"Conviction Code\", \"Driver Claims\"], [\"<Driver Name>\", \"<Driver DOB>\", \"<Licence Date>\", \"<Conviction Code>\", \"<Driver Claims>\"] ]\n\nLet's think step by step.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193424",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Today_date",
                                        "value": "Today date"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Details",
                                        "value": "Driver Details"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "driver_details:{{ \\ Driver Details \\ }}\ntoday_date: {{ \\ Today date \\ }}\nthe driver_details table contains a column 'Driver Name' and 'Driver DOB'. \ncalculate each driver age in complete years using 'Driver DOB' and today_date\nif Driver DOB is missing or empty in a row return that row with the age as empty\nreturn final output as a table with two columns: 'Driver Name' , 'Age'"
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Driver Age@0"
                                    }
                                ],
                                "function_id": 17079,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Driver Age",
                        "position": 25,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193425",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17080,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Today date",
                        "position": 23,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193426",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Data_Merged",
                                        "value": "Driver Data Merged"
                                    }
                                ],
                                "function_id": 17081,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Drive Type",
                        "position": 28,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193427",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Details",
                                        "value": "Driver Details"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Age",
                                        "value": "Driver Age"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Licence_Tenure",
                                        "value": "Driver Licence Tenure"
                                    }
                                ],
                                "function_id": 17082,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Driver Data Merged",
                        "position": 27,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193428",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Driver_Data_Merged",
                                        "value": "Driver Data Merged"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Drive_Type",
                                        "value": "Drive Type"
                                    }
                                ],
                                "function_id": 17083,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Driver Party Table",
                        "position": 29,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Excess Type for Accident Damage, such as ADFT or ADFTWS.\nLook for values labeled under \"Excess Type\" or near the Accident Damage section.\nIf no such value is found, return \"N/A\".\n\nOutput:\nReturn either ADFT, ADFTWS, or \"N/A\"",
                        "field_id": "193429",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean numeric data associated with **Excess Value**.\nIf no Excess Value is found, return \"0\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Excess Type Accident Damage",
                        "position": 31,
                        "prompt": "Extract the Excess Type for All Damage, typically labeled as AD, ADFT, or ADFTWS.\n\nInstructions:\n\nLook for values labeled under or near \"Excess Type\", \"All Damage\", \"AD\", \"ADFT\", \"ADFTWS\".\nIf no such value is found, return an empty string \"\".\n\nOutput Format:\nExcess Value: \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the Excess Type for Accident Damage generally referred as AD or ADFT or ADFTWS. Look for values labeled under \"Excess Type\" .\nIf no such value is found, return \"N/A\".\n\nOutput:\nReturn excess value if not found retrun \"\"",
                        "field_id": "193430",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean numeric data associated with **Excess Value**.\nIf no Excess Value is found, return \"0\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Excess Type Fire",
                        "position": 30,
                        "prompt": "Extract the Excess Type for Fire, typically labeled under fire-related sections.\n\nInstructions:\n\nLook for values labeled under or near \"Fire Excess\", \"Excess Type - Fire\", \"ADFT\", \"ADFTWS\", \"ADF&F\", \"AD&FT\" or similar fire-related descriptions.\n\nIf no such value is found, return an empty string \"\".\n\nOutput Format:\nExcess Value: \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the values labeled under \"Excess Type\" for Fire Excess, also referred to as **Excess** or **ADFT** or **ADFTWS**. If no such value is found, return \"N/A\".\n\nOutput:\nReturn excess value if not found retrun \"\"",
                        "field_id": "193431",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean numeric data associated with **Excess Value**.\nIf no Excess Value is found, return \"0\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Excess Type Theft",
                        "position": 32,
                        "prompt": "Extract the Excess Type for Theft, typically labeled under theft-related sections.\n\nInstructions:\n\nLook for values labeled under \"Theft Excess\", \"Excess Type - Theft\", \"Theft\", \"ADFT\", \"ADF&T\", \"AD&FT\" or related terms.\nIf no such value is found, return an empty string \"\".\n\nOutput Format:\nExcess Value: \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the values labeled under \"Excess Type\" for Fire Excess, also referred to as **Excess** or **ADFT** or **ADFTWS**. If no such value is found, return \"N/A\".\n\nOutput:\nReturn excess value if not found retrun \"\"",
                        "field_id": "193432",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean numeric data associated with **Excess Value**.\nIf no Excess Value is found, return \"0\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Excess Type WS",
                        "position": 33,
                        "prompt": "Extract the Excess Type for Windscreen, typically labeled under windscreen-related sections.\n\nInstructions:\n\nLook for values labeled under or near \"Windscreen Excess\", \"Excess Type - Windscreen\", \"Windscreen\", \"ADFTWS\" , \"WS Excess\" or similar terms.\n\nIf no such value is found, return an empty string \"\".\n\nOutput Format:\nExcess Value: \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "**Vehicle Registration** - This column should contain the registration number (license plate) of each vehicle (e.g., \"AB12 XYZ\", \"XYZ 1234\").",
                        "field_id": "193433",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Return only the extracted data in a list of lists format like below:\n[[\"Vehicle Registration\", \"Cover Vehicle\"], [\"AB12 XYZ\", \"TPO\"], [\"XY99 ZZZ\", \"\"]]\nDo not include explanations, comments, or headings.\nDo not wrap the output in markdown or quote blocks.\n\nIf no valid data is found, return an empty list: [].\n\nAll unrelated or irrelevant content must be excluded from the output."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Vehicle Schedule Data@1"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Effective_From",
                                        "value": "Effective From"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Effective_To",
                                        "value": "Effective To"
                                    }
                                ],
                                "function_id": 17084,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Vehicle Schedule Data",
                        "position": 35,
                        "prompt": "Your task is to extract a table with two columns: \"Vehicle Registration\" (e.g., \"AB12 XYZ\", \"KL01AB1234\") and \"Cover Vehicle\" (e.g., \"TPO\", \"TPFT\", \"Comprehensive\", \"Comp\").\n\nStep1: Identify valid vehicle registration numbers and their associated cover types.\n\nStep2: Convert all vehicle registration numbers to uppercase.\n\nStep3: If a cover type is not found for a valid registration, include the registration number with an empty string.\n\nStep4: Ignore any entries without valid registration numbers and exclude irrelevant text (e.g., disclaimers or general notes).\n\nOutput Format:\n\n[ [\"Vehicle Registration\", \"Cover Vehicle\"], [\"AB12 XYZ\", \"TPO\"],]\nIf no valid rows found, return [] only.\n\nLet's think step by step",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193434",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17085,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Business Category",
                        "position": 34,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193435",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Schedule_Table",
                                        "value": "Vehicle Schedule Table"
                                    }
                                ],
                                "function_id": 17086,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Vehicle Count From Table",
                        "position": 40,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193436",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Number_of_Notifiable_Vehicles",
                                        "value": "Vehicle Count From Table"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Target_Price",
                                        "value": "Target Price"
                                    }
                                ],
                                "function_id": 17087,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Offering Type",
                        "position": 42,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193437",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Schedule_Data",
                                        "value": "Vehicle Schedule Data"
                                    }
                                ],
                                "function_id": 17088,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Unique_Values_Cover_Basis",
                        "position": 36,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193438",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Unique__Values__Cover__Basis",
                                        "value": "Unique_Values_Cover_Basis"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "For each value in the list {{\\ Unique_Values_Cover_Basis \\}}, generate a dictionary mapping where each value is normalised to one of the standard cover basis categories. You may receive values in full form (e.g., \"Third Party, Fire and Theft\"), as acronyms (e.g., \"TPFT\", \"TPO\", \"Comp\"), or as variations of known phrases (e.g., \"Comprehensive Cover\", \"Laid Up - Fire &amp; Theft\"). Use the following mapping rules: \"Third Party, Fire and Theft\" or similar \u2192 TPFT, \"Comprehensive\" or similar \u2192 Comp, \"Third Party Only\" or similar \u2192 TPO, \"Laid Up (Accidental Damage, Fire, and Theft)\" or similar \u2192 Laid Up (ADFT), \"Laid Up (Fire and Theft)\" , LUFT or similar \u2192 Laid up (FT Only). Direct acronyms like TPO, TPFT, Comp should map to themselves  exclude LUFT, Since LUFT -&gt; Laid up (FT Only). If a value is irrelevant or does not match any known category, map it to an empty string \"\" Return the output as a dictionary like this: {\u201coriginal value\u201d: mapped category, }"
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Cover Basis Mapping@0"
                                    }
                                ],
                                "function_id": 17089,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Cover Basis Mapping",
                        "position": 37,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193439",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Cover_Basis_Mapping",
                                        "value": "Cover Basis Mapping"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Unique__Values__Cover__Basis",
                                        "value": "Unique_Values_Cover_Basis"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Schedule_Data",
                                        "value": "Vehicle Schedule Data"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Effective_From",
                                        "value": "Effective From"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Effective_To",
                                        "value": "Effective To"
                                    }
                                ],
                                "function_id": 17090,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Vehicle Schedule Table",
                        "position": 38,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "Extract the number of vehicles mentioned in the Email.",
                        "field_id": "193440",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Just return the vehicle count, otherwise return \"\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Vehicle Count From Email",
                        "position": 39,
                        "prompt": "You are an expert at processing vehicle insurance Emails.\n\nYour task is to extract and return the numerical value for number of vehicles explicitly mentioned in the email. Look for phrases such as:\n\n\"Number of vehicles in MID\", \"Number of vehicles in vehicle schedule\", \"Total vehicles listed\", \"Vehicles submitted to MID\",\nor any similar phrasing indicating a specific count of vehicles.\n\nDo not calculate or count vehicle entries manually. Only extract the number if it is clearly and explicitly stated in close proximity to these phrases.\n\nIf no explicit count is mentioned, return \"\".\n\nLet's think step by step.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193441",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Count_From_Email",
                                        "value": "Vehicle Count From Email"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Count_From_Table",
                                        "value": "Vehicle Count From Table"
                                    }
                                ],
                                "function_id": 17091,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Number of Notifiable Vehicles",
                        "position": 41,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "From the provided email content, extract the Agency Code.\n\nThe Agency Code is a 7 digit numeric value.\n\nIt is commonly found in the **email subject** line, often after a hyphen (-) or embedded between other reference numbers.\n\nLook for patterns like:\n[Galelio Number]-[AgencyCode]-[Submission Name]\n\nIt is the second numeric value in a hyphen-separated subject string.\n\nExtract only the numeric Agency Code.\nIf no agency code is found, return an empty string \"\"",
                        "field_id": "193442",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "From the extracted text, return only the clean **Agency Code**.\n\nIf no Agency Code is found, return \"\""
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Agency Code",
                        "position": 43,
                        "prompt": "Extract the Agency Code from the provided email subject line.\n\nThe Agency Code is a 7-digit numeric value.\n\nIt is commonly found in the email subject.\n\nThe expected format is:\n\n[Galileo Number] \u2013 [Agency Code] \u2013 [Submission Name]\n\nExample:\nQ009258247 - 1169877 - FW EXTERNAL New Motor Fleet Enquiry\n\nSeparators may vary (e.g., hyphens, dashes, or spaces), but the Agency Code is typically the second 7-digit number in the sequence.\n\nExtract the 7-digit number that appears between the Galileo Number and the submission description.\n\nIf no valid Agency Code is found, return an empty string \"\"",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193443",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Main_Cover_Type",
                                        "value": "Main Cover Type"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Main_cover_type : {{ \\ Main Cover Type \\ }}\n\nValid Normalized Types:\n\n1) COMP \u2192 includes: \"Comp\", \"COMP\", \"Comprehensive\", \"Comprehensive Cover\"\n\n2) TPFT \u2192 includes: \"TPFT\", \"Third Party Fire and Theft\", \"Third Party, Fire and Theft\", \"Third Party Fire &amp; Theft\"\n\n3) TPO \u2192 includes: \"TPO\", \"Third Party Only\", \"Third Party\"\n\n4) Laid up (ADFT) \u2192 includes: \"Laid Up (Accidental Damage, Fire, and Theft)\", \"Laid Up ADFT\", \"Laid-Up ADFT\"\n\n5) Laid up (FT Only) \u2192 includes: \"Laid Up (Fire and Theft)\", \"LUFT\", \"Laid-Up FT\"\n\nInstructions:\n\nMatch the Main_cover_type with the above Valid Normalized types (case-insensitive, ignore minor formatting differences).\n\nIf matched, return the normalized type exactly as listed above.\n\nIf no valid match, return \"\"."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Main Cover Type Mapped@0"
                                    }
                                ],
                                "function_id": 17092,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Main Cover Type Mapped",
                        "position": 44,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193444",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Line_1",
                                        "value": "Party Address"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17093,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 2",
                        "position": 47,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193445",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17094,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 3",
                        "position": 48,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193446",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17095,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Country",
                        "position": 49,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193447",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17096,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address State",
                        "position": 50,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193448",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17097,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address City",
                        "position": 51,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193449",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17098,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Postcode",
                        "position": 52,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193450",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17099,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 1",
                        "position": 46,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193451",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address",
                                        "value": "Party Address"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "You are an intelligent address extraction assistant.\n\nGiven a full address string {{ \\ Party Address \\}}, extract the following components:\nAddress Line 1: Only the building number/name and street/road name. (Exclude landmarks, estates, zones, or locality info.)\nAddress Line 2: Estate, locality, landmark, or additional location detail\nAddress Line 3: Optional detail like block, zone, village, or area\nCity: Town or city name\nState: The state, province, or region\nCountry: Country name\nPostcode: postal code (UK format)\n\nExtraction Rules:\nDo not merge unrelated fields across lines\nDo not confuse postcode with city/state.\nReturn empty strings if any field is missing for that component \n{\n  \"Address Line 1\": \"\",\n  \"Address Line 2\": \"\",\n  \"Address Line 3\": \"\",\n  \"City\": \"\",\n  \"State\": \"\",\n  \"Country\": \"\",\n  \"Postcode\": \"\"\n}"
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Party Address Details@0"
                                    }
                                ],
                                "function_id": 17100,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Details",
                        "position": 45,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193452",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17101,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Trade Descriptions",
                        "position": 53,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193453",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Business_Description",
                                        "value": "Business Description"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Trade_Descriptions",
                                        "value": "Trade Descriptions"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "You are a domain expert in classifying business descriptions.\n\nYou are given:\n\nTrades_List: { \\ Trade Descriptions \\} \nBusiness_Description: { \\ Business Description \\}\n\nInstructions:\nFor the business description provided:\nCompare it against both the trade names and their aliases in Trades_List.\nDetermine the closest and similar matching **trade**.\nIf no suitable match is found, return \"Fleet - Unclassified\"."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "AXA Trade Description@0"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Business_Description",
                                        "value": "Business Description"
                                    }
                                ],
                                "function_id": 17102,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "AXA Trade Description@1"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Trade_Descriptions",
                                        "value": "Trade Descriptions"
                                    }
                                ],
                                "function_id": 17103,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "AXA Trade Description",
                        "position": 54,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193454",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Agency_Name",
                                        "value": "Agency Name"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Holding_Broker",
                                        "value": "Holding Broker"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "holding_broker: \\ Holding Broker \\\nagency_name: \\ Agency Name \\\n\nComparare `holding_broker` and `agency_name` to determine if they represent the same organization.\n\nRules:\n1. If `holding_broker` is an empty string, return: {\"holding_broker\": \"\", \"basis\": \"Holding broker is empty\"}.\n\n2.If the two names refer to the same or a similar entity \u2014 for example, when they differ only by abbreviations, suffix variations (e.g., \"Ltd\" vs \"Limited\", \"Corp\" vs \"Corporation\"), or extra generic words (e.g., \"Group\", \"Financial\") \u2014 return:\n{\"holding_broker\": \"Yes\", \"basis\": \"Names are similar after normalization\"}\n\n3. If they are different entities, return: {\"holding_broker\": \"No\", \"basis\": \"Names are different after normalization\"}.\n\nSteps:\n- Normalize both names: lowercase, remove legal suffixes (like ltd, corp, inc), and special characters.\n- Then compare using fuzzy similarity or normalized string match.\n\nReturn output strictly in JSON format with keys:\n- `holding_broker`: \"Yes\", \"No\", or \"\"\n- `basis`: explanation of the decision"
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Is Holding Broker Details",
                        "position": 19,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "You are given an email content. Identify if the Broker is the Holding Broker using the steps  below. \nReturn only a JSON result in this format:\n{\n  \"holding_broker\": \"<Yes | No | Blank>\",\n  \"source_text\": \"<matched phrase or explanation>\"\n}\n\nStep 1 \u2013 If the email contains (case-insensitive):\n\t\u2022\t\"we hold\"\n\t\u2022\t\"we hold with\"\n\t\u2022\t\"we currently hold\"\n\t\u2022\t\"we are the holding broker\"\n\t\u2022\t\"risk we hold\"\u2028Return:\n{ \"holding_broker\": \"Yes\", \"source_text\": \"<matched phrase>\" }\n\nStep 2 \u2013 If the email contains (case-insensitive):\n\t\u2022\t\"we are attacking this risk\"\n\t\u2022\t\"we are attacking this one\"\n\t\u2022\t\"we are not the holding broker\"\u2028Return:\n{ \"holding_broker\": \"No\", \"source_text\": \"<matched phrase>\" }\n\nStep 3 \u2013 If no match:\u2028Return:\n{ \"holding_broker\": \"\", \"source_text\": \"No identifiable holding broker in statement\" }\n\nLet's think step by step.",
                        "field_id": "193455",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Just return the final json output without any additional text."
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Is Holding Broker From Context",
                        "position": 20,
                        "prompt": "You are given an email content. Identify if the Broker is the Holding Broker using the steps  below. \n\nStep 1 \u2013 If the email contains (case insensitive, exact match required):\n\t\u2022\t\"we hold\"\n\t\u2022\t\"we hold with\"\n\t\u2022\t\"we currently hold\" but not fleet, insure\n\t\u2022\t\"we are the holding broker\"\n\t\u2022\t\"risk we hold\" but not by other broker\n\t\u2022\t\"existing client\u201d\n\t\u2022\t\"our client\u201d\n\t\u2022\t\"client of ours\u201d\u2028Return:\n{ \"holding_broker\": \"Yes\", \"source_text\": \"<matched phrase>\" }\n\nStep 2 \u2013 If the email contains (case insensitive, exact match required):\n\t\u2022\t\"we are attacking this risk\"\n\t\u2022\t\"we are attacking this one\"\n\t\u2022\t\"we are not the holding broker\"\n\t\u2022\t\"we hold all lines except fleet\"\u2028Return:\n{ \"holding_broker\": \"No\", \"source_text\": \"<matched phrase>\" }\n\nStep 3 \u2013 If no matches are from Step 1 and Step 2:\u2028Return:\n{ \"holding_broker\": \"\", \"source_text\": \"No identifiable holding broker in statement\" }\n\nLet's think step by step.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    }
                ]
            },
            "Haulage Fact Finder": {
                "class_id": "13547",
                "description": "Classify the document as a Haulage Fact Finder if it is a structured form or questionnaire used to collect information for motor fleet or haulage insurance. These documents typically include Renewal quotations as well.\n\nIf the document consists of keywords such as **Operator**, **Hazardous** or **Trailer** with in the \"Motor fleet fact finder document\", then classify it as **Haulage Fact Finder**.",
                "fields": [
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193574",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17186,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Haulage Fact Finder received",
                        "position": 0,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    }
                ]
            },
            "Other": {
                "class_id": "13543",
                "description": "",
                "fields": []
            },
            "Vehicle Schedule": {
                "class_id": "13546",
                "description": "Classify a document as \u201cVehicle Schedule\u201d if:\n\nThe document lists insured vehicles or motor claims, often under titles such as:\n**Motor Insurance Database (MID) List**,\n**Motor Vehicle List**,\n**Vehicle Schedule**,\n**Motor Fleet Schedule**\n\nAlso check for table headers or repeated row entries that include fields like:\nVehicle Registration Number (Reg No / VRN)\nCover Type\nMake, Model, Usage\n\nIf these patterns are present, classify as:\nDocument Type: Vehicle Schedule",
                "fields": [
                    {
                        "data_type": "TEXT",
                        "description": "You are an expert in processing commercial fleet insurance documents.\n\nYou are working with a document classified as a \"Vehicle Schedule\" - this is typically a structured list of vehicles insured under a policy. The document may appear as a spreadsheet, a table within a PDF, or part of a broker submission, and usually includes fields such as registration number, vehicle make/model, type of cover, and other vehicle-specific information.\n\nYour task is to extract the **Insured Name** - the legal name of the company or individual under whose name the vehicle fleet is insured.\n\nThe Insured Name is often found:\n- At the top of the document as a header\n- Near or alongside policy details like \"Policyholder\", \"Insured\", or \"Client Name\"\n- In the document title or footers (e.g., \"Fleet List for XYZ Logistics Ltd.\")\n- In company letterheads or metadata, especially in MID or policy-linked schedules\n\nIf no insured name or relevant phrase is found, return **\"N/A\"**.\n\nIgnore names that appear only in the context of individual drivers, vehicle owners, or claimants - focus on the entity responsible for the policy.\n\nReturn only the Insured Name, even if multiple names or addresses are present.",
                        "field_id": "193553",
                        "field_visibility": "VISIBLE",
                        "lines": [],
                        "model_type": "DEFAULT",
                        "name": "Insured",
                        "position": 0,
                        "prompt": "You are an expert in processing commercial fleet insurance documents.\n\nYou are working with a document classified as a \"Vehicle Schedule\" - this is typically a structured list of vehicles insured under a policy. The document may appear as a spreadsheet, a table within a PDF, or part of a broker submission, and usually includes fields such as registration number, vehicle make/model, type of cover, and other vehicle-specific information.\n\nYour task is to extract the **Insured Name** - the legal name of the company or individual under whose name the vehicle fleet is insured.\n\nThe Insured Name is often found:\n- At the top of the document as a header\n- Near or alongside policy details like \"Policyholder\", \"Insured\", or \"Client Name\"\n- In the document title or footers (e.g., \"Fleet List for XYZ Logistics Ltd.\")\n- In company letterheads or metadata, especially in MID or policy-linked schedules\n\nIf no insured name or relevant phrase is found, return \"\"\n\nIgnore names that appear only in the context of individual drivers, vehicle owners, or claimants - focus on the entity responsible for the policy.\n\nReturn only the Insured Name, even if multiple names or addresses are present.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "You are an expert in processing commercial fleet insurance submissions.\n\nYou are working with a document classified as a \"Vehicle Schedule\". While the main content focuses on listing insured vehicles, these documents often contain metadata such as the name and address of the insured party.\n\nYour task is to extract the **Party Address** - the full address of the insured entity (also referred to as the policyholder or client).\n\nThe address may appear:\n- Near the top of the document, under labels such as \"Address\", \"Client Address\", \"Insured Address\", or as part of a header block\n- Below or next to the Insured Name\n- In a section containing policy or administrative details\n- In a footer or cover page, if the schedule is part of a larger submission\n\nThe address typically includes:\n- Street name and number\n- City or locality\n- Postcode (or ZIP code)\n- Country (optional)\n\nIf no address can be reliably located, return the value: **\"N/A\"**.\n\nDo not extract email addresses, website URLs, or claim-related contact information. Return only the physical or postal address associated with the insured party.",
                        "field_id": "193554",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Clean and return **only the address details**. This means:\n- Preserve the physical address lines.\n- Format it as a single clean string separated by commas.\n- If no valid address is present, return \"\"."
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address",
                        "position": 1,
                        "prompt": "You are an expert in processing commercial fleet insurance submissions.\n\nYou are working with a document classified as a \"Vehicle Schedule\". While the main content focuses on listing insured vehicles, these documents often contain metadata such as the name and address of the insured party.\n\nYour task is to extract the **Party Address** - the full address of the insured entity (also referred to as the policyholder or client).\n\nThe address may appear:\n- Near the top of the document, under labels such as \"Address\", \"Client Address\", \"Insured Address\", or as part of a header block\n- Below or next to the Insured Name\n- In a section containing policy or administrative details\n- In a footer or cover page, if the schedule is part of a larger submission\n\nThe address typically includes:\n- Street name and number\n- City or locality\n- Postcode (or ZIP code)\n- Country (optional)\n\nReturn the complete address exactly as it is displayed in the document, without omitting, reformatting, or adding any explanation.\n\nIf no address can be reliably located, return \"\"\n\nDo not extract email addresses, website URLs, or claim-related contact information. Return only the physical or postal address associated with the insured party.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "You are an expert in processing commercial fleet insurance documents.\n\nYou are working with a document classified as a \"Vehicle Schedule\", which contains a list of insured vehicles and their details.\n\nYour task is to extract the **Cover Type** based on the most frequently occurring value in the relevant column.\n\nCover Type will be found in a column labelled something like:\n- \"Cover\"\n- \"Type of Cover\"\n- \"Coverage\"\n\nFrom this column, identify the most commonly listed cover type and return only that value. If not mentioned, return N/A\n\nOnly the following values are acceptable:\n- Comprehensive  \n- Third party  \n- TPO  \n- TPFT  \n\nNormalise variations as needed (e.g., \"Third Party\" \u2192 \"Third party\").\n\nDo not return multiple values. Return only the single most frequent valid cover type from the list above.",
                        "field_id": "193555",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Main Cover Type@0"
                                    }
                                ],
                                "function_id": 17170,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Main Cover Type",
                        "position": 2,
                        "prompt": "You are an expert in processing commercial fleet insurance documents.\n\nYou are working with a document classified as a \"Vehicle Schedule\", which contains a list of insured vehicles and their details.\n\nYour task is to extract the **Cover Type** based on the most frequently occurring value in the relevant column.\n\nCover Type will be found in a column labelled something like:\n- \"Cover\"\n- \"Type of Cover\"\n- \"Coverage\"\n\nFrom this column, identify the most commonly listed cover type and return only that value. If not mentioned, return \"\"\n\nOnly the following values are acceptable:\n- Comprehensive (Comp)  \n- Third party Only (TPO)\n- TPFT (Third Party Fire and Theft)\n- LUFT (Laid Up - Fire and Theft)\n- LU ADFT (Laid Up - Accidental damage, fire and theft)\n\nNormalise variations as needed (e.g., \"Third Party\" \u2192 \"Third party\").\n\nDo not return multiple values. Return only the single most frequent valid cover type from the list above.",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193556",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address",
                                        "value": "Party Address"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "insured_address: {{ \\ Party Address \\ }}\n\nExtract the post code from the insured_address. The post code is typically an alphanumeric value located at the end of the address.\n\nIf no post code is found, return \"\"."
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Risk Postcode",
                        "position": 3,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "You are working with a Vehicle Schedule document from a commercial fleet insurance submission. Extract a structured table with the following two columns:\n\nCover Vehicle \u2013 Extract cover type (e.g., \"TPO\", \"COMP\", \"Comprehensive\") only if found under relevant headers like Cover, Cover Type, etc.\n\nVehicle Registration \u2013 Extract registration number (e.g., \"AB12 XYZ\") only if found under headers like Vehicle Reg, Registration, Reg, Registration Number, Vehicle Registration Number, etc.\n\nIdentify relevant columns before extracting\u2014do not infer or guess values. If a relevant column or data is missing for a vehicle, set that field as \"N/A\". Ignore all unrelated attributes.\n\nReturn the output only as a list of lists in tabular format as shown below. If no valid data is found, return \"N/A\" only.\n\nOutput Format:\n[[\"Cover Vehicle\", \"Vehicle Registration\"], [],]",
                        "field_id": "193557",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Vehicle Schedule Data@0"
                                    }
                                ],
                                "function_id": 17171,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Vehicle Schedule Data",
                        "position": 6,
                        "prompt": "Scan and Extract a structured table with the following two columns:\n\t\u2022\tCover Vehicle \u2013 Extract the cover type (e.g., \"TPO\", \"COMP\", \"Comprehensive\") only if found under relevant headers like Cover, Cover Type, etc. Remove unnecessary brackets if any from Cover Vehicle.\n\t\u25e6\tIf a cover type is mentioned at the top or bottom (e.g., under \"Cover\" or \"Cover Type\") and applies to all vehicles, use that value for all vehicles unless overridden in the rows.\n\t\u2022\tVehicle Registration \u2013 Extract the registration number (e.g., \"AB12 XYZ\",\"C120674\") only if found under headers like **Serial No/ID Etc**, Chassis Number, Vehicle Reg, etc. (Registration numbers are always alphanumeric).\n\t\u25e6\tInclude Serial No and Chassis Number values as Vehicle Registration if found, even if no Cover Vehicle is available. In such cases, keep Cover Vehicle as an empty string \"\".\nImportant Rules:\n\t1\tStrictly do not extract irrelevant data like product names or unrelated fields.\n\t2\tKeep duplicate Vehicle Registration numbers as well.\n\t3\tDo not change the order of Vehicle Registrations\u2014maintain the same order as in the document.\n\t4\tIdentify relevant columns before extracting\u2014do not infer or guess values.\n\t5\tIf a relevant column or data is missing for a vehicle, set that field as \"\".\n\t6\tIgnore all unrelated attributes or values.\n\t7\tDrop rows from extraction if they do not contain a valid registration number.\nReturn Format:\nReturn the final answer only as a list of lists in the following structure, without any additional explanation or text:\n\n[[\"Cover Vehicle\", \"Vehicle Registration\"], [\"Comp\", \"KV79XOC\"], [\"Comp\", \"VV59XPF\"], [\"\", \"SN123456\"], [\"\", \"CH987654\"]]",
                        "prompt_schema": null,
                        "prompt_type": "advanced"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193558",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17172,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Business Category",
                        "position": 4,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193559",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [],
                                "function_id": 17173,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "ADVANCED",
                        "name": "Transaction Type",
                        "position": 5,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193560",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Schedule_Data",
                                        "value": "Vehicle Schedule Data"
                                    }
                                ],
                                "function_id": 17174,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Unique_Values_Cover_Basis",
                        "position": 7,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193561",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Schedule_Data",
                                        "value": "Vehicle Schedule Data"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Unique__Values__Cover__Basis",
                                        "value": "Unique_Values_Cover_Basis"
                                    },
                                    {
                                        "data_type": "FIELD",
                                        "name": "Cover_Basis_Mapping",
                                        "value": "Cover Basis Mapping"
                                    }
                                ],
                                "function_id": 17175,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Vehicle Schedule Table",
                        "position": 9,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193562",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Vehicle_Schedule_Table",
                                        "value": "Vehicle Schedule Table"
                                    }
                                ],
                                "function_id": 17176,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Number of Notifiable Vehicles",
                        "position": 10,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193563",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Number_of_Notifiable_Vehicles",
                                        "value": "Number of Notifiable Vehicles"
                                    }
                                ],
                                "function_id": 17177,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Offering Type VST",
                        "position": 11,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193564",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Unique__Values__Cover__Basis",
                                        "value": "Unique_Values_Cover_Basis"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "For each value in the list {{\\ Unique_Values_Cover_Basis \\}}, generate a dictionary mapping where each value is normalised to one of the standard cover basis categories. You may receive values in full form (e.g., \"Third Party, Fire and Theft\"), as acronyms (e.g., \"TPFT\", \"TPO\", \"Comp\"), or as variations of known phrases (e.g., \"Comprehensive Cover\", \"Laid Up - Fire &amp; Theft\"). Use the following mapping rules: \"Third Party, Fire and Theft\" or similar \u2192 TPFT, \"Comprehensive\" or similar \u2192 Comp, \"Third Party Only\" or similar \u2192 TPO, \"Laid Up (Accidental Damage, Fire, and Theft)\" or similar \u2192 Laid Up (ADFT), \"Laid Up (Fire and Theft)\" , LUFT or similar \u2192 Laid up (FT Only). Direct acronyms like TPO, TPFT, Comp should map to themselves exclude LUFT, Since LUFT -&gt; Laid up (FT Only). If a value is irrelevant or does not match any known category, map it to an empty string \"\" Return the output as a dictionary like this: {\u201coriginal value\u201d: mapped category, }"
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Cover Basis Mapping",
                        "position": 8,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193565",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Main_Cover_Type",
                                        "value": "Main Cover Type"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "Main_cover_type : {{ \\ Main Cover Type \\ }}\n\nValid Normalized Types:\n\n1) COMP \u2192 includes: \"Comp\", \"COMP\", \"Comprehensive\", \"Comprehensive Cover\"\n\n2) TPFT \u2192 includes: \"TPFT\", \"Third Party Fire and Theft\", \"Third Party, Fire and Theft\", \"Third Party Fire &amp; Theft\"\n\n3) TPO \u2192 includes: \"TPO\", \"Third Party Only\", \"Third Party\"\n\n4) Laid up (ADFT) \u2192 includes: \"Laid Up (Accidental Damage, Fire, and Theft)\", \"Laid Up ADFT\", \"Laid-Up ADFT\"\n\n5) Laid up (FT Only) \u2192 includes: \"Laid Up (Fire and Theft)\", \"LUFT\", \"Laid-Up FT\"\n\nInstructions:\n\nMatch the Main_cover_type with the above Valid Normalized types (case-insensitive, ignore minor formatting differences).\n\nIf matched, return the normalized type exactly as listed above.\n\nIf no valid match, return \"\"."
                            },
                            {
                                "function_args": [
                                    {
                                        "data_type": "LINE",
                                        "name": "previous_line",
                                        "value": "Main Cover Type Mapped@0"
                                    }
                                ],
                                "function_id": 17178,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Main Cover Type Mapped",
                        "position": 12,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193566",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17179,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 2",
                        "position": 15,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193567",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17180,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 3",
                        "position": 16,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193568",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17181,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Country",
                        "position": 17,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193569",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17182,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address State",
                        "position": 18,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193570",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17183,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address City",
                        "position": 19,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193571",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17184,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Postcode",
                        "position": 20,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193572",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address_Details",
                                        "value": "Party Address Details"
                                    }
                                ],
                                "function_id": 17185,
                                "line_type": "UDF",
                                "native_fn_name": null,
                                "prompt": null
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Line 1",
                        "position": 14,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "udf"
                    },
                    {
                        "data_type": "TEXT",
                        "description": "",
                        "field_id": "193573",
                        "field_visibility": "VISIBLE",
                        "lines": [
                            {
                                "function_args": [
                                    {
                                        "data_type": "FIELD",
                                        "name": "Party_Address",
                                        "value": "Party Address"
                                    }
                                ],
                                "function_id": null,
                                "line_type": "PROMPT",
                                "native_fn_name": null,
                                "prompt": "You are an intelligent address extraction assistant.\n\nGiven a full address string {{ \\ Party Address \\ }}, extract the following components:\nAddress Line 1: Only the building number/name and street/road name. (Exclude landmarks, estates, zones, or locality info.)\nAddress Line 2: Estate, locality, landmark, or additional location detail\nAddress Line 3: Optional detail like block, zone, village, or area\nCity: Town or city name\nState: The state, province, or region\nCountry: Country name\nPostcode: postal code (UK format)\n\nExtraction Rules:\nDo not merge unrelated fields across lines\nDo not confuse postcode with city/state.\nReturn empty strings if any field is missing for that component \n{\n  \"Address Line 1\": \"\",\n  \"Address Line 2\": \"\",\n  \"Address Line 3\": \"\",\n  \"City\": \"\",\n  \"State\": \"\",\n  \"Country\": \"\",\n  \"Postcode\": \"\"\n}"
                            }
                        ],
                        "model_type": "DEFAULT",
                        "name": "Party Address Details",
                        "position": 13,
                        "prompt": "",
                        "prompt_schema": null,
                        "prompt_type": "derived"
                    }
                ]
            }
        }
    },
    "project_udfs": {
        "17068": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Target Price@1"
                }
            ],
            "code": "\n\ndef unnamed_custom_function(previous_line, context = {}, keys = {}, **kwargs):\n\treturn previous_line.replace(\"\u00a3\",\"\").replace(\",\",\"\")",
            "docstring": null,
            "function_code": "\treturn previous_line.replace(\"\u00a3\",\"\").replace(\",\",\"\")",
            "id": 17068,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/914ee07e-052c-4f59-9873-84891e3e37f4_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/914ee07e-052c-4f59-9873-84891e3e37f4.json",
            "type": "REFINER"
        },
        "17069": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Broker Deadline@1"
                }
            ],
            "code": "\n\ndef adding_business_logic(previous_line, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime, timedelta\n\t\n\t\n\tdef add_calendar_days(start_date, days):\n\t    \"\"\"Add calendar days (including weekends) to a given date.\"\"\"\n\t    return start_date + timedelta(days=days)\n\t\n\tdef add_weekdays(start_date, days):\n\t    \"\"\"Add weekdays to a given date, skipping weekends.\"\"\"\n\t    while days > 0:\n\t        start_date += timedelta(days=1)\n\t        if start_date.weekday() < 5:  # Weekdays: Mon-Fri = 0-4\n\t            days -= 1\n\t    return start_date\n\t\n\tdef process_date(input_str):\n\t    today = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)\n\t\n\t    # Try to parse dd/mm/yyyy\n\t    try:\n\t        parsed_date = datetime.strptime(input_str, \"%d/%m/%Y\")\n\t    except ValueError:\n\t        # If year not present, try dd/mm check\n\t        try:\n\t            try:\n\t                parsed_date = datetime.strptime(input_str, \"%d/%m\")\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t            except:\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t        except ValueError:\n\t            final_date = add_calendar_days(today, 5)\n\t            return final_date.strftime(\"%d/%m/%Y\")\n\t            \n\t    # Compare dates\n\t    base_date = parsed_date if parsed_date >= today else today\n\t\n\t    return base_date.strftime(\"%d/%m/%Y\")\n\t\n\tresult = process_date(previous_line) \n\treturn result",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime, timedelta\n\t\n\t\n\tdef add_calendar_days(start_date, days):\n\t    \"\"\"Add calendar days (including weekends) to a given date.\"\"\"\n\t    return start_date + timedelta(days=days)\n\t\n\tdef add_weekdays(start_date, days):\n\t    \"\"\"Add weekdays to a given date, skipping weekends.\"\"\"\n\t    while days > 0:\n\t        start_date += timedelta(days=1)\n\t        if start_date.weekday() < 5:  # Weekdays: Mon-Fri = 0-4\n\t            days -= 1\n\t    return start_date\n\t\n\tdef process_date(input_str):\n\t    today = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)\n\t\n\t    # Try to parse dd/mm/yyyy\n\t    try:\n\t        parsed_date = datetime.strptime(input_str, \"%d/%m/%Y\")\n\t    except ValueError:\n\t        # If year not present, try dd/mm check\n\t        try:\n\t            try:\n\t                parsed_date = datetime.strptime(input_str, \"%d/%m\")\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t            except:\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t        except ValueError:\n\t            final_date = add_calendar_days(today, 5)\n\t            return final_date.strftime(\"%d/%m/%Y\")\n\t            \n\t    # Compare dates\n\t    base_date = parsed_date if parsed_date >= today else today\n\t\n\t    return base_date.strftime(\"%d/%m/%Y\")\n\t\n\tresult = process_date(previous_line) \n\treturn result",
            "id": 17069,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/44eb448b-8d39-4bb3-81d5-e3d64992b242_output.json",
            "name": "adding_business_logic",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/44eb448b-8d39-4bb3-81d5-e3d64992b242.json",
            "type": "REFINER"
        },
        "17070": {
            "args": [],
            "code": "\n\ndef default_value(context = {}, keys = {}, **kwargs):\n\treturn \"Fleet\"",
            "docstring": null,
            "function_code": "\treturn \"Fleet\"",
            "id": 17070,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/bcf13204-6348-4f58-9430-794bb0939a47_output.json",
            "name": "default_value",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/bcf13204-6348-4f58-9430-794bb0939a47.json",
            "type": "REFINER"
        },
        "17071": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Incepts_On",
                    "value": "Incepts On"
                }
            ],
            "code": "\n\ndef unnamed_custom_function(Incepts_On, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime, timedelta\n\t\n\tdef add_one_year_minus_one_day(date_str):\n\t    # Parse the input date (DD/MM/YYYY)\n\t    input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t    try:\n\t        # Try to add one year directly\n\t        one_year_later = input_date.replace(year=input_date.year + 1)\n\t    except ValueError:\n\t        # Handle Feb 29 (leap year issue) and other invalid dates\n\t        temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t        one_year_later = temp_date\n\t    # Subtract one day\n\t    final_date = one_year_later - timedelta(days=1)\n\t    # Return formatted date as DD/MM/YYYY with leading zeros\n\t    return final_date.strftime(\"%d/%m/%Y\")\n\t    \n\ttry:\n\t    expiry_date = add_one_year_minus_one_day(Incepts_On)\n\t    return expiry_date\n\texcept Exception as e:\n\t    return \"\"\n\t\n\t",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime, timedelta\n\t\n\tdef add_one_year_minus_one_day(date_str):\n\t    # Parse the input date (DD/MM/YYYY)\n\t    input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t    try:\n\t        # Try to add one year directly\n\t        one_year_later = input_date.replace(year=input_date.year + 1)\n\t    except ValueError:\n\t        # Handle Feb 29 (leap year issue) and other invalid dates\n\t        temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t        one_year_later = temp_date\n\t    # Subtract one day\n\t    final_date = one_year_later - timedelta(days=1)\n\t    # Return formatted date as DD/MM/YYYY with leading zeros\n\t    return final_date.strftime(\"%d/%m/%Y\")\n\t    \n\ttry:\n\t    expiry_date = add_one_year_minus_one_day(Incepts_On)\n\t    return expiry_date\n\texcept Exception as e:\n\t    return \"\"\n\t\n\t",
            "id": 17071,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/3b0642b5-6d61-42f8-ab49-3a5f8ca9b225_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/3b0642b5-6d61-42f8-ab49-3a5f8ca9b225.json",
            "type": "REFINER"
        },
        "17072": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Main Cover Type@1"
                }
            ],
            "code": "\n\ndef check_cover_type(previous_line, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
            "id": 17072,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/5f64974f-13fb-42ba-8ed9-5ffd77e22056_output.json",
            "name": "check_cover_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/5f64974f-13fb-42ba-8ed9-5ffd77e22056.json",
            "type": "REFINER"
        },
        "17073": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Driver Licence Tenure@0"
                }
            ],
            "code": "\n\ndef clean_llm_result(previous_line, context = {}, keys = {}, **kwargs):\n\tcolumns = ['Driver Name', 'Licence Tenure']\n\tif previous_line == '[]':\n\t    return [columns]\n\telse:\n\t  return previous_line",
            "docstring": null,
            "function_code": "\tcolumns = ['Driver Name', 'Licence Tenure']\n\tif previous_line == '[]':\n\t    return [columns]\n\telse:\n\t  return previous_line",
            "id": 17073,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/352a062b-e7a4-4a68-af68-dac37862fcff_output.json",
            "name": "clean_llm_result",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/352a062b-e7a4-4a68-af68-dac37862fcff.json",
            "type": "REFINER"
        },
        "17074": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Incepts_On",
                    "value": "Incepts On"
                }
            ],
            "code": "\n\ndef get_inception_date(Incepts_On, context = {}, keys = {}, **kwargs):\n\treturn Incepts_On",
            "docstring": null,
            "function_code": "\treturn Incepts_On",
            "id": 17074,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/cdc271aa-ae88-4c57-9cb1-946de8e41082_output.json",
            "name": "get_inception_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/cdc271aa-ae88-4c57-9cb1-946de8e41082.json",
            "type": "REFINER"
        },
        "17075": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Expires_On",
                    "value": "Expires On"
                }
            ],
            "code": "\n\ndef get_expiry_date(Expires_On, context = {}, keys = {}, **kwargs):\n\treturn Expires_On",
            "docstring": null,
            "function_code": "\treturn Expires_On",
            "id": 17075,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/26d6d2bc-1ed5-44a1-b842-32c693580535_output.json",
            "name": "get_expiry_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/26d6d2bc-1ed5-44a1-b842-32c693580535.json",
            "type": "REFINER"
        },
        "17076": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Is_Holding_Broker_Details",
                    "value": "Is Holding Broker Details"
                },
                {
                    "data_type": "FIELD",
                    "name": "Is_Holding_Broker_From_Context",
                    "value": "Is Holding Broker From Context"
                }
            ],
            "code": "\n\ndef return_holding_broker_value(Is_Holding_Broker_Details, Is_Holding_Broker_From_Context, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef extract_holding_broker_from_text(text: str) -> str:\n\t    brace_stack = []\n\t    start_idx = -1\n\t    end_idx = -1\n\t\n\t    # Step 1: Find the first complete {...} block\n\t    for i, ch in enumerate(text):\n\t        if ch == '{':\n\t            if not brace_stack:\n\t                start_idx = i\n\t            brace_stack.append('{')\n\t        elif ch == '}':\n\t            if brace_stack:\n\t                brace_stack.pop()\n\t                if not brace_stack:\n\t                    end_idx = i + 1  # Include the closing brace\n\t                    break\n\t\n\t    # Step 2: If we found a full JSON block\n\t    if start_idx != -1 and end_idx != -1:\n\t        json_str = text[start_idx:end_idx]\n\t\n\t        try:\n\t            data = json.loads(json_str)\n\t            print(data)\n\t            holding_broker = data.get('holding_broker')  \n\t            print(\"holding broker\", holding_broker)\n\t            return holding_broker\n\t        except Exception as e:\n\t            print(e)\n\t            return ''\n\t\n\t    return ''\n\t\n\tdef normalize_holding_broker(value: str) -> str:\n\t    value = value.strip().lower()\n\t    if value == 'yes':\n\t        return 'Yes'\n\t    elif value == 'no':\n\t        return 'No'\n\t    return ''\n\t\n\tholding_broker_explicit = extract_holding_broker_from_text(Is_Holding_Broker_Details)\n\tholding_broker_context = extract_holding_broker_from_text(Is_Holding_Broker_From_Context)\n\t\n\tholding_broker_explicit = normalize_holding_broker(holding_broker_explicit)\n\tholding_broker_context = normalize_holding_broker(holding_broker_context)\n\t\n\tprint(\"holding_broker_explicit\",holding_broker_explicit)\n\tprint(\"holding_broker_context\",holding_broker_context)\n\t\n\t# Prefer explicit if available\n\tif holding_broker_explicit:\n\t    print('if')\n\t    result = holding_broker_explicit\n\telse:\n\t    print('else')\n\t    result = holding_broker_context\n\treturn result",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef extract_holding_broker_from_text(text: str) -> str:\n\t    brace_stack = []\n\t    start_idx = -1\n\t    end_idx = -1\n\t\n\t    # Step 1: Find the first complete {...} block\n\t    for i, ch in enumerate(text):\n\t        if ch == '{':\n\t            if not brace_stack:\n\t                start_idx = i\n\t            brace_stack.append('{')\n\t        elif ch == '}':\n\t            if brace_stack:\n\t                brace_stack.pop()\n\t                if not brace_stack:\n\t                    end_idx = i + 1  # Include the closing brace\n\t                    break\n\t\n\t    # Step 2: If we found a full JSON block\n\t    if start_idx != -1 and end_idx != -1:\n\t        json_str = text[start_idx:end_idx]\n\t\n\t        try:\n\t            data = json.loads(json_str)\n\t            print(data)\n\t            holding_broker = data.get('holding_broker')  \n\t            print(\"holding broker\", holding_broker)\n\t            return holding_broker\n\t        except Exception as e:\n\t            print(e)\n\t            return ''\n\t\n\t    return ''\n\t\n\tdef normalize_holding_broker(value: str) -> str:\n\t    value = value.strip().lower()\n\t    if value == 'yes':\n\t        return 'Yes'\n\t    elif value == 'no':\n\t        return 'No'\n\t    return ''\n\t\n\tholding_broker_explicit = extract_holding_broker_from_text(Is_Holding_Broker_Details)\n\tholding_broker_context = extract_holding_broker_from_text(Is_Holding_Broker_From_Context)\n\t\n\tholding_broker_explicit = normalize_holding_broker(holding_broker_explicit)\n\tholding_broker_context = normalize_holding_broker(holding_broker_context)\n\t\n\tprint(\"holding_broker_explicit\",holding_broker_explicit)\n\tprint(\"holding_broker_context\",holding_broker_context)\n\t\n\t# Prefer explicit if available\n\tif holding_broker_explicit:\n\t    print('if')\n\t    result = holding_broker_explicit\n\telse:\n\t    print('else')\n\t    result = holding_broker_context\n\treturn result",
            "id": 17076,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/135a511b-7275-4ae2-872f-ae3ff84ed6d1_output.json",
            "name": "return_holding_broker_value",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/135a511b-7275-4ae2-872f-ae3ff84ed6d1.json",
            "type": "REFINER"
        },
        "17077": {
            "args": [],
            "code": "\n\ndef get_transaction_type(context = {}, keys = {}, **kwargs):\n\treturn \"New Business\"",
            "docstring": null,
            "function_code": "\treturn \"New Business\"",
            "id": 17077,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/62c6511a-0435-44d9-9233-d917f88eed57_output.json",
            "name": "get_transaction_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/62c6511a-0435-44d9-9233-d917f88eed57.json",
            "type": "REFINER"
        },
        "17078": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Driver Details@0"
                }
            ],
            "code": "\n\ndef get_cleaned_df(previous_line, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json, re, ast\n\timport traceback\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = [\n\t         \"Driver Name\",\n\t         \"Driver DOB\",\n\t         \"Licence Date\",\n\t         \"Conviction Code\",\n\t          \"Driver Claims\"\n\t    ]\n\t\n\t\n\tdef get_df_with_regex_match(previous_line, columns_order):\n\t    parsed, inner_content = None, None\n\t    df = pd.DataFrame(columns=columns_order)\n\t    try:\n\t        match = re.search(r\"```(?:\\w+)?\\s*([\\s\\S]*?)\\s*```\", previous_line)\n\t        if match:\n\t            inner_content = match.group(1).strip()\n\t    \n\t            # Step 2: Try to parse it as JSON\n\t            try:\n\t                parsed = json.loads(inner_content)\n\t                print(parsed)\n\t            except json.JSONDecodeError:\n\t                # If JSON parsing fails, fallback to literal_eval\n\t                try:\n\t                    parsed = ast.literal_eval(inner_content)\n\t                except Exception as e:\n\t                    print(\"Parsing failed:\", e)\n\t                    parsed = None\n\t          \n\t        if parsed:\n\t            df = convert_to_dataframe(parsed)\n\t            return True, df \n\t        else:\n\t            return False, df \n\t    except Exception as e:\n\t      return False, pd.DataFrame(columns=columns_order)\n\t\n\tdef parse_markdown_format(previous_line, columns_order):\n\t    try:\n\t        lines = [line.strip() for line in previous_line.strip().split('\\n') if line.strip().startswith('|') and '---' not in line]\n\t        list_of_lists = [ [cell.strip() for cell in line.strip('|').split('|')] for line in lines ]\n\t        if list_of_lists:\n\t            df = convert_to_dataframe(list_of_lists)\n\t            return True, df\n\t        else:\n\t            return False, pd.DataFrame(columns=columns_order)\n\t    except:\n\t        return False, pd.DataFrame(columns=columns_order)\n\t\n\ttry:\n\t    \n\t    df = pd.DataFrame(columns=columns_order)\n\t    \n\t    flg, df = get_df_with_regex_match(previous_line, columns_order)\n\t    print(flg)\n\t    if not flg:\n\t        flg, df = parse_markdown_format(previous_line, columns_order)\n\t    # print(previous_line)\n\t    print(flg)\n\t    if not flg:\n\t      try:\n\t          data = json.loads(previous_line)\n\t          # print(\"data\",data)\n\t          df = convert_to_dataframe(data)\n\t          # print(\"1st\",df)\n\t\n\t      except Exception as e:\n\t          data = ast.literal_eval(previous_line)\n\t          df = convert_to_dataframe(data)\n\t\n\t    # print(\"2nd \",df)\n\t    df.fillna(\"\", inplace=True)\n\t    \n\t    if len(df.columns.tolist()) > 0:\n\t        # df[\"Conviction Code\"] = df[\"Conviction Code\"].replace([\"No\", \"no\"], \"\")\n\t        df[\"Conviction Code\"] = df[\"Conviction Code\"].astype(str).str.strip().replace([\"no\", \"No\"], \"\")\n\t        df[\"Licence Date\"] = df[\"Licence Date\"].apply(\n\t    lambda x: f\"01/01/{x.strip()}\" if re.fullmatch(r\"\\s*(19|20)\\d{2}\\s*\", str(x)) else x)\n\t        df = df[columns_order]\n\t        df = [df.columns.tolist()] + df.values.tolist()\n\t        return df\n\t    else:\n\t        return [columns_order]\n\t      \n\texcept Exception as e:\n\t    return [columns_order]",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json, re, ast\n\timport traceback\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = [\n\t         \"Driver Name\",\n\t         \"Driver DOB\",\n\t         \"Licence Date\",\n\t         \"Conviction Code\",\n\t          \"Driver Claims\"\n\t    ]\n\t\n\t\n\tdef get_df_with_regex_match(previous_line, columns_order):\n\t    parsed, inner_content = None, None\n\t    df = pd.DataFrame(columns=columns_order)\n\t    try:\n\t        match = re.search(r\"```(?:\\w+)?\\s*([\\s\\S]*?)\\s*```\", previous_line)\n\t        if match:\n\t            inner_content = match.group(1).strip()\n\t    \n\t            # Step 2: Try to parse it as JSON\n\t            try:\n\t                parsed = json.loads(inner_content)\n\t                print(parsed)\n\t            except json.JSONDecodeError:\n\t                # If JSON parsing fails, fallback to literal_eval\n\t                try:\n\t                    parsed = ast.literal_eval(inner_content)\n\t                except Exception as e:\n\t                    print(\"Parsing failed:\", e)\n\t                    parsed = None\n\t          \n\t        if parsed:\n\t            df = convert_to_dataframe(parsed)\n\t            return True, df \n\t        else:\n\t            return False, df \n\t    except Exception as e:\n\t      return False, pd.DataFrame(columns=columns_order)\n\t\n\tdef parse_markdown_format(previous_line, columns_order):\n\t    try:\n\t        lines = [line.strip() for line in previous_line.strip().split('\\n') if line.strip().startswith('|') and '---' not in line]\n\t        list_of_lists = [ [cell.strip() for cell in line.strip('|').split('|')] for line in lines ]\n\t        if list_of_lists:\n\t            df = convert_to_dataframe(list_of_lists)\n\t            return True, df\n\t        else:\n\t            return False, pd.DataFrame(columns=columns_order)\n\t    except:\n\t        return False, pd.DataFrame(columns=columns_order)\n\t\n\ttry:\n\t    \n\t    df = pd.DataFrame(columns=columns_order)\n\t    \n\t    flg, df = get_df_with_regex_match(previous_line, columns_order)\n\t    print(flg)\n\t    if not flg:\n\t        flg, df = parse_markdown_format(previous_line, columns_order)\n\t    # print(previous_line)\n\t    print(flg)\n\t    if not flg:\n\t      try:\n\t          data = json.loads(previous_line)\n\t          # print(\"data\",data)\n\t          df = convert_to_dataframe(data)\n\t          # print(\"1st\",df)\n\t\n\t      except Exception as e:\n\t          data = ast.literal_eval(previous_line)\n\t          df = convert_to_dataframe(data)\n\t\n\t    # print(\"2nd \",df)\n\t    df.fillna(\"\", inplace=True)\n\t    \n\t    if len(df.columns.tolist()) > 0:\n\t        # df[\"Conviction Code\"] = df[\"Conviction Code\"].replace([\"No\", \"no\"], \"\")\n\t        df[\"Conviction Code\"] = df[\"Conviction Code\"].astype(str).str.strip().replace([\"no\", \"No\"], \"\")\n\t        df[\"Licence Date\"] = df[\"Licence Date\"].apply(\n\t    lambda x: f\"01/01/{x.strip()}\" if re.fullmatch(r\"\\s*(19|20)\\d{2}\\s*\", str(x)) else x)\n\t        df = df[columns_order]\n\t        df = [df.columns.tolist()] + df.values.tolist()\n\t        return df\n\t    else:\n\t        return [columns_order]\n\t      \n\texcept Exception as e:\n\t    return [columns_order]",
            "id": 17078,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/844a0976-55df-4e10-9128-5c29b93b3f90_output.json",
            "name": "get_cleaned_df",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/844a0976-55df-4e10-9128-5c29b93b3f90.json",
            "type": "REFINER"
        },
        "17079": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Driver Age@0"
                }
            ],
            "code": "\n\ndef clean_llm_result(previous_line, context = {}, keys = {}, **kwargs):\n\tcolumns = ['Driver Name', 'Age']\n\tif previous_line == '[]':\n\t    return [columns]\n\telse:\n\t  return previous_line",
            "docstring": null,
            "function_code": "\tcolumns = ['Driver Name', 'Age']\n\tif previous_line == '[]':\n\t    return [columns]\n\telse:\n\t  return previous_line",
            "id": 17079,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/68710c60-85df-4fe5-87ff-7ef73fe2c363_output.json",
            "name": "clean_llm_result",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/68710c60-85df-4fe5-87ff-7ef73fe2c363.json",
            "type": "REFINER"
        },
        "17080": {
            "args": [],
            "code": "\n\ndef get_todays_date(context = {}, keys = {}, **kwargs):\n\tfrom datetime import date\n\ttoday = str(date.today())\n\tprint(today)\n\treturn today",
            "docstring": null,
            "function_code": "\tfrom datetime import date\n\ttoday = str(date.today())\n\tprint(today)\n\treturn today",
            "id": 17080,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2c38e145-7512-4ebf-82ea-bdd81c511897_output.json",
            "name": "get_todays_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2c38e145-7512-4ebf-82ea-bdd81c511897.json",
            "type": "REFINER"
        },
        "17081": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Driver_Data_Merged",
                    "value": "Driver Data Merged"
                }
            ],
            "code": "\n\ndef get_driver_type(Driver_Data_Merged, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\t\n\tdef classify_driver(row):\n\t    try:\n\t        age = int(row[\"Age\"])\n\t        conviction_code = str(row[\"Conviction Code\"]).strip()\n\t        \n\t        if age < 25:\n\t            return \"Young Driver\"\n\t        elif age >= 25 and str(conviction_code).strip() not in [\"\", None,\"N/A\",\"null\"]:\n\t            return \"Convicted Driver\"\n\t        else:\n\t            return \"\"\n\t    except:\n\t        return \"\"  # Fallback in case of data issues\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Driver Type']\n\t\n\ttry:\n\t  table = json.loads(Driver_Data_Merged)\n\t  df = convert_to_dataframe(table)\n\t  df[\"Driver Type\"] = df.apply(classify_driver, axis=1)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\t\n\tdef classify_driver(row):\n\t    try:\n\t        age = int(row[\"Age\"])\n\t        conviction_code = str(row[\"Conviction Code\"]).strip()\n\t        \n\t        if age < 25:\n\t            return \"Young Driver\"\n\t        elif age >= 25 and str(conviction_code).strip() not in [\"\", None,\"N/A\",\"null\"]:\n\t            return \"Convicted Driver\"\n\t        else:\n\t            return \"\"\n\t    except:\n\t        return \"\"  # Fallback in case of data issues\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Driver Type']\n\t\n\ttry:\n\t  table = json.loads(Driver_Data_Merged)\n\t  df = convert_to_dataframe(table)\n\t  df[\"Driver Type\"] = df.apply(classify_driver, axis=1)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
            "id": 17081,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/72343869-0bdc-450a-88c0-5e9e469262b0_output.json",
            "name": "get_driver_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/72343869-0bdc-450a-88c0-5e9e469262b0.json",
            "type": "REFINER"
        },
        "17082": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Driver_Details",
                    "value": "Driver Details"
                },
                {
                    "data_type": "FIELD",
                    "name": "Driver_Age",
                    "value": "Driver Age"
                },
                {
                    "data_type": "FIELD",
                    "name": "Driver_Licence_Tenure",
                    "value": "Driver Licence Tenure"
                }
            ],
            "code": "\n\ndef merge_driver_details(Driver_Details, Driver_Age, Driver_Licence_Tenure, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\ttry:\n\t  Driver_Details = json.loads(Driver_Details)\n\t  Driver_Age = json.loads(Driver_Age)\n\t  Driver_Licence_Tenure = json.loads(Driver_Licence_Tenure)\n\t  merged_df = []\n\t  \n\t  try:\n\t      Driver_Details = convert_to_dataframe(Driver_Details)\n\t      merged_df = Driver_Details\n\t  except Exception as e:\n\t      print(\"no driver details \", e)\n\t\n\t  try:\n\t      Driver_Age = convert_to_dataframe(Driver_Age)\n\t  except Exception as e:\n\t      print(\"no driver conviction details \", e)\n\t\n\t  try:\n\t      Driver_Licence_Tenure = convert_to_dataframe(Driver_Licence_Tenure)\n\t  except Exception as e:\n\t      print(\"no driver licence details \", e)\n\t\n\t  # print(Driver_Details.columns)\n\t  # print(Driver_Age.columns)\n\t\n\t  try:\n\t      merged_df = pd.merge(Driver_Details, Driver_Age,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(\"no driver licence details \", e)\n\t\n\t  try:\n\t      merged_df_all = pd.merge(merged_df, Driver_Licence_Tenure, on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      merged_df_all = merged_df.copy()\n\t      merged_df_all.fillna('', inplace=True)\n\t      if len(merged_df_all.columns.tolist()) > 0:\n\t          merged_Df = [merged_df_all.columns.tolist()] + merged_df_all.values.tolist()\n\t          print(\"merging issue \", e)\n\t          return merged_Df\n\t      else:\n\t          return [[\"Driver DOB\", \"Licence Date\", \"Conviction Code\", \"Age\", \"Driver Name\", \"Licence Tenure\"]]\n\t\n\t  merged_df_all.fillna('', inplace=True)\n\t\n\t  if len(merged_df_all.columns.tolist()) > 0:\n\t      merged_Df = [merged_df_all.columns.tolist()] + merged_df_all.values.tolist()\n\t      return merged_Df\n\t  else:\n\t      return [[\"Driver DOB\", \"Licence Date\", \"Conviction Code\", \"Age\", \"Driver Name\", \"Licence Tenure\"]]\n\texcept:\n\t  return [[\"Driver DOB\", \"Licence Date\", \"Conviction Code\", \"Age\", \"Driver Name\", \"Licence Tenure\"]]",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\ttry:\n\t  Driver_Details = json.loads(Driver_Details)\n\t  Driver_Age = json.loads(Driver_Age)\n\t  Driver_Licence_Tenure = json.loads(Driver_Licence_Tenure)\n\t  merged_df = []\n\t  \n\t  try:\n\t      Driver_Details = convert_to_dataframe(Driver_Details)\n\t      merged_df = Driver_Details\n\t  except Exception as e:\n\t      print(\"no driver details \", e)\n\t\n\t  try:\n\t      Driver_Age = convert_to_dataframe(Driver_Age)\n\t  except Exception as e:\n\t      print(\"no driver conviction details \", e)\n\t\n\t  try:\n\t      Driver_Licence_Tenure = convert_to_dataframe(Driver_Licence_Tenure)\n\t  except Exception as e:\n\t      print(\"no driver licence details \", e)\n\t\n\t  # print(Driver_Details.columns)\n\t  # print(Driver_Age.columns)\n\t\n\t  try:\n\t      merged_df = pd.merge(Driver_Details, Driver_Age,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(\"no driver licence details \", e)\n\t\n\t  try:\n\t      merged_df_all = pd.merge(merged_df, Driver_Licence_Tenure, on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      merged_df_all = merged_df.copy()\n\t      merged_df_all.fillna('', inplace=True)\n\t      if len(merged_df_all.columns.tolist()) > 0:\n\t          merged_Df = [merged_df_all.columns.tolist()] + merged_df_all.values.tolist()\n\t          print(\"merging issue \", e)\n\t          return merged_Df\n\t      else:\n\t          return [[\"Driver DOB\", \"Licence Date\", \"Conviction Code\", \"Age\", \"Driver Name\", \"Licence Tenure\"]]\n\t\n\t  merged_df_all.fillna('', inplace=True)\n\t\n\t  if len(merged_df_all.columns.tolist()) > 0:\n\t      merged_Df = [merged_df_all.columns.tolist()] + merged_df_all.values.tolist()\n\t      return merged_Df\n\t  else:\n\t      return [[\"Driver DOB\", \"Licence Date\", \"Conviction Code\", \"Age\", \"Driver Name\", \"Licence Tenure\"]]\n\texcept:\n\t  return [[\"Driver DOB\", \"Licence Date\", \"Conviction Code\", \"Age\", \"Driver Name\", \"Licence Tenure\"]]",
            "id": 17082,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/4a47c5c8-9fc5-4caf-88a1-3bea9d8d6058_output.json",
            "name": "merge_driver_details",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/4a47c5c8-9fc5-4caf-88a1-3bea9d8d6058.json",
            "type": "REFINER"
        },
        "17083": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Driver_Data_Merged",
                    "value": "Driver Data Merged"
                },
                {
                    "data_type": "FIELD",
                    "name": "Drive_Type",
                    "value": "Drive Type"
                }
            ],
            "code": "\n\ndef form_driver_table(Driver_Data_Merged, Drive_Type, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json, traceback\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\ttry:\n\t  Driver_Data_Merged = json.loads(Driver_Data_Merged)\n\t  Drive_Type = json.loads(Drive_Type)\n\t\n\t  merged_df = []\n\t\n\t  try:\n\t      Driver_Data_Merged = convert_to_dataframe(Driver_Data_Merged)\n\t  except Exception as e:\n\t      print(e)\n\t      print(traceback.format_exc())\n\t  try:\n\t      Drive_Type = convert_to_dataframe(Drive_Type)\n\t  except Exception as e:\n\t      merged_df = Driver_Data_Merged.copy()\n\t      print(traceback.format_exc())\n\t      print(e)\n\t\n\t  print(Driver_Data_Merged.columns,\"hi\", Drive_Type.columns)\n\t  try:\n\t      merged_df = pd.merge(Driver_Data_Merged, Drive_Type,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(e)\n\t      print(traceback.format_exc())\n\t  \n\t  try:\n\t      rename_dict = {\n\t        'Driver Name': 'Driver Name',\n\t        'Driver DOB' : 'Driver D.O.B',\n\t        'Licence Date': 'Drivers Licence - Date obtained',\n\t        'Licence Tenure':'Driver: Years Appropriate Licence Held',\n\t        'Conviction Code': 'Conviction Details',\n\t        'Driver Type': 'Driver Type'\n\t      }\n\t\n\t      safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t      merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t      \n\t      columns_order = [ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']\n\t\n\t      for col in columns_order:\n\t          if col not in merged_df.columns:\n\t              merged_df[col] = ''\n\t      \n\t      merged_df['Driver: Years Appropriate Licence Held'] = merged_df['Driver: Years Appropriate Licence Held'].applymap(lambda x: x if x in ['1', '2', '3', '4', '5'] else '5+')\n\t\n\t      existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t                  \n\t      merged_df.replace('N/A', '', inplace=True)\n\t      merged_df.replace('\"\"', '', inplace=True)\n\t      merged_df.replace('null', '', inplace=True)\n\t\n\t      merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t      merged_df = merged_df.dropna(how='all')\n\t      # merged_df = merged_df.where(pd.notna(merged_df), \"\")\n\t\n\t      merged_df = merged_df.fillna('')\n\t\n\t      merged_df = merged_df[existing_columns]\n\t\n\t      # merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t  except Exception as e:\n\t      print(\"Exception in renaming \", e)\n\t\n\t      for col in columns_order:\n\t          if col not in merged_df.columns:\n\t              merged_df[col] = ''\n\t      \n\t      if len(merged_df.columns.tolist()) > 0:\n\t          merged_df = merged_df[columns_order]\n\t          merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t          return merged_df\n\t      else:\n\t          return [columns_order]\n\t  \n\t  if len(merged_df.columns.tolist()) > 0:\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept:\n\t  return [[ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']]",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json, traceback\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\ttry:\n\t  Driver_Data_Merged = json.loads(Driver_Data_Merged)\n\t  Drive_Type = json.loads(Drive_Type)\n\t\n\t  merged_df = []\n\t\n\t  try:\n\t      Driver_Data_Merged = convert_to_dataframe(Driver_Data_Merged)\n\t  except Exception as e:\n\t      print(e)\n\t      print(traceback.format_exc())\n\t  try:\n\t      Drive_Type = convert_to_dataframe(Drive_Type)\n\t  except Exception as e:\n\t      merged_df = Driver_Data_Merged.copy()\n\t      print(traceback.format_exc())\n\t      print(e)\n\t\n\t  print(Driver_Data_Merged.columns,\"hi\", Drive_Type.columns)\n\t  try:\n\t      merged_df = pd.merge(Driver_Data_Merged, Drive_Type,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(e)\n\t      print(traceback.format_exc())\n\t  \n\t  try:\n\t      rename_dict = {\n\t        'Driver Name': 'Driver Name',\n\t        'Driver DOB' : 'Driver D.O.B',\n\t        'Licence Date': 'Drivers Licence - Date obtained',\n\t        'Licence Tenure':'Driver: Years Appropriate Licence Held',\n\t        'Conviction Code': 'Conviction Details',\n\t        'Driver Type': 'Driver Type'\n\t      }\n\t\n\t      safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t      merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t      \n\t      columns_order = [ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']\n\t\n\t      for col in columns_order:\n\t          if col not in merged_df.columns:\n\t              merged_df[col] = ''\n\t      \n\t      merged_df['Driver: Years Appropriate Licence Held'] = merged_df['Driver: Years Appropriate Licence Held'].applymap(lambda x: x if x in ['1', '2', '3', '4', '5'] else '5+')\n\t\n\t      existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t                  \n\t      merged_df.replace('N/A', '', inplace=True)\n\t      merged_df.replace('\"\"', '', inplace=True)\n\t      merged_df.replace('null', '', inplace=True)\n\t\n\t      merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t      merged_df = merged_df.dropna(how='all')\n\t      # merged_df = merged_df.where(pd.notna(merged_df), \"\")\n\t\n\t      merged_df = merged_df.fillna('')\n\t\n\t      merged_df = merged_df[existing_columns]\n\t\n\t      # merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t  except Exception as e:\n\t      print(\"Exception in renaming \", e)\n\t\n\t      for col in columns_order:\n\t          if col not in merged_df.columns:\n\t              merged_df[col] = ''\n\t      \n\t      if len(merged_df.columns.tolist()) > 0:\n\t          merged_df = merged_df[columns_order]\n\t          merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t          return merged_df\n\t      else:\n\t          return [columns_order]\n\t  \n\t  if len(merged_df.columns.tolist()) > 0:\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept:\n\t  return [[ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']]",
            "id": 17083,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a3a8ac1b-ffa3-4aea-9875-770f3c81735c_output.json",
            "name": "form_driver_table",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a3a8ac1b-ffa3-4aea-9875-770f3c81735c.json",
            "type": "REFINER"
        },
        "17084": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Vehicle Schedule Data@1"
                },
                {
                    "data_type": "FIELD",
                    "name": "Effective_From",
                    "value": "Effective From"
                },
                {
                    "data_type": "FIELD",
                    "name": "Effective_To",
                    "value": "Effective To"
                }
            ],
            "code": "\n\ndef form_vehicle_schedule(previous_line, Effective_From, Effective_To, context = {}, keys = {}, **kwargs):\n\t\n\timport pandas as pd\n\timport json, re\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\tcolumn_order = ['Effective From', 'Effective To','Vehicle Registration', 'Cover - Vehicle']\n\tprint(previous_line)\n\ttry:\n\t    try:\n\t        vehicel_schedule_data = json.loads(str(previous_line))\n\t    except:\n\t        # previous_line = previous_line.replace(\"```\",\"\").replace(\"json\",\"\")\n\t        matches = re.findall(r'\\[.*?\\]', previous_line)\n\t        \n\t        if len(matches) == 1:\n\t            vehicel_schedule_data = json.loads(matches[0])\n\t        else:\n\t            # Case 2: Multiple lists, convert all\n\t            vehicel_schedule_data = [json.loads(m) for m in matches]\n\t\n\t    merged_df = []\n\t    \n\t    try:\n\t        vehicel_schedule_data = convert_to_dataframe(vehicel_schedule_data)\n\t        \n\t    except Exception as e:\n\t        print(\"no vehicel_schedule_data details \", e)\n\t    \n\t    try:\n\t        vehicel_schedule_data['Effective From'] = Effective_From\n\t    except:\n\t        vehicel_schedule_data['Effective From'] = ''\n\t    \n\t    try:\n\t        vehicel_schedule_data['Effective To'] = Effective_To\n\t    except:\n\t        vehicel_schedule_data['Effective To'] = ''\n\t\n\t    vehicel_schedule_data.fillna('', inplace=True)\n\t    vehicel_schedule_data = vehicel_schedule_data.replace('N/A', '')\n\t    vehicel_schedule_data = vehicel_schedule_data.replace('\"\"', '')\n\t    try:\n\t        vehicel_schedule_data = vehicel_schedule_data.rename(columns={'Cover Vehicle': 'Cover - Vehicle'})\n\t    except Exception as e:\n\t        print(e)\n\t\n\t    for col in column_order:\n\t        if col not in vehicel_schedule_data.columns:\n\t            vehicel_schedule_data[col] = ''\n\t\n\t\n\t    if len(vehicel_schedule_data.columns.tolist()) > 0:\n\t        vehicel_schedule_data = vehicel_schedule_data[column_order]\n\t        vehicel_schedule_data = [vehicel_schedule_data.columns.tolist()] + vehicel_schedule_data.values.tolist()\n\t        return vehicel_schedule_data\n\t    else:\n\t        return [['Effective From', 'Effective To','Vehicle Registration', 'Cover - Vehicle']]\n\texcept:\n\t    return [['Effective From', 'Effective To','Vehicle Registration', 'Cover - Vehicle']]",
            "docstring": null,
            "function_code": "\t\n\timport pandas as pd\n\timport json, re\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\tcolumn_order = ['Effective From', 'Effective To','Vehicle Registration', 'Cover - Vehicle']\n\tprint(previous_line)\n\ttry:\n\t    try:\n\t        vehicel_schedule_data = json.loads(str(previous_line))\n\t    except:\n\t        # previous_line = previous_line.replace(\"```\",\"\").replace(\"json\",\"\")\n\t        matches = re.findall(r'\\[.*?\\]', previous_line)\n\t        \n\t        if len(matches) == 1:\n\t            vehicel_schedule_data = json.loads(matches[0])\n\t        else:\n\t            # Case 2: Multiple lists, convert all\n\t            vehicel_schedule_data = [json.loads(m) for m in matches]\n\t\n\t    merged_df = []\n\t    \n\t    try:\n\t        vehicel_schedule_data = convert_to_dataframe(vehicel_schedule_data)\n\t        \n\t    except Exception as e:\n\t        print(\"no vehicel_schedule_data details \", e)\n\t    \n\t    try:\n\t        vehicel_schedule_data['Effective From'] = Effective_From\n\t    except:\n\t        vehicel_schedule_data['Effective From'] = ''\n\t    \n\t    try:\n\t        vehicel_schedule_data['Effective To'] = Effective_To\n\t    except:\n\t        vehicel_schedule_data['Effective To'] = ''\n\t\n\t    vehicel_schedule_data.fillna('', inplace=True)\n\t    vehicel_schedule_data = vehicel_schedule_data.replace('N/A', '')\n\t    vehicel_schedule_data = vehicel_schedule_data.replace('\"\"', '')\n\t    try:\n\t        vehicel_schedule_data = vehicel_schedule_data.rename(columns={'Cover Vehicle': 'Cover - Vehicle'})\n\t    except Exception as e:\n\t        print(e)\n\t\n\t    for col in column_order:\n\t        if col not in vehicel_schedule_data.columns:\n\t            vehicel_schedule_data[col] = ''\n\t\n\t\n\t    if len(vehicel_schedule_data.columns.tolist()) > 0:\n\t        vehicel_schedule_data = vehicel_schedule_data[column_order]\n\t        vehicel_schedule_data = [vehicel_schedule_data.columns.tolist()] + vehicel_schedule_data.values.tolist()\n\t        return vehicel_schedule_data\n\t    else:\n\t        return [['Effective From', 'Effective To','Vehicle Registration', 'Cover - Vehicle']]\n\texcept:\n\t    return [['Effective From', 'Effective To','Vehicle Registration', 'Cover - Vehicle']]",
            "id": 17084,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/51d025c9-203c-402b-bb0b-623afd4195fd_output.json",
            "name": "form_vehicle_schedule",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/51d025c9-203c-402b-bb0b-623afd4195fd.json",
            "type": "REFINER"
        },
        "17085": {
            "args": [],
            "code": "\n\ndef return_business_category(context = {}, keys = {}, **kwargs):\n\treturn \"Mid Market\"",
            "docstring": null,
            "function_code": "\treturn \"Mid Market\"",
            "id": 17085,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/0099a5c4-ffc4-4361-9c05-8034d9403ada_output.json",
            "name": "return_business_category",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/0099a5c4-ffc4-4361-9c05-8034d9403ada.json",
            "type": "REFINER"
        },
        "17086": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Schedule_Table",
                    "value": "Vehicle Schedule Table"
                }
            ],
            "code": "\n\ndef get_number_of_notifiable_vehicles(Vehicle_Schedule_Table, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame( data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\ttry:\n\t  if Vehicle_Schedule_Table == \"[]\" or Vehicle_Schedule_Table == 'n/a':\n\t    return \"0\"\n\t  else:\n\t    Vehicle_Schedule_Table = json.loads(Vehicle_Schedule_Table)\n\t    Vehicle_Schedule_Table = convert_to_dataframe(Vehicle_Schedule_Table)\n\t    non_empty_count = Vehicle_Schedule_Table['Vehicle Registration'].replace('', np.nan).dropna().shape[0]\n\t    return non_empty_count\n\texcept:\n\t  return \"0\"",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame( data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\ttry:\n\t  if Vehicle_Schedule_Table == \"[]\" or Vehicle_Schedule_Table == 'n/a':\n\t    return \"0\"\n\t  else:\n\t    Vehicle_Schedule_Table = json.loads(Vehicle_Schedule_Table)\n\t    Vehicle_Schedule_Table = convert_to_dataframe(Vehicle_Schedule_Table)\n\t    non_empty_count = Vehicle_Schedule_Table['Vehicle Registration'].replace('', np.nan).dropna().shape[0]\n\t    return non_empty_count\n\texcept:\n\t  return \"0\"",
            "id": 17086,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/99f85194-3acc-42fe-8716-440df971e3ad_output.json",
            "name": "get_number_of_notifiable_vehicles",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/99f85194-3acc-42fe-8716-440df971e3ad.json",
            "type": "REFINER"
        },
        "17087": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Number_of_Notifiable_Vehicles",
                    "value": "Vehicle Count From Table"
                },
                {
                    "data_type": "FIELD",
                    "name": "Target_Price",
                    "value": "Target Price"
                }
            ],
            "code": "\n\ndef get_offering_type(Number_of_Notifiable_Vehicles, Target_Price, context = {}, keys = {}, **kwargs):\n\t# def clean_premium(premium_value):\n\t#     if premium_value is None:\n\t#         return None\n\t#     if isinstance(premium_value, (int, float)):\n\t#         return float(premium_value)\n\t#     try:\n\t#         # Remove currency symbols, commas, and whitespace\n\t#         # print(premium_value)\n\t#         # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t#         cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t#         print(cleaned)\n\t#         return float(cleaned)\n\t#     except Exception:\n\t#         return None\n\t# def determine_offering_type(vehicle_count, premium_cleaned=None):\n\t#     if vehicle_count is None:\n\t#         return \"\"\n\t#     if vehicle_count <= 19:\n\t#         if premium_cleaned is None or premium_cleaned < 10000:\n\t#             return \"Mini Fleet\"\n\t#     elif 20 <= vehicle_count <= 149:\n\t#         if premium_cleaned is None or 10000 <= premium_cleaned < 250000:\n\t#             # print(\"yes\")\n\t#             return \"Vantage Fleet\"\n\t#     elif vehicle_count >= 150:\n\t#         if premium_cleaned is None or premium_cleaned >= 250000:\n\t#             return \"Mid Corp\"\n\t#     return \"None\"\n\t# try:\n\t#     Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\t# except:\n\t#     return \"None\"\n\t# try:\n\t#     premium_cleaned = clean_premium(Target_Price)\n\t#     print(premium_cleaned)\n\t# except:\n\t#     premium_cleaned = None\n\t\n\t# offering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\t# return offering_type\n\t\n\t\n\tdef clean_premium(premium_value):\n\t    if premium_value is None:\n\t        return None\n\t    if isinstance(premium_value, (int, float)):\n\t        return float(premium_value)\n\t    try:\n\t        # Remove currency symbols, commas, and whitespace\n\t        # print(premium_value)\n\t        # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t        cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t        print(cleaned)\n\t        return float(cleaned)\n\t    except Exception:\n\t        return None\n\t\n\tdef determine_offering_type(vehicle_count, premium_cleaned=None):    \n\t\n\t    if vehicle_count == 0:\n\t        if premium_cleaned is None or premium_cleaned < 10000:\n\t            return \"Mini Fleet\"\n\t        \n\t        elif 10000 <= premium_cleaned < 250000:\n\t            return \"Vantage Fleet\"\n\t        \n\t        elif premium_cleaned >= 250000:\n\t            return \"Mid Corp\"\n\t\n\t    if vehicle_count <= 19:\n\t            return \"Mini Fleet\"\n\t            \n\t    elif 20 <= vehicle_count <= 149:\n\t            return \"Vantage Fleet\"\n\t        \n\t    elif vehicle_count >= 150:\n\t            return \"Mid Corp\"\n\t\n\t\n\ttry:\n\t    Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\texcept:\n\t    Number_of_Notifiable_Vehicles = 0\n\t\n\ttry:\n\t    premium_cleaned = clean_premium(Target_Price)\n\texcept:\n\t    premium_cleaned = None\n\t\n\toffering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\treturn offering_type",
            "docstring": null,
            "function_code": "\t# def clean_premium(premium_value):\n\t#     if premium_value is None:\n\t#         return None\n\t#     if isinstance(premium_value, (int, float)):\n\t#         return float(premium_value)\n\t#     try:\n\t#         # Remove currency symbols, commas, and whitespace\n\t#         # print(premium_value)\n\t#         # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t#         cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t#         print(cleaned)\n\t#         return float(cleaned)\n\t#     except Exception:\n\t#         return None\n\t# def determine_offering_type(vehicle_count, premium_cleaned=None):\n\t#     if vehicle_count is None:\n\t#         return \"\"\n\t#     if vehicle_count <= 19:\n\t#         if premium_cleaned is None or premium_cleaned < 10000:\n\t#             return \"Mini Fleet\"\n\t#     elif 20 <= vehicle_count <= 149:\n\t#         if premium_cleaned is None or 10000 <= premium_cleaned < 250000:\n\t#             # print(\"yes\")\n\t#             return \"Vantage Fleet\"\n\t#     elif vehicle_count >= 150:\n\t#         if premium_cleaned is None or premium_cleaned >= 250000:\n\t#             return \"Mid Corp\"\n\t#     return \"None\"\n\t# try:\n\t#     Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\t# except:\n\t#     return \"None\"\n\t# try:\n\t#     premium_cleaned = clean_premium(Target_Price)\n\t#     print(premium_cleaned)\n\t# except:\n\t#     premium_cleaned = None\n\t\n\t# offering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\t# return offering_type\n\t\n\t\n\tdef clean_premium(premium_value):\n\t    if premium_value is None:\n\t        return None\n\t    if isinstance(premium_value, (int, float)):\n\t        return float(premium_value)\n\t    try:\n\t        # Remove currency symbols, commas, and whitespace\n\t        # print(premium_value)\n\t        # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t        cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t        print(cleaned)\n\t        return float(cleaned)\n\t    except Exception:\n\t        return None\n\t\n\tdef determine_offering_type(vehicle_count, premium_cleaned=None):    \n\t\n\t    if vehicle_count == 0:\n\t        if premium_cleaned is None or premium_cleaned < 10000:\n\t            return \"Mini Fleet\"\n\t        \n\t        elif 10000 <= premium_cleaned < 250000:\n\t            return \"Vantage Fleet\"\n\t        \n\t        elif premium_cleaned >= 250000:\n\t            return \"Mid Corp\"\n\t\n\t    if vehicle_count <= 19:\n\t            return \"Mini Fleet\"\n\t            \n\t    elif 20 <= vehicle_count <= 149:\n\t            return \"Vantage Fleet\"\n\t        \n\t    elif vehicle_count >= 150:\n\t            return \"Mid Corp\"\n\t\n\t\n\ttry:\n\t    Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\texcept:\n\t    Number_of_Notifiable_Vehicles = 0\n\t\n\ttry:\n\t    premium_cleaned = clean_premium(Target_Price)\n\texcept:\n\t    premium_cleaned = None\n\t\n\toffering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\treturn offering_type",
            "id": 17087,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/9ac456f9-1563-4d49-8f5f-5543854cd807_output.json",
            "name": "get_offering_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/9ac456f9-1563-4d49-8f5f-5543854cd807.json",
            "type": "REFINER"
        },
        "17088": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Schedule_Data",
                    "value": "Vehicle Schedule Data"
                }
            ],
            "code": "\n\ndef get_unique_values_from_cover_basis_col(Vehicle_Schedule_Data, context = {}, keys = {}, **kwargs):\n\t# # Import Python packages\n\timport json\n\timport pandas as pd\n\t\n\t# list_of_dct = json.loads(Vehicle_Schedule_Data)\n\t# # print(list_of_dct)\n\t\n\t# try:\n\t#     unique_cover_basis_set = set(item['Cover Vehicle'] for item in list_of_dct)\n\t#     unique_cover_basis_list = list(unique_cover_basis_set)\n\t#     unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t#     return unique_cover_basis_list_without_na\n\t\n\t# except Exception as e:\n\t#     print(\"in ex\", e)\n\t#     # print(type(list_of_dct))\n\t#     unique_first_col = list(set(row[0] for row in list_of_dct))\n\t#     if 'Cover Vehicle' in unique_first_col:\n\t#         unique_first_col.remove('Cover Vehicle')\n\t\n\t#     return unique_first_col\n\t    \n\t\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tVehicle_Schedule_Data = json.loads(Vehicle_Schedule_Data)\n\t\n\tif Vehicle_Schedule_Data in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tVehicle_Schedule_Data = convert_to_dataframe(Vehicle_Schedule_Data)\n\tunique_vals = Vehicle_Schedule_Data['Cover - Vehicle'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
            "docstring": null,
            "function_code": "\t# # Import Python packages\n\timport json\n\timport pandas as pd\n\t\n\t# list_of_dct = json.loads(Vehicle_Schedule_Data)\n\t# # print(list_of_dct)\n\t\n\t# try:\n\t#     unique_cover_basis_set = set(item['Cover Vehicle'] for item in list_of_dct)\n\t#     unique_cover_basis_list = list(unique_cover_basis_set)\n\t#     unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t#     return unique_cover_basis_list_without_na\n\t\n\t# except Exception as e:\n\t#     print(\"in ex\", e)\n\t#     # print(type(list_of_dct))\n\t#     unique_first_col = list(set(row[0] for row in list_of_dct))\n\t#     if 'Cover Vehicle' in unique_first_col:\n\t#         unique_first_col.remove('Cover Vehicle')\n\t\n\t#     return unique_first_col\n\t    \n\t\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tVehicle_Schedule_Data = json.loads(Vehicle_Schedule_Data)\n\t\n\tif Vehicle_Schedule_Data in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tVehicle_Schedule_Data = convert_to_dataframe(Vehicle_Schedule_Data)\n\tunique_vals = Vehicle_Schedule_Data['Cover - Vehicle'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
            "id": 17088,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/97e4aeb5-0b8f-4e11-9d41-09b0119370eb_output.json",
            "name": "get_unique_values_from_cover_basis_col",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/97e4aeb5-0b8f-4e11-9d41-09b0119370eb.json",
            "type": "REFINER"
        },
        "17089": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Cover Basis Mapping@0"
                }
            ],
            "code": "\n\ndef refine_result(previous_line, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\ttry:\n\t    if previous_line not in [\"\", None, \"[]\", \" \", \"N/A\", \"null\"]:\n\t        result = previous_line\n\t    else:\n\t        result = \"{}\"\n\t\n\texcept Exception as e:\n\t    print(e)\n\t    result = \"{}\"\n\t\n\treturn result  # Ensure this is inside a function\n\t",
            "docstring": null,
            "function_code": "\timport json\n\t\n\ttry:\n\t    if previous_line not in [\"\", None, \"[]\", \" \", \"N/A\", \"null\"]:\n\t        result = previous_line\n\t    else:\n\t        result = \"{}\"\n\t\n\texcept Exception as e:\n\t    print(e)\n\t    result = \"{}\"\n\t\n\treturn result  # Ensure this is inside a function\n\t",
            "id": 17089,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/702bc713-4cbd-4cc3-b28c-933d4cef0472_output.json",
            "name": "refine_result",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/702bc713-4cbd-4cc3-b28c-933d4cef0472.json",
            "type": "REFINER"
        },
        "17090": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Cover_Basis_Mapping",
                    "value": "Cover Basis Mapping"
                },
                {
                    "data_type": "FIELD",
                    "name": "Unique__Values__Cover__Basis",
                    "value": "Unique_Values_Cover_Basis"
                },
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Schedule_Data",
                    "value": "Vehicle Schedule Data"
                },
                {
                    "data_type": "FIELD",
                    "name": "Effective_From",
                    "value": "Effective From"
                },
                {
                    "data_type": "FIELD",
                    "name": "Effective_To",
                    "value": "Effective To"
                }
            ],
            "code": "\n\ndef form_vehicle_schedule_table(Cover_Basis_Mapping, Unique__Values__Cover__Basis, Vehicle_Schedule_Data, Effective_From, Effective_To, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tcolumns_order = ['Effective From', \\\n\t                  'Effective To', \\\n\t                'Vehicle Registration', \\\n\t                'Cover - Vehicle', \\\n\t                'Cover - Vehicle - Mapped']\n\t\n\t\n\tif Vehicle_Schedule_Data == \"[]\":\n\t    return [columns_order]\n\ttry:\n\t    \n\t    vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t    vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t    \n\t    # if vehicle_num_and_cover_type_table_df.empty and ((Effective_From not in ['','N/A']) or (Effective_To not in ['','N/A'])):\n\t    #   vehicle_num_and_cover_type_table_df.loc[0] = {'Effective From': Effective_From, 'Effective To': Effective_To}\n\t\n\t\n\t    vehicle_num_and_cover_type_table_df['Effective From'] = str(Effective_From)\n\t    vehicle_num_and_cover_type_table_df['Effective To'] = str(Effective_To)\n\t    \n\t    # print(\"hi\", vehicle_num_and_cover_type_table_df)\n\t\n\t    vehicle_num_and_cover_type_table_df.rename(columns={\n\t        'Cover - Vehicle' : 'Cover - Vehicle',\n\t        'Vehicle Registration Number' : 'Vehicle Registration' \n\t      }, inplace=True)\n\t    \n\t    try:\n\t        Unique__Values__Cover__Basis = json.loads(Unique__Values__Cover__Basis)\n\t    except:\n\t        Unique__Values__Cover__Basis = eval(Unique__Values__Cover__Basis)\n\t\n\t    if Unique__Values__Cover__Basis:\n\t        Cover_Basis_Mapping_dct = json.loads(Cover_Basis_Mapping)\n\t        vehicle_num_and_cover_type_table_df['Cover - Vehicle - Mapped'] = vehicle_num_and_cover_type_table_df['Cover - Vehicle'].replace(Cover_Basis_Mapping_dct)\n\t\n\t    # print(vehicle_num_and_cover_type_table_df, Effective_From, Effective_To)\n\t    \n\t    for col in columns_order:\n\t          if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t\n\t    invalid_values = [\"\", \"n/a\", \"null\", \"none\", \"nan\"]\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t\n\t\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.replace('N/A', '')\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.replace('\"\"', '')\n\t    vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t\n\t    if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t        vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t        return vehicle_num_and_cover_type_table_df\n\t    else:\n\t        return [columns_order]\n\t\n\texcept Exception as e:\n\t    print(\"In exception \", e)\n\t    try:\n\t        vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t        vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t        \n\t        # if vehicle_num_and_cover_type_table_df.empty and ((Effective_From not in ['','N/A']) or (Effective_To not in ['','N/A'])):\n\t        #   vehicle_num_and_cover_type_table_df.loc[0] = {'Effective From': Effective_From, 'Effective To': Effective_To}\n\t\n\t\n\t        try:\n\t            vehicle_num_and_cover_type_table_df.rename(columns={\n\t                'Cover - Vehicle' : 'Cover - Vehicle',\n\t                'Vehicle Registration Number' : 'Vehicle Registration' \n\t              }, inplace=True)\n\t\n\t        except:\n\t            print()\n\t\n\t        for col in columns_order:\n\t          if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t        \n\t        if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t            vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t            vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t            vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t            return vehicle_num_and_cover_type_table_df\n\t        else:\n\t            return [columns_order]\n\t    except:\n\t        return [columns_order]\n\t    return [columns_order]",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tcolumns_order = ['Effective From', \\\n\t                  'Effective To', \\\n\t                'Vehicle Registration', \\\n\t                'Cover - Vehicle', \\\n\t                'Cover - Vehicle - Mapped']\n\t\n\t\n\tif Vehicle_Schedule_Data == \"[]\":\n\t    return [columns_order]\n\ttry:\n\t    \n\t    vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t    vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t    \n\t    # if vehicle_num_and_cover_type_table_df.empty and ((Effective_From not in ['','N/A']) or (Effective_To not in ['','N/A'])):\n\t    #   vehicle_num_and_cover_type_table_df.loc[0] = {'Effective From': Effective_From, 'Effective To': Effective_To}\n\t\n\t\n\t    vehicle_num_and_cover_type_table_df['Effective From'] = str(Effective_From)\n\t    vehicle_num_and_cover_type_table_df['Effective To'] = str(Effective_To)\n\t    \n\t    # print(\"hi\", vehicle_num_and_cover_type_table_df)\n\t\n\t    vehicle_num_and_cover_type_table_df.rename(columns={\n\t        'Cover - Vehicle' : 'Cover - Vehicle',\n\t        'Vehicle Registration Number' : 'Vehicle Registration' \n\t      }, inplace=True)\n\t    \n\t    try:\n\t        Unique__Values__Cover__Basis = json.loads(Unique__Values__Cover__Basis)\n\t    except:\n\t        Unique__Values__Cover__Basis = eval(Unique__Values__Cover__Basis)\n\t\n\t    if Unique__Values__Cover__Basis:\n\t        Cover_Basis_Mapping_dct = json.loads(Cover_Basis_Mapping)\n\t        vehicle_num_and_cover_type_table_df['Cover - Vehicle - Mapped'] = vehicle_num_and_cover_type_table_df['Cover - Vehicle'].replace(Cover_Basis_Mapping_dct)\n\t\n\t    # print(vehicle_num_and_cover_type_table_df, Effective_From, Effective_To)\n\t    \n\t    for col in columns_order:\n\t          if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t\n\t    invalid_values = [\"\", \"n/a\", \"null\", \"none\", \"nan\"]\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t\n\t\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.replace('N/A', '')\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.replace('\"\"', '')\n\t    vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t\n\t    if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t        vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t        return vehicle_num_and_cover_type_table_df\n\t    else:\n\t        return [columns_order]\n\t\n\texcept Exception as e:\n\t    print(\"In exception \", e)\n\t    try:\n\t        vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t        vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t        \n\t        # if vehicle_num_and_cover_type_table_df.empty and ((Effective_From not in ['','N/A']) or (Effective_To not in ['','N/A'])):\n\t        #   vehicle_num_and_cover_type_table_df.loc[0] = {'Effective From': Effective_From, 'Effective To': Effective_To}\n\t\n\t\n\t        try:\n\t            vehicle_num_and_cover_type_table_df.rename(columns={\n\t                'Cover - Vehicle' : 'Cover - Vehicle',\n\t                'Vehicle Registration Number' : 'Vehicle Registration' \n\t              }, inplace=True)\n\t\n\t        except:\n\t            print()\n\t\n\t        for col in columns_order:\n\t          if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t        \n\t        if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t            vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t            vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t            vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t            return vehicle_num_and_cover_type_table_df\n\t        else:\n\t            return [columns_order]\n\t    except:\n\t        return [columns_order]\n\t    return [columns_order]",
            "id": 17090,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/4ad109a5-bb41-4951-9179-93f2565ed611_output.json",
            "name": "form_vehicle_schedule_table",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/4ad109a5-bb41-4951-9179-93f2565ed611.json",
            "type": "REFINER"
        },
        "17091": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Count_From_Email",
                    "value": "Vehicle Count From Email"
                },
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Count_From_Table",
                    "value": "Vehicle Count From Table"
                }
            ],
            "code": "\n\ndef get_count_of_notifiable_vehicles(Vehicle_Count_From_Email, Vehicle_Count_From_Table, context = {}, keys = {}, **kwargs):\n\t# try:\n\t#     Vehicle_Count_From_Email = int(Vehicle_Count_From_Email)\n\t#     return Vehicle_Count_From_Email\n\t# except:\n\ttry:\n\t    Vehicle_Count_From_Table = int(Vehicle_Count_From_Table)\n\t    return Vehicle_Count_From_Table\n\texcept:\n\t    return 0",
            "docstring": null,
            "function_code": "\t# try:\n\t#     Vehicle_Count_From_Email = int(Vehicle_Count_From_Email)\n\t#     return Vehicle_Count_From_Email\n\t# except:\n\ttry:\n\t    Vehicle_Count_From_Table = int(Vehicle_Count_From_Table)\n\t    return Vehicle_Count_From_Table\n\texcept:\n\t    return 0",
            "id": 17091,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/49c8ab9f-811c-4a35-9ed4-724653ece661_output.json",
            "name": "get_count_of_notifiable_vehicles",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/49c8ab9f-811c-4a35-9ed4-724653ece661.json",
            "type": "REFINER"
        },
        "17092": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Main Cover Type Mapped@0"
                }
            ],
            "code": "\n\ndef check_cover_type_mapped(previous_line, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
            "id": 17092,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/b356ad3c-7cd6-40e2-a27a-a4330cfa29e6_output.json",
            "name": "check_cover_type_mapped",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/b356ad3c-7cd6-40e2-a27a-a4330cfa29e6.json",
            "type": "REFINER"
        },
        "17093": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Line_1",
                    "value": "Party Address"
                },
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line(Party_Address_Line_1, Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t# # Import Python packages\n\t# # import json\n\t\n\t# # Log statements using print()\n\t# # print(\"This will appear in the logs\")\n\t\n\t# # Return the cleaned output\n\t# # print(previous_line)\n\t# # return previous_line\n\t# import json\n\t# key = \"Line_2\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start,len(text)):\n\t#     if text[i] == '{':\n\t#       brace_count += 1\n\t#     elif text[i] == '}':\n\t#       brace_count  -= 1\n\t#       if brace_count == 0:\n\t#         end = i\n\t#         break\n\t#   if end is None:\n\t#     return None\n\t#   return text[start:end+1]\n\t\n\t# if Party_Address_Line_1:\n\t#   json_part = extract_first_json_object(Party_Address_Line_1)\n\t#   if json_part is None:\n\t#     return None\n\t#   try:\n\t#     data = json.loads(json_part)\n\t#   except json.JSONDecodeError:\n\t#     return \"Extracted JSON is invalid.\"\n\t#   if key == \"Line_3\":\n\t#     parts = []\n\t#     line3 = data.get(\"Line_3\")\n\t#     if line3:\n\t#       parts.append(line3)\n\t#     extra = data.get(\"Extra_Lines\",[])\n\t#     if extra and isinstance(extra,list):\n\t#       parts.extend(extra)\n\t#     return ', '.join(parts) if parts else None\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# else:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
            "docstring": null,
            "function_code": "\t# # Import Python packages\n\t# # import json\n\t\n\t# # Log statements using print()\n\t# # print(\"This will appear in the logs\")\n\t\n\t# # Return the cleaned output\n\t# # print(previous_line)\n\t# # return previous_line\n\t# import json\n\t# key = \"Line_2\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start,len(text)):\n\t#     if text[i] == '{':\n\t#       brace_count += 1\n\t#     elif text[i] == '}':\n\t#       brace_count  -= 1\n\t#       if brace_count == 0:\n\t#         end = i\n\t#         break\n\t#   if end is None:\n\t#     return None\n\t#   return text[start:end+1]\n\t\n\t# if Party_Address_Line_1:\n\t#   json_part = extract_first_json_object(Party_Address_Line_1)\n\t#   if json_part is None:\n\t#     return None\n\t#   try:\n\t#     data = json.loads(json_part)\n\t#   except json.JSONDecodeError:\n\t#     return \"Extracted JSON is invalid.\"\n\t#   if key == \"Line_3\":\n\t#     parts = []\n\t#     line3 = data.get(\"Line_3\")\n\t#     if line3:\n\t#       parts.append(line3)\n\t#     extra = data.get(\"Extra_Lines\",[])\n\t#     if extra and isinstance(extra,list):\n\t#       parts.extend(extra)\n\t#     return ', '.join(parts) if parts else None\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# else:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
            "id": 17093,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a0611902-5475-43e4-b576-ae079e9c8fa8_output.json",
            "name": "get_address_line",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a0611902-5475-43e4-b576-ae079e9c8fa8.json",
            "type": "REFINER"
        },
        "17094": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line3(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 3\")\n\treturn val",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 3\")\n\treturn val",
            "id": 17094,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2872a05c-01c4-4d85-9555-7a41bbb7ed23_output.json",
            "name": "get_address_line3",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2872a05c-01c4-4d85-9555-7a41bbb7ed23.json",
            "type": "REFINER"
        },
        "17095": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_country(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Country\")\n\treturn val",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Country\")\n\treturn val",
            "id": 17095,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/da91fe7c-d078-4424-ad28-5b8c526be85e_output.json",
            "name": "get_country",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/da91fe7c-d078-4424-ad28-5b8c526be85e.json",
            "type": "REFINER"
        },
        "17096": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_state(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"State\")\n\treturn val",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"State\")\n\treturn val",
            "id": 17096,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/e6ae390d-af62-4ed3-8308-3a33890d19f1_output.json",
            "name": "get_state",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/e6ae390d-af62-4ed3-8308-3a33890d19f1.json",
            "type": "REFINER"
        },
        "17097": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_city(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"City\")\n\treturn val",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"City\")\n\treturn val",
            "id": 17097,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a470a09a-6c01-454b-a674-64c42281b931_output.json",
            "name": "get_city",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a470a09a-6c01-454b-a674-64c42281b931.json",
            "type": "REFINER"
        },
        "17098": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_postcode(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t# def get_value(data, key):\n\t#     if isinstance(data, dict):\n\t#         return data.get(key, \"\")\n\t#     return \"\"\n\t\n\t# val = get_value(Party_Address_Details, \"Postcode\")\n\t# return val\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Postcode\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\t# def get_value(data, key):\n\t#     if isinstance(data, dict):\n\t#         return data.get(key, \"\")\n\t#     return \"\"\n\t\n\t# val = get_value(Party_Address_Details, \"Postcode\")\n\t# return val\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Postcode\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "id": 17098,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/35f6720a-8023-4f78-ae6e-0a38da9eae2b_output.json",
            "name": "get_postcode",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/35f6720a-8023-4f78-ae6e-0a38da9eae2b.json",
            "type": "REFINER"
        },
        "17099": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_party_address_line1(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
            "id": 17099,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/04f3cfb7-6db1-4d5c-8c70-fc286ba57fe6_output.json",
            "name": "get_party_address_line1",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/04f3cfb7-6db1-4d5c-8c70-fc286ba57fe6.json",
            "type": "REFINER"
        },
        "17100": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Party Address Details@0"
                }
            ],
            "code": "\n\ndef clean(previous_line, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\ttry:\n\t  previous_line = eval(previous_line)\n\t  # print(type(previous_line))\n\texcept:\n\t  previous_line = json.loads(previous_line)\n\t  # print(type(previous_line))\n\t\n\treturn previous_line",
            "docstring": null,
            "function_code": "\timport json\n\t\n\ttry:\n\t  previous_line = eval(previous_line)\n\t  # print(type(previous_line))\n\texcept:\n\t  previous_line = json.loads(previous_line)\n\t  # print(type(previous_line))\n\t\n\treturn previous_line",
            "id": 17100,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/6e7fd210-446b-4afe-bc56-e02cd85a4e8d_output.json",
            "name": "clean",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/6e7fd210-446b-4afe-bc56-e02cd85a4e8d.json",
            "type": "REFINER"
        },
        "17101": {
            "args": [],
            "code": "\n\ndef unnamed_custom_function(context = {}, keys = {}, **kwargs):\n\tdata = [\n\t  {\n\t    \"trade\": \"General Manufacturing\",\n\t    \"aliases\": [\"steel\", \"metal\", \"samples\", \"manufacturing of glass\", \"manufacturing of furniture\", \"manufacturing\", \"general manufacturing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Fleet - Unclassified\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Contractor\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Builders\",\n\t    \"aliases\": [\"Building Contractors\", \"house builders\", \"builders\"]\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers UK only\",\n\t    \"aliases\": [\"haulage contractors - uk only\", \"hauliers uk only\"]\n\t  },\n\t  {\n\t    \"trade\": \"Electrician\",\n\t    \"aliases\": [\"Electrical Contractors\", \"electrical installation\", \"electrical testing\", \"electrician\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plumbing & Heating Engineer\",\n\t    \"aliases\": [\"plumbing & heating contractors\", \"gas servicing\", \"boiler maintenance\", \"plumber\", \"heating engineer\"]\n\t  },\n\t  {\n\t    \"trade\": \"Engineering\",\n\t    \"aliases\": [ \"engineers\", \"mechanical engineers\", \"engineering\", \"electrical engineers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Civil engineering\",\n\t    \"aliases\": [\"civil engineering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Scaffolding Contractors\",\n\t    \"aliases\": [\"scaffolders\", \"scaffolding contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Wholesale (non food and drink)\",\n\t    \"aliases\": [\"wholesalers of electrical components\", \"building materials\", \"wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Cleaning Contractors\",\n\t    \"aliases\": [\"domestic cleaners\", \"office cleaners\", \"cleaning contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plant Hire\",\n\t    \"aliases\": [\"plant hire operator\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Wholesale\",\n\t    \"aliases\": [\"cash & carry\", \"beer & wine wholesalers\", \"food wholesalers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Retail (non food and drink)\",\n\t    \"aliases\": [\"department store\", \"shopping centre\"]\n\t  },\n\t  {\n\t    \"trade\": \"Property Owners\",\n\t    \"aliases\": [\"landlords\", \"commercial property owners\"]\n\t  },\n\t  {\n\t    \"trade\": \"Telecommunications & IT\",\n\t    \"aliases\": [\"telecommunications installation\", \"it installation\", \"it services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Double Glazing\",\n\t    \"aliases\": [\"double glazing manufacture\", \"double glazing installation\"]\n\t  },\n\t  {\n\t    \"trade\": \"Landscape Gardener\",\n\t    \"aliases\": [\"gardening\", \"landscape gardener\"]\n\t  },\n\t  {\n\t    \"trade\": \"Other Prof/Sci/Tech\",\n\t    \"aliases\": [\"laboratory\"]\n\t  },\n\t  {\n\t    \"trade\": \"Removal Contractor\",\n\t    \"aliases\": [\"removals and storage\"]\n\t  },\n\t  {\n\t    \"trade\": \"Security and investigation\",\n\t    \"aliases\": [\"private detectives\", \"security guarding\", \"security services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Builders Merchant\",\n\t    \"aliases\": [\"building supplies\", \"suppliers of aggregates\"]\n\t  },\n\t  {\n\t    \"trade\": \"Service Engineers\",\n\t    \"aliases\": [\"mechanical servicing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Asphalters/Pavers/Engineers\",\n\t    \"aliases\": [\"road maintenance\", \"road surfacing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Flooring and Carpet\",\n\t    \"aliases\": [\"carpet fitters\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure Industry\",\n\t    \"aliases\": [\"nightclub\", \"pub\", \"leisure centre\", \"gym\"]\n\t  },\n\t  {\n\t    \"trade\": \"Social Religious or Charitable\",\n\t    \"aliases\": [\"charity\"]\n\t  },\n\t  {\n\t    \"trade\": \"Business Services\",\n\t    \"aliases\": [\"document storage\", \"administration\", \"consultants\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Services\",\n\t    \"aliases\": [\"food delivery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Retail\",\n\t    \"aliases\": [\"restaurant\", \"pub\", \"takeaway\"]\n\t  },\n\t  {\n\t    \"trade\": \"Estate Agent\",\n\t    \"aliases\": [\"lettings agents\", \"estate agent\"]\n\t  },\n\t  {\n\t    \"trade\": \"Furniture Sale and Manufacture\",\n\t    \"aliases\": [\"furniture retail\", \"furniture showroom\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure (Hotel, clubs & pubs)\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Shop Fitting\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"NHS Trust\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Import/Export\",\n\t    \"aliases\": [\"Import\", \"Export\"]\n\t  },\n\t  {\n\t    \"trade\": \"Catering\",\n\t    \"aliases\": [\"licensed catering\", \"unlicensed catering\", \"outside catering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Residential Care\",\n\t    \"aliases\": [\"care homes\", \"retirement homes\"]\n\t  },\n\t  {\n\t    \"trade\": \"Manufacturing Timber/Furniture\",\n\t    \"aliases\": [\"joinery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Printers and publishers\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Demolition Contractors\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Housing Association\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Timber Merchant\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Private Ambulance Service\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers Overseas\",\n\t    \"aliases\": [\"haulage contractors - overseas\", \"hauliers overseas\"]\n\t  },\n\t  {\n\t    \"trade\": \"Textiles & Clothing\",\n\t    \"aliases\": [\"clothing manufacturing\", \"textile manufacturing\", \"clothing retail\"]\n\t  },\n\t  {\n\t    \"trade\": \"Automotive Industry\",\n\t    \"aliases\": [\"vehicle manufacturing\", \"vehicle repairs\", \"vehicle parts manufacturing\", \"vehicle parts wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Pharmaceutical\",\n\t    \"aliases\": [\"pharmacy\", \"medical laboratories\"]\n\t  },\n\t  {\n\t    \"trade\": \"Farmer\",\n\t    \"aliases\": [\"dairy farmers\", \"arable farmers\", \"livestock farmers\"]\n\t  }\n\t]\n\t\n\treturn data\n\t\n\t",
            "docstring": null,
            "function_code": "\tdata = [\n\t  {\n\t    \"trade\": \"General Manufacturing\",\n\t    \"aliases\": [\"steel\", \"metal\", \"samples\", \"manufacturing of glass\", \"manufacturing of furniture\", \"manufacturing\", \"general manufacturing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Fleet - Unclassified\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Contractor\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Builders\",\n\t    \"aliases\": [\"Building Contractors\", \"house builders\", \"builders\"]\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers UK only\",\n\t    \"aliases\": [\"haulage contractors - uk only\", \"hauliers uk only\"]\n\t  },\n\t  {\n\t    \"trade\": \"Electrician\",\n\t    \"aliases\": [\"Electrical Contractors\", \"electrical installation\", \"electrical testing\", \"electrician\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plumbing & Heating Engineer\",\n\t    \"aliases\": [\"plumbing & heating contractors\", \"gas servicing\", \"boiler maintenance\", \"plumber\", \"heating engineer\"]\n\t  },\n\t  {\n\t    \"trade\": \"Engineering\",\n\t    \"aliases\": [ \"engineers\", \"mechanical engineers\", \"engineering\", \"electrical engineers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Civil engineering\",\n\t    \"aliases\": [\"civil engineering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Scaffolding Contractors\",\n\t    \"aliases\": [\"scaffolders\", \"scaffolding contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Wholesale (non food and drink)\",\n\t    \"aliases\": [\"wholesalers of electrical components\", \"building materials\", \"wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Cleaning Contractors\",\n\t    \"aliases\": [\"domestic cleaners\", \"office cleaners\", \"cleaning contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plant Hire\",\n\t    \"aliases\": [\"plant hire operator\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Wholesale\",\n\t    \"aliases\": [\"cash & carry\", \"beer & wine wholesalers\", \"food wholesalers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Retail (non food and drink)\",\n\t    \"aliases\": [\"department store\", \"shopping centre\"]\n\t  },\n\t  {\n\t    \"trade\": \"Property Owners\",\n\t    \"aliases\": [\"landlords\", \"commercial property owners\"]\n\t  },\n\t  {\n\t    \"trade\": \"Telecommunications & IT\",\n\t    \"aliases\": [\"telecommunications installation\", \"it installation\", \"it services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Double Glazing\",\n\t    \"aliases\": [\"double glazing manufacture\", \"double glazing installation\"]\n\t  },\n\t  {\n\t    \"trade\": \"Landscape Gardener\",\n\t    \"aliases\": [\"gardening\", \"landscape gardener\"]\n\t  },\n\t  {\n\t    \"trade\": \"Other Prof/Sci/Tech\",\n\t    \"aliases\": [\"laboratory\"]\n\t  },\n\t  {\n\t    \"trade\": \"Removal Contractor\",\n\t    \"aliases\": [\"removals and storage\"]\n\t  },\n\t  {\n\t    \"trade\": \"Security and investigation\",\n\t    \"aliases\": [\"private detectives\", \"security guarding\", \"security services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Builders Merchant\",\n\t    \"aliases\": [\"building supplies\", \"suppliers of aggregates\"]\n\t  },\n\t  {\n\t    \"trade\": \"Service Engineers\",\n\t    \"aliases\": [\"mechanical servicing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Asphalters/Pavers/Engineers\",\n\t    \"aliases\": [\"road maintenance\", \"road surfacing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Flooring and Carpet\",\n\t    \"aliases\": [\"carpet fitters\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure Industry\",\n\t    \"aliases\": [\"nightclub\", \"pub\", \"leisure centre\", \"gym\"]\n\t  },\n\t  {\n\t    \"trade\": \"Social Religious or Charitable\",\n\t    \"aliases\": [\"charity\"]\n\t  },\n\t  {\n\t    \"trade\": \"Business Services\",\n\t    \"aliases\": [\"document storage\", \"administration\", \"consultants\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Services\",\n\t    \"aliases\": [\"food delivery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Retail\",\n\t    \"aliases\": [\"restaurant\", \"pub\", \"takeaway\"]\n\t  },\n\t  {\n\t    \"trade\": \"Estate Agent\",\n\t    \"aliases\": [\"lettings agents\", \"estate agent\"]\n\t  },\n\t  {\n\t    \"trade\": \"Furniture Sale and Manufacture\",\n\t    \"aliases\": [\"furniture retail\", \"furniture showroom\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure (Hotel, clubs & pubs)\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Shop Fitting\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"NHS Trust\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Import/Export\",\n\t    \"aliases\": [\"Import\", \"Export\"]\n\t  },\n\t  {\n\t    \"trade\": \"Catering\",\n\t    \"aliases\": [\"licensed catering\", \"unlicensed catering\", \"outside catering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Residential Care\",\n\t    \"aliases\": [\"care homes\", \"retirement homes\"]\n\t  },\n\t  {\n\t    \"trade\": \"Manufacturing Timber/Furniture\",\n\t    \"aliases\": [\"joinery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Printers and publishers\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Demolition Contractors\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Housing Association\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Timber Merchant\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Private Ambulance Service\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers Overseas\",\n\t    \"aliases\": [\"haulage contractors - overseas\", \"hauliers overseas\"]\n\t  },\n\t  {\n\t    \"trade\": \"Textiles & Clothing\",\n\t    \"aliases\": [\"clothing manufacturing\", \"textile manufacturing\", \"clothing retail\"]\n\t  },\n\t  {\n\t    \"trade\": \"Automotive Industry\",\n\t    \"aliases\": [\"vehicle manufacturing\", \"vehicle repairs\", \"vehicle parts manufacturing\", \"vehicle parts wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Pharmaceutical\",\n\t    \"aliases\": [\"pharmacy\", \"medical laboratories\"]\n\t  },\n\t  {\n\t    \"trade\": \"Farmer\",\n\t    \"aliases\": [\"dairy farmers\", \"arable farmers\", \"livestock farmers\"]\n\t  }\n\t]\n\t\n\treturn data\n\t\n\t",
            "id": 17101,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/39ea008a-bcba-4c28-b258-84860337a9cd_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/39ea008a-bcba-4c28-b258-84860337a9cd.json",
            "type": "REFINER"
        },
        "17102": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "AXA Trade Description@0"
                },
                {
                    "data_type": "FIELD",
                    "name": "Business_Description",
                    "value": "Business Description"
                }
            ],
            "code": "\n\ndef clean(previous_line, Business_Description, context = {}, keys = {}, **kwargs):\n\tif Business_Description:\n\t  if Business_Description == \"\":\n\t      return \"\"\n\t  else:\n\t      return previous_line\n\telse:\n\t    return \"\"",
            "docstring": null,
            "function_code": "\tif Business_Description:\n\t  if Business_Description == \"\":\n\t      return \"\"\n\t  else:\n\t      return previous_line\n\telse:\n\t    return \"\"",
            "id": 17102,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/dc8966b8-e421-4058-8580-f5182c14ab3e_output.json",
            "name": "clean",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/dc8966b8-e421-4058-8580-f5182c14ab3e.json",
            "type": "REFINER"
        },
        "17103": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "AXA Trade Description@1"
                },
                {
                    "data_type": "FIELD",
                    "name": "Trade_Descriptions",
                    "value": "Trade Descriptions"
                }
            ],
            "code": "\n\ndef test_wih_exisitng_list(previous_line, Trade_Descriptions, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\timport json\n\t\n\t\n\t\n\t# try:\n\t#   Trade_Descriptions = eval(Trade_Descriptions)\n\t# except:\n\t#   try:\n\t#       Trade_Descriptions = json.loads(Trade_Descriptions)\n\t#   except:\n\t#       # return \"Fleet - Unclassified\"\n\t#       return \"Other\"\n\t\n\t# # Trade_Descriptions_ls = [item.lower() for item in Trade_Descriptions]\n\t\n\t# Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t\n\t# if previous_line.lower() in Trade_Descriptions_ls:\n\t#   return previous_line\n\t# else:\n\t#   return \"Other\"\n\t\n\tprint(repr(previous_line))\n\tif previous_line == '\"\"' or previous_line == '' or previous_line == \"\":\n\t    return \"\"\n\t\n\tif previous_line:\n\t  try:\n\t      Trade_Descriptions = eval(Trade_Descriptions)\n\t  except:\n\t      try:\n\t          Trade_Descriptions = json.loads(Trade_Descriptions)\n\t      except:\n\t          # return \"Fleet - Unclassified\"\n\t          return \"Fleet - Unclassified\"\n\t\n\t  Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t    \n\t  if previous_line.lower() in Trade_Descriptions_ls:\n\t    return previous_line\n\t  else:\n\t    return \"Fleet - Unclassified\"\n\telse:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\timport json\n\t\n\t\n\t\n\t# try:\n\t#   Trade_Descriptions = eval(Trade_Descriptions)\n\t# except:\n\t#   try:\n\t#       Trade_Descriptions = json.loads(Trade_Descriptions)\n\t#   except:\n\t#       # return \"Fleet - Unclassified\"\n\t#       return \"Other\"\n\t\n\t# # Trade_Descriptions_ls = [item.lower() for item in Trade_Descriptions]\n\t\n\t# Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t\n\t# if previous_line.lower() in Trade_Descriptions_ls:\n\t#   return previous_line\n\t# else:\n\t#   return \"Other\"\n\t\n\tprint(repr(previous_line))\n\tif previous_line == '\"\"' or previous_line == '' or previous_line == \"\":\n\t    return \"\"\n\t\n\tif previous_line:\n\t  try:\n\t      Trade_Descriptions = eval(Trade_Descriptions)\n\t  except:\n\t      try:\n\t          Trade_Descriptions = json.loads(Trade_Descriptions)\n\t      except:\n\t          # return \"Fleet - Unclassified\"\n\t          return \"Fleet - Unclassified\"\n\t\n\t  Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t    \n\t  if previous_line.lower() in Trade_Descriptions_ls:\n\t    return previous_line\n\t  else:\n\t    return \"Fleet - Unclassified\"\n\telse:\n\t  return \"\"",
            "id": 17103,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/88663633-36e9-4162-9a4d-3ab3da40d272_output.json",
            "name": "test_wih_exisitng_list",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/88663633-36e9-4162-9a4d-3ab3da40d272.json",
            "type": "REFINER"
        },
        "17104": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Target Price@1"
                }
            ],
            "code": "\n\ndef unnamed_custom_function(previous_line, context = {}, keys = {}, **kwargs):\n\treturn previous_line.replace(\"\u00a3\",\"\").replace(\",\",\"\")",
            "docstring": null,
            "function_code": "\treturn previous_line.replace(\"\u00a3\",\"\").replace(\",\",\"\")",
            "id": 17104,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/ba0622b7-a03a-41ac-9bdc-22a5d14f390b_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/ba0622b7-a03a-41ac-9bdc-22a5d14f390b.json",
            "type": "REFINER"
        },
        "17105": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Broker Deadline@1"
                }
            ],
            "code": "\n\ndef business_logic_for_broker_deadline(previous_line, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime, timedelta\n\t\n\tdef add_calendar_days(start_date, days):\n\t    \"\"\"Add calendar days (including weekends) to a given date.\"\"\"\n\t    return start_date + timedelta(days=days)\n\t\n\tdef add_weekdays(start_date, days):\n\t    \"\"\"Add weekdays to a given date, skipping weekends.\"\"\"\n\t    while days > 0:\n\t        start_date += timedelta(days=1)\n\t        if start_date.weekday() < 5:  # Weekdays: Mon-Fri = 0-4\n\t            days -= 1\n\t    return start_date\n\t\n\tdef process_date(input_str):\n\t    today = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)\n\t\n\t    # Try to parse dd/mm/yyyy\n\t    try:\n\t        parsed_date = datetime.strptime(input_str, \"%d/%m/%Y\")\n\t    except ValueError:\n\t        # If year not present, try dd/mm check\n\t        try:\n\t            try:\n\t                parsed_date = datetime.strptime(input_str, \"%d/%m\")\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t            except:\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t        except ValueError:\n\t            final_date = add_calendar_days(today, 5)\n\t            return final_date.strftime(\"%d/%m/%Y\")\n\t            \n\t    # Compare dates\n\t    base_date = parsed_date if parsed_date >= today else today\n\t\n\t    return base_date.strftime(\"%d/%m/%Y\")\n\t\n\tresult = process_date(previous_line) \n\treturn result",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime, timedelta\n\t\n\tdef add_calendar_days(start_date, days):\n\t    \"\"\"Add calendar days (including weekends) to a given date.\"\"\"\n\t    return start_date + timedelta(days=days)\n\t\n\tdef add_weekdays(start_date, days):\n\t    \"\"\"Add weekdays to a given date, skipping weekends.\"\"\"\n\t    while days > 0:\n\t        start_date += timedelta(days=1)\n\t        if start_date.weekday() < 5:  # Weekdays: Mon-Fri = 0-4\n\t            days -= 1\n\t    return start_date\n\t\n\tdef process_date(input_str):\n\t    today = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)\n\t\n\t    # Try to parse dd/mm/yyyy\n\t    try:\n\t        parsed_date = datetime.strptime(input_str, \"%d/%m/%Y\")\n\t    except ValueError:\n\t        # If year not present, try dd/mm check\n\t        try:\n\t            try:\n\t                parsed_date = datetime.strptime(input_str, \"%d/%m\")\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t            except:\n\t                final_date = add_calendar_days(today, 5)\n\t                return final_date.strftime(\"%d/%m/%Y\")\n\t        except ValueError:\n\t            final_date = add_calendar_days(today, 5)\n\t            return final_date.strftime(\"%d/%m/%Y\")\n\t            \n\t    # Compare dates\n\t    base_date = parsed_date if parsed_date >= today else today\n\t\n\t    return base_date.strftime(\"%d/%m/%Y\")\n\t\n\tresult = process_date(previous_line) \n\treturn result",
            "id": 17105,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/790e1de7-663a-4492-bf8d-2b4ca898cc2a_output.json",
            "name": "business_logic_for_broker_deadline",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/790e1de7-663a-4492-bf8d-2b4ca898cc2a.json",
            "type": "REFINER"
        },
        "17106": {
            "args": [],
            "code": "\n\ndef get_product_name(context = {}, keys = {}, **kwargs):\n\treturn \"Fleet\"",
            "docstring": null,
            "function_code": "\treturn \"Fleet\"",
            "id": 17106,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2c24c6b8-abf4-4493-910b-666160d2faa7_output.json",
            "name": "get_product_name",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2c24c6b8-abf4-4493-910b-666160d2faa7.json",
            "type": "REFINER"
        },
        "17107": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Incepts_On",
                    "value": "Incepts On"
                }
            ],
            "code": "\n\ndef unnamed_custom_function(Incepts_On, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime, timedelta\n\t\n\tdef add_one_year_minus_one_day(date_str):\n\t    # Parse the input date (DD/MM/YYYY)\n\t    input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t    try:\n\t        # Try to add one year directly\n\t        one_year_later = input_date.replace(year=input_date.year + 1)\n\t    except ValueError:\n\t        # Handle Feb 29 (leap year issue) and other invalid dates\n\t        temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t        one_year_later = temp_date\n\t    # Subtract one day\n\t    final_date = one_year_later - timedelta(days=1)\n\t    # Return formatted date as DD/MM/YYYY with leading zeros\n\t    return final_date.strftime(\"%d/%m/%Y\")\n\t    \n\ttry:\n\t    expiry_date = add_one_year_minus_one_day(Incepts_On)\n\t    return expiry_date\n\texcept Exception as e:\n\t    return \"\"\n\t\n\t",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime, timedelta\n\t\n\tdef add_one_year_minus_one_day(date_str):\n\t    # Parse the input date (DD/MM/YYYY)\n\t    input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t    try:\n\t        # Try to add one year directly\n\t        one_year_later = input_date.replace(year=input_date.year + 1)\n\t    except ValueError:\n\t        # Handle Feb 29 (leap year issue) and other invalid dates\n\t        temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t        one_year_later = temp_date\n\t    # Subtract one day\n\t    final_date = one_year_later - timedelta(days=1)\n\t    # Return formatted date as DD/MM/YYYY with leading zeros\n\t    return final_date.strftime(\"%d/%m/%Y\")\n\t    \n\ttry:\n\t    expiry_date = add_one_year_minus_one_day(Incepts_On)\n\t    return expiry_date\n\texcept Exception as e:\n\t    return \"\"\n\t\n\t",
            "id": 17107,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/e4029531-0ec8-43e0-994d-7649b8017e17_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/e4029531-0ec8-43e0-994d-7649b8017e17.json",
            "type": "REFINER"
        },
        "17108": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Main Cover Type@1"
                }
            ],
            "code": "\n\ndef check_cover_type(previous_line, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
            "id": 17108,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/e7f85908-f04b-48fc-b910-71b0ade23999_output.json",
            "name": "check_cover_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/e7f85908-f04b-48fc-b910-71b0ade23999.json",
            "type": "REFINER"
        },
        "17109": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Incepts_On",
                    "value": "Incepts On"
                }
            ],
            "code": "\n\ndef get_effective_date(Incepts_On, context = {}, keys = {}, **kwargs):\n\tif Incepts_On:\n\t  return Incepts_On\n\telse:\n\t  return ''",
            "docstring": null,
            "function_code": "\tif Incepts_On:\n\t  return Incepts_On\n\telse:\n\t  return ''",
            "id": 17109,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/0182c889-86c6-494a-9d8f-018bf62925b3_output.json",
            "name": "get_effective_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/0182c889-86c6-494a-9d8f-018bf62925b3.json",
            "type": "REFINER"
        },
        "17110": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Expires_On",
                    "value": "Expires On"
                }
            ],
            "code": "\n\ndef get_expiry_date(Expires_On, context = {}, keys = {}, **kwargs):\n\tif Expires_On:\n\t  return Expires_On\n\telse:\n\t  return ''",
            "docstring": null,
            "function_code": "\tif Expires_On:\n\t  return Expires_On\n\telse:\n\t  return ''",
            "id": 17110,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/1a7ac13a-713e-4590-affb-306b5b7c217d_output.json",
            "name": "get_expiry_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/1a7ac13a-713e-4590-affb-306b5b7c217d.json",
            "type": "REFINER"
        },
        "17111": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Holding Broker@0"
                }
            ],
            "code": "\n\ndef return_holding_broker(previous_line, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef extract_holding_broker_from_text(text: str) -> str:\n\t    brace_stack = []\n\t    start_idx = -1\n\t    end_idx = -1\n\t\n\t    # Step 1: Find the first complete {...} block\n\t    for i, ch in enumerate(text):\n\t        if ch == '{':\n\t            if not brace_stack:\n\t                start_idx = i\n\t            brace_stack.append('{')\n\t        elif ch == '}':\n\t            if brace_stack:\n\t                brace_stack.pop()\n\t                if not brace_stack:\n\t                    end_idx = i + 1  # Include the closing brace\n\t                    break\n\t\n\t    # Step 2: If we found a full JSON block\n\t    if start_idx != -1 and end_idx != -1:\n\t        json_str = text[start_idx:end_idx]\n\t\n\t        try:\n\t            data = json.loads(json_str)\n\t            holding_broker = data.get('holding_broker')  \n\t            return holding_broker\n\t        except:\n\t            return ''\n\t\n\t    return ''\n\t\n\treturn extract_holding_broker_from_text(previous_line)\n\t",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef extract_holding_broker_from_text(text: str) -> str:\n\t    brace_stack = []\n\t    start_idx = -1\n\t    end_idx = -1\n\t\n\t    # Step 1: Find the first complete {...} block\n\t    for i, ch in enumerate(text):\n\t        if ch == '{':\n\t            if not brace_stack:\n\t                start_idx = i\n\t            brace_stack.append('{')\n\t        elif ch == '}':\n\t            if brace_stack:\n\t                brace_stack.pop()\n\t                if not brace_stack:\n\t                    end_idx = i + 1  # Include the closing brace\n\t                    break\n\t\n\t    # Step 2: If we found a full JSON block\n\t    if start_idx != -1 and end_idx != -1:\n\t        json_str = text[start_idx:end_idx]\n\t\n\t        try:\n\t            data = json.loads(json_str)\n\t            holding_broker = data.get('holding_broker')  \n\t            return holding_broker\n\t        except:\n\t            return ''\n\t\n\t    return ''\n\t\n\treturn extract_holding_broker_from_text(previous_line)\n\t",
            "id": 17111,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/03044fce-dff9-4831-8523-fca98f39bc89_output.json",
            "name": "return_holding_broker",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/03044fce-dff9-4831-8523-fca98f39bc89.json",
            "type": "REFINER"
        },
        "17112": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Is Holding Broker@0"
                },
                {
                    "data_type": "FIELD",
                    "name": "Holding_Broker",
                    "value": "Holding Broker"
                }
            ],
            "code": "\n\ndef check_return_holding_broker_value(previous_line, Holding_Broker, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t\n\tprint(Holding_Broker)\n\tif Holding_Broker =='':\n\t  return ''\n\t# return previous_line\n\t\n\t\n\t\n\timport json\n\t\n\tdef extract_holding_broker_from_text(text: str) -> str:\n\t    brace_stack = []\n\t    start_idx = -1\n\t    end_idx = -1\n\t\n\t    # Step 1: Find the first complete {...} block\n\t    for i, ch in enumerate(text):\n\t        if ch == '{':\n\t            if not brace_stack:\n\t                start_idx = i\n\t            brace_stack.append('{')\n\t        elif ch == '}':\n\t            if brace_stack:\n\t                brace_stack.pop()\n\t                if not brace_stack:\n\t                    end_idx = i + 1  # Include the closing brace\n\t                    break\n\t\n\t    # Step 2: If we found a full JSON block\n\t    if start_idx != -1 and end_idx != -1:\n\t        json_str = text[start_idx:end_idx]\n\t\n\t        try:\n\t            data = json.loads(json_str)\n\t            holding_broker = data.get('holding_broker')  \n\t            print(holding_broker)\n\t            return holding_broker\n\t        except:\n\t            return ''\n\t\n\t    return ''\n\t\n\treturn extract_holding_broker_from_text(previous_line)\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t\n\tprint(Holding_Broker)\n\tif Holding_Broker =='':\n\t  return ''\n\t# return previous_line\n\t\n\t\n\t\n\timport json\n\t\n\tdef extract_holding_broker_from_text(text: str) -> str:\n\t    brace_stack = []\n\t    start_idx = -1\n\t    end_idx = -1\n\t\n\t    # Step 1: Find the first complete {...} block\n\t    for i, ch in enumerate(text):\n\t        if ch == '{':\n\t            if not brace_stack:\n\t                start_idx = i\n\t            brace_stack.append('{')\n\t        elif ch == '}':\n\t            if brace_stack:\n\t                brace_stack.pop()\n\t                if not brace_stack:\n\t                    end_idx = i + 1  # Include the closing brace\n\t                    break\n\t\n\t    # Step 2: If we found a full JSON block\n\t    if start_idx != -1 and end_idx != -1:\n\t        json_str = text[start_idx:end_idx]\n\t\n\t        try:\n\t            data = json.loads(json_str)\n\t            holding_broker = data.get('holding_broker')  \n\t            print(holding_broker)\n\t            return holding_broker\n\t        except:\n\t            return ''\n\t\n\t    return ''\n\t\n\treturn extract_holding_broker_from_text(previous_line)\n\t",
            "id": 17112,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/cf5c549d-3664-4dd0-a578-144b87c10f37_output.json",
            "name": "check_return_holding_broker_value",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/cf5c549d-3664-4dd0-a578-144b87c10f37.json",
            "type": "REFINER"
        },
        "17113": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Driver Details@0"
                }
            ],
            "code": "\n\ndef get_cleaned_df(previous_line, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json, re, ast\n\timport traceback\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = [\n\t         \"Driver Name\",\n\t         \"Driver DOB\",\n\t         \"Licence Date\",\n\t         \"Conviction Code\",\n\t          \"Driver Claims\"\n\t    ]\n\t\n\t\n\tdef get_df_with_regex_match(previous_line, columns_order):\n\t    parsed, inner_content = None, None\n\t    df = pd.DataFrame(columns=columns_order)\n\t    try:\n\t        match = re.search(r\"```(?:\\w+)?\\s*([\\s\\S]*?)\\s*```\", previous_line)\n\t        if match:\n\t            inner_content = match.group(1).strip()\n\t    \n\t            # Step 2: Try to parse it as JSON\n\t            try:\n\t                parsed = json.loads(inner_content)\n\t                print(parsed)\n\t            except json.JSONDecodeError:\n\t                # If JSON parsing fails, fallback to literal_eval\n\t                try:\n\t                    parsed = ast.literal_eval(inner_content)\n\t                except Exception as e:\n\t                    print(\"Parsing failed:\", e)\n\t                    parsed = None\n\t          \n\t        if parsed:\n\t            df = convert_to_dataframe(parsed)\n\t            return True, df \n\t        else:\n\t            return False, df \n\t    except Exception as e:\n\t      return False, pd.DataFrame(columns=columns_order)\n\t\n\tdef parse_markdown_format(previous_line, columns_order):\n\t    try:\n\t        lines = [line.strip() for line in previous_line.strip().split('\\n') if line.strip().startswith('|') and '---' not in line]\n\t        list_of_lists = [ [cell.strip() for cell in line.strip('|').split('|')] for line in lines ]\n\t        if list_of_lists:\n\t            df = convert_to_dataframe(list_of_lists)\n\t            return True, df\n\t        else:\n\t            return False, pd.DataFrame(columns=columns_order)\n\t    except:\n\t        return False, pd.DataFrame(columns=columns_order)\n\t\n\ttry:\n\t    \n\t    df = pd.DataFrame(columns=columns_order)\n\t    \n\t    flg, df = get_df_with_regex_match(previous_line, columns_order)\n\t    \n\t    if not flg:\n\t        flg, df = parse_markdown_format(previous_line, columns_order)\n\t\n\t    if not flg:\n\t      try:\n\t          data = json.loads(previous_line)\n\t          df = convert_to_dataframe(data)\n\t      except Exception as e:\n\t          data = ast.literal_eval(previous_line)\n\t          df = convert_to_dataframe(data)\n\t\n\t    \n\t    df.fillna(\"\", inplace=True)\n\t    \n\t    if len(df.columns.tolist()) > 0:\n\t        # df[\"Conviction Code\"] = df[\"Conviction Code\"].replace([\"No\", \"no\"], \"\")\n\t        df[\"Conviction Code\"] = df[\"Conviction Code\"].astype(str).str.strip().replace([\"no\", \"No\"], \"\")\n\t        df[\"Licence Date\"] = df[\"Licence Date\"].apply(\n\t    lambda x: f\"01/01/{x.strip()}\" if re.fullmatch(r\"\\s*(19|20)\\d{2}\\s*\", str(x)) else x)\n\t        df.replace(to_replace=[\"N/A\", \"null\"], value=\"\", inplace=True)\n\t        df = df[columns_order]\n\t        df = [df.columns.tolist()] + df.values.tolist()        \n\t        return df\n\t    else:\n\t        return  [columns_order]\n\t      \n\t      \n\texcept Exception as e:\n\t    return [columns_order]",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json, re, ast\n\timport traceback\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = [\n\t         \"Driver Name\",\n\t         \"Driver DOB\",\n\t         \"Licence Date\",\n\t         \"Conviction Code\",\n\t          \"Driver Claims\"\n\t    ]\n\t\n\t\n\tdef get_df_with_regex_match(previous_line, columns_order):\n\t    parsed, inner_content = None, None\n\t    df = pd.DataFrame(columns=columns_order)\n\t    try:\n\t        match = re.search(r\"```(?:\\w+)?\\s*([\\s\\S]*?)\\s*```\", previous_line)\n\t        if match:\n\t            inner_content = match.group(1).strip()\n\t    \n\t            # Step 2: Try to parse it as JSON\n\t            try:\n\t                parsed = json.loads(inner_content)\n\t                print(parsed)\n\t            except json.JSONDecodeError:\n\t                # If JSON parsing fails, fallback to literal_eval\n\t                try:\n\t                    parsed = ast.literal_eval(inner_content)\n\t                except Exception as e:\n\t                    print(\"Parsing failed:\", e)\n\t                    parsed = None\n\t          \n\t        if parsed:\n\t            df = convert_to_dataframe(parsed)\n\t            return True, df \n\t        else:\n\t            return False, df \n\t    except Exception as e:\n\t      return False, pd.DataFrame(columns=columns_order)\n\t\n\tdef parse_markdown_format(previous_line, columns_order):\n\t    try:\n\t        lines = [line.strip() for line in previous_line.strip().split('\\n') if line.strip().startswith('|') and '---' not in line]\n\t        list_of_lists = [ [cell.strip() for cell in line.strip('|').split('|')] for line in lines ]\n\t        if list_of_lists:\n\t            df = convert_to_dataframe(list_of_lists)\n\t            return True, df\n\t        else:\n\t            return False, pd.DataFrame(columns=columns_order)\n\t    except:\n\t        return False, pd.DataFrame(columns=columns_order)\n\t\n\ttry:\n\t    \n\t    df = pd.DataFrame(columns=columns_order)\n\t    \n\t    flg, df = get_df_with_regex_match(previous_line, columns_order)\n\t    \n\t    if not flg:\n\t        flg, df = parse_markdown_format(previous_line, columns_order)\n\t\n\t    if not flg:\n\t      try:\n\t          data = json.loads(previous_line)\n\t          df = convert_to_dataframe(data)\n\t      except Exception as e:\n\t          data = ast.literal_eval(previous_line)\n\t          df = convert_to_dataframe(data)\n\t\n\t    \n\t    df.fillna(\"\", inplace=True)\n\t    \n\t    if len(df.columns.tolist()) > 0:\n\t        # df[\"Conviction Code\"] = df[\"Conviction Code\"].replace([\"No\", \"no\"], \"\")\n\t        df[\"Conviction Code\"] = df[\"Conviction Code\"].astype(str).str.strip().replace([\"no\", \"No\"], \"\")\n\t        df[\"Licence Date\"] = df[\"Licence Date\"].apply(\n\t    lambda x: f\"01/01/{x.strip()}\" if re.fullmatch(r\"\\s*(19|20)\\d{2}\\s*\", str(x)) else x)\n\t        df.replace(to_replace=[\"N/A\", \"null\"], value=\"\", inplace=True)\n\t        df = df[columns_order]\n\t        df = [df.columns.tolist()] + df.values.tolist()        \n\t        return df\n\t    else:\n\t        return  [columns_order]\n\t      \n\t      \n\texcept Exception as e:\n\t    return [columns_order]",
            "id": 17113,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8410296b-4bd9-429c-9e4c-dacaea295a83_output.json",
            "name": "get_cleaned_df",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8410296b-4bd9-429c-9e4c-dacaea295a83.json",
            "type": "REFINER"
        },
        "17114": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Driver Age@0"
                }
            ],
            "code": "\n\ndef refine_result(previous_line, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Age']\n\t\n\ttry:\n\t  table = json.loads(previous_line)\n\t  df = convert_to_dataframe(table)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Age']\n\t\n\ttry:\n\t  table = json.loads(previous_line)\n\t  df = convert_to_dataframe(table)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
            "id": 17114,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/dccd92d3-5547-47c2-8bd5-b2ab25535090_output.json",
            "name": "refine_result",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/dccd92d3-5547-47c2-8bd5-b2ab25535090.json",
            "type": "REFINER"
        },
        "17115": {
            "args": [],
            "code": "\n\ndef get_todays_date(context = {}, keys = {}, **kwargs):\n\tfrom datetime import date\n\ttoday = str(date.today())\n\tprint(today)\n\treturn today",
            "docstring": null,
            "function_code": "\tfrom datetime import date\n\ttoday = str(date.today())\n\tprint(today)\n\treturn today",
            "id": 17115,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/9c2bd03c-1d35-4890-ac2a-530f8571ca51_output.json",
            "name": "get_todays_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/9c2bd03c-1d35-4890-ac2a-530f8571ca51.json",
            "type": "REFINER"
        },
        "17116": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Driver Licence Tenure@0"
                }
            ],
            "code": "\n\ndef unnamed_custom_function(previous_line, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Licence Tenure']\n\t\n\ttry:\n\t  table = json.loads(previous_line)\n\t  df = convert_to_dataframe(table)\n\t  try:\n\t    df['Licence Tenure'] = df['Licence Tenure'].apply(lambda x: '5+' if x > 5 else x)\n\t  except:\n\t    df['Licence Tenure'] = df['Licence Tenure'].apply(lambda x: '5+' if str(x).replace('.', '', 1).isdigit() and float(x) > 5 else x)\n\t  print(df)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Licence Tenure']\n\t\n\ttry:\n\t  table = json.loads(previous_line)\n\t  df = convert_to_dataframe(table)\n\t  try:\n\t    df['Licence Tenure'] = df['Licence Tenure'].apply(lambda x: '5+' if x > 5 else x)\n\t  except:\n\t    df['Licence Tenure'] = df['Licence Tenure'].apply(lambda x: '5+' if str(x).replace('.', '', 1).isdigit() and float(x) > 5 else x)\n\t  print(df)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
            "id": 17116,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/df889ffe-9806-4d29-8914-05a986f84061_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/df889ffe-9806-4d29-8914-05a986f84061.json",
            "type": "REFINER"
        },
        "17117": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Vehicle Registration Numbers And Cover Basis@0"
                }
            ],
            "code": "\n\ndef clean_llm_result(previous_line, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = ['Vehicle Registration', 'Cover Basis']\n\ttry:\n\t  previous_line = json.loads(previous_line)\n\t  if previous_line:\n\t      df = convert_to_dataframe(previous_line)\n\t      if len(df.columns.tolist()) > 0:\n\t          return [df.columns.tolist()] + df.values.tolist()\n\t      else:\n\t        return [columns_order]\n\t  else:\n\t      return [columns_order]\n\texcept:\n\t  return [columns_order]\n\t  ",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = ['Vehicle Registration', 'Cover Basis']\n\ttry:\n\t  previous_line = json.loads(previous_line)\n\t  if previous_line:\n\t      df = convert_to_dataframe(previous_line)\n\t      if len(df.columns.tolist()) > 0:\n\t          return [df.columns.tolist()] + df.values.tolist()\n\t      else:\n\t        return [columns_order]\n\t  else:\n\t      return [columns_order]\n\texcept:\n\t  return [columns_order]\n\t  ",
            "id": 17117,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/cf069a5b-04a9-4493-81de-749c224bfb74_output.json",
            "name": "clean_llm_result",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/cf069a5b-04a9-4493-81de-749c224bfb74.json",
            "type": "REFINER"
        },
        "17118": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Driver_Age",
                    "value": "Driver Age"
                },
                {
                    "data_type": "FIELD",
                    "name": "Driver_Licence_Tenure",
                    "value": "Driver Licence Tenure"
                },
                {
                    "data_type": "FIELD",
                    "name": "Driver_Details",
                    "value": "Driver Details"
                }
            ],
            "code": "\n\ndef merge_driver_age_licence_tenure(Driver_Age, Driver_Licence_Tenure, Driver_Details, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\ttry:\n\t  print(\"yes\", Driver_Licence_Tenure)\n\t  merged_df = []\n\t\n\t  try:\n\t      Driver_Details = json.loads(Driver_Details)\n\t      Driver_Details = convert_to_dataframe(Driver_Details)\n\t      merged_df = Driver_Details.copy()\n\t  except Exception as e:\n\t      \n\t      print(\"no driver details \", e)\n\t\n\t  # try:\n\t  #     Conviction_Code = json.loads(Conviction_Code)\n\t  #     Conviction_Code = convert_to_dataframe(Conviction_Code)\n\t  #     print(Conviction_Code)\n\t  # except Exception as e:\n\t  #     print(\"no driver details \", e)\n\t\n\t  try:\n\t      Driver_Age = json.loads(Driver_Age)\n\t      Driver_Age = convert_to_dataframe(Driver_Age)\n\t  except Exception as e:\n\t      print(\"no driver conviction details \", e)\n\t\n\t  try:\n\t      Driver_Licence_Tenure = json.loads(Driver_Licence_Tenure)\n\t      Driver_Licence_Tenure = convert_to_dataframe(Driver_Licence_Tenure)\n\t     \n\t  except Exception as e:\n\t      print(\"no driver licence details \", e)\n\t\n\t  try:\n\t      merged_df = pd.merge(Driver_Details, Driver_Age,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      merged_df = Driver_Details.copy()\n\t      print(\"merging issue  \", e)\n\t\n\t  try:\n\t      \n\t      merged_df = pd.merge(merged_df, Driver_Licence_Tenure,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(\"merging issue  \", e)\n\t\n\t  # try:\n\t  #     # print(\"before\", merged_df.columns)\n\t  #     if Conviction_Code.empty:\n\t  #       print(\"yes\")\n\t  #     else:\n\t  #       merged_df.drop(columns=['Conviction Code'], inplace=True)\n\t  #       merged_df = pd.merge(merged_df, Conviction_Code, on='Driver Name', how='outer')\n\t  #     # print(\"after\", merged_df.columns)\n\t  # except Exception as e:\n\t\n\t      # merged_df_all.fillna('', inplace=True)\n\t      # merged_Df = [merged_df_all.columns.tolist()] + merged_df_all.values.tolist()\n\t      print(\"merging issue in conviction details\", e)\n\t      # return merged_f\n\t      \n\t\n\t  merged_df.fillna('', inplace=True)\n\t\n\t  if len(merged_df.columns.tolist()) > 0:\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [[\"Driver Name\",\t\"Driver DOB\",\t\"Licence Date\",\t\"Age\",\t\"Licence Tenure\",\t\"Conviction Code\"]]\n\t\n\texcept:\n\t  return [[\"Driver Name\",\t\"Driver DOB\",\t\"Licence Date\",\t\"Age\",\t\"Licence Tenure\",\t\"Conviction Code\"]]",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\ttry:\n\t  print(\"yes\", Driver_Licence_Tenure)\n\t  merged_df = []\n\t\n\t  try:\n\t      Driver_Details = json.loads(Driver_Details)\n\t      Driver_Details = convert_to_dataframe(Driver_Details)\n\t      merged_df = Driver_Details.copy()\n\t  except Exception as e:\n\t      \n\t      print(\"no driver details \", e)\n\t\n\t  # try:\n\t  #     Conviction_Code = json.loads(Conviction_Code)\n\t  #     Conviction_Code = convert_to_dataframe(Conviction_Code)\n\t  #     print(Conviction_Code)\n\t  # except Exception as e:\n\t  #     print(\"no driver details \", e)\n\t\n\t  try:\n\t      Driver_Age = json.loads(Driver_Age)\n\t      Driver_Age = convert_to_dataframe(Driver_Age)\n\t  except Exception as e:\n\t      print(\"no driver conviction details \", e)\n\t\n\t  try:\n\t      Driver_Licence_Tenure = json.loads(Driver_Licence_Tenure)\n\t      Driver_Licence_Tenure = convert_to_dataframe(Driver_Licence_Tenure)\n\t     \n\t  except Exception as e:\n\t      print(\"no driver licence details \", e)\n\t\n\t  try:\n\t      merged_df = pd.merge(Driver_Details, Driver_Age,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      merged_df = Driver_Details.copy()\n\t      print(\"merging issue  \", e)\n\t\n\t  try:\n\t      \n\t      merged_df = pd.merge(merged_df, Driver_Licence_Tenure,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(\"merging issue  \", e)\n\t\n\t  # try:\n\t  #     # print(\"before\", merged_df.columns)\n\t  #     if Conviction_Code.empty:\n\t  #       print(\"yes\")\n\t  #     else:\n\t  #       merged_df.drop(columns=['Conviction Code'], inplace=True)\n\t  #       merged_df = pd.merge(merged_df, Conviction_Code, on='Driver Name', how='outer')\n\t  #     # print(\"after\", merged_df.columns)\n\t  # except Exception as e:\n\t\n\t      # merged_df_all.fillna('', inplace=True)\n\t      # merged_Df = [merged_df_all.columns.tolist()] + merged_df_all.values.tolist()\n\t      print(\"merging issue in conviction details\", e)\n\t      # return merged_f\n\t      \n\t\n\t  merged_df.fillna('', inplace=True)\n\t\n\t  if len(merged_df.columns.tolist()) > 0:\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [[\"Driver Name\",\t\"Driver DOB\",\t\"Licence Date\",\t\"Age\",\t\"Licence Tenure\",\t\"Conviction Code\"]]\n\t\n\texcept:\n\t  return [[\"Driver Name\",\t\"Driver DOB\",\t\"Licence Date\",\t\"Age\",\t\"Licence Tenure\",\t\"Conviction Code\"]]",
            "id": 17118,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/b9e26847-7ac1-4ffa-b865-008cc23ff129_output.json",
            "name": "merge_driver_age_licence_tenure",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/b9e26847-7ac1-4ffa-b865-008cc23ff129.json",
            "type": "REFINER"
        },
        "17119": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Driver_Merged_Data",
                    "value": "Driver Merged Data"
                }
            ],
            "code": "\n\ndef get_driver_type(Driver_Merged_Data, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\t\n\tdef classify_driver(row):\n\t    try:\n\t        age = int(row[\"Age\"])\n\t        conviction_code = str(row[\"Conviction Code\"]).strip()\n\t        \n\t        if age < 25:\n\t            return \"Young Driver\"\n\t        elif age >= 25 and str(conviction_code).strip() not in [\"\", None,\"N/A\",\"null\"]:\n\t            return \"Convicted Driver\"\n\t        else:\n\t            return \"\"\n\t    except:\n\t        return \"\"  # Fallback in case of data issues\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Driver Type']\n\t\n\ttry:\n\t  table = json.loads(Driver_Merged_Data)\n\t  df = convert_to_dataframe(table)\n\t  df[\"Driver Type\"] = df.apply(classify_driver, axis=1)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\t\n\tdef classify_driver(row):\n\t    try:\n\t        age = int(row[\"Age\"])\n\t        conviction_code = str(row[\"Conviction Code\"]).strip()\n\t        \n\t        if age < 25:\n\t            return \"Young Driver\"\n\t        elif age >= 25 and str(conviction_code).strip() not in [\"\", None,\"N/A\",\"null\"]:\n\t            return \"Convicted Driver\"\n\t        else:\n\t            return \"\"\n\t    except:\n\t        return \"\"  # Fallback in case of data issues\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns = ['Driver Name', 'Driver Type']\n\t\n\ttry:\n\t  table = json.loads(Driver_Merged_Data)\n\t  df = convert_to_dataframe(table)\n\t  df[\"Driver Type\"] = df.apply(classify_driver, axis=1)\n\t  return [columns] + df[columns].values.tolist()\n\t  \n\texcept Exception as e:\n\t  print(e)\n\t  return [columns]",
            "id": 17119,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/aad6e786-106b-4f9f-8db7-6c02f28c2500_output.json",
            "name": "get_driver_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/aad6e786-106b-4f9f-8db7-6c02f28c2500.json",
            "type": "REFINER"
        },
        "17120": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Registration_Numbers_And_Cover_Basis",
                    "value": "Vehicle Registration Numbers And Cover Basis"
                }
            ],
            "code": "\n\ndef get_unique_values_from_cover_basis_col(Vehicle_Registration_Numbers_And_Cover_Basis, context = {}, keys = {}, **kwargs):\n\t# # Import Python packages\n\t# import json\n\t\n\t# list_of_dct = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t# unique_cover_basis_set = set(item['Cover Basis'] for item in list_of_dct)\n\t# unique_cover_basis_list = list(unique_cover_basis_set)\n\t# unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t# return unique_cover_basis_list_without_na\n\t\n\t# # Import Python packages\n\timport json\n\timport pandas as pd\n\t\n\t# list_of_dct = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t# # print(list_of_dct)\n\t\n\t# try:\n\t#     unique_cover_basis_set = set(item['Cover Vehicle'] for item in list_of_dct)\n\t#     unique_cover_basis_list = list(unique_cover_basis_set)\n\t#     unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t#     return unique_cover_basis_list_without_na\n\t\n\t# except Exception as e:\n\t#     print(\"in ex\", e)\n\t#     # print(type(list_of_dct))\n\t#     unique_first_col = list(set(row[0] for row in list_of_dct))\n\t#     if 'Cover Vehicle' in unique_first_col:\n\t#         unique_first_col.remove('Cover Vehicle')\n\t\n\t#     return unique_first_col\n\t    \n\t\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tVehicle_Registration_Numbers_And_Cover_Basis = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t\n\tif Vehicle_Registration_Numbers_And_Cover_Basis in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tVehicle_Registration_Numbers_And_Cover_Basis = convert_to_dataframe(Vehicle_Registration_Numbers_And_Cover_Basis)\n\tunique_vals = Vehicle_Registration_Numbers_And_Cover_Basis['Cover Basis'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
            "docstring": null,
            "function_code": "\t# # Import Python packages\n\t# import json\n\t\n\t# list_of_dct = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t# unique_cover_basis_set = set(item['Cover Basis'] for item in list_of_dct)\n\t# unique_cover_basis_list = list(unique_cover_basis_set)\n\t# unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t# return unique_cover_basis_list_without_na\n\t\n\t# # Import Python packages\n\timport json\n\timport pandas as pd\n\t\n\t# list_of_dct = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t# # print(list_of_dct)\n\t\n\t# try:\n\t#     unique_cover_basis_set = set(item['Cover Vehicle'] for item in list_of_dct)\n\t#     unique_cover_basis_list = list(unique_cover_basis_set)\n\t#     unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t#     return unique_cover_basis_list_without_na\n\t\n\t# except Exception as e:\n\t#     print(\"in ex\", e)\n\t#     # print(type(list_of_dct))\n\t#     unique_first_col = list(set(row[0] for row in list_of_dct))\n\t#     if 'Cover Vehicle' in unique_first_col:\n\t#         unique_first_col.remove('Cover Vehicle')\n\t\n\t#     return unique_first_col\n\t    \n\t\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tVehicle_Registration_Numbers_And_Cover_Basis = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t\n\tif Vehicle_Registration_Numbers_And_Cover_Basis in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tVehicle_Registration_Numbers_And_Cover_Basis = convert_to_dataframe(Vehicle_Registration_Numbers_And_Cover_Basis)\n\tunique_vals = Vehicle_Registration_Numbers_And_Cover_Basis['Cover Basis'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
            "id": 17120,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2ac46dc8-de85-467a-b112-bfa25e201f9a_output.json",
            "name": "get_unique_values_from_cover_basis_col",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2ac46dc8-de85-467a-b112-bfa25e201f9a.json",
            "type": "REFINER"
        },
        "17121": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Cover Basis Mapping@0"
                }
            ],
            "code": "\n\ndef refine_result(previous_line, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\ttry:\n\t    if previous_line not in [\"\", None, \"[]\", \" \", \"N/A\", \"null\"]:\n\t        result = previous_line\n\t    else:\n\t        result = \"{}\"\n\t\n\texcept Exception as e:\n\t    print(e)\n\t    result = \"{}\"\n\t\n\treturn result  # Ensure this is inside a function\n\t",
            "docstring": null,
            "function_code": "\timport json\n\t\n\ttry:\n\t    if previous_line not in [\"\", None, \"[]\", \" \", \"N/A\", \"null\"]:\n\t        result = previous_line\n\t    else:\n\t        result = \"{}\"\n\t\n\texcept Exception as e:\n\t    print(e)\n\t    result = \"{}\"\n\t\n\treturn result  # Ensure this is inside a function\n\t",
            "id": 17121,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/779eba7d-8f72-474b-831c-a54526201e57_output.json",
            "name": "refine_result",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/779eba7d-8f72-474b-831c-a54526201e57.json",
            "type": "REFINER"
        },
        "17122": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Registration_Numbers_And_Cover_Basis",
                    "value": "Vehicle Registration Numbers And Cover Basis"
                },
                {
                    "data_type": "FIELD",
                    "name": "Unique__Values__Cover__Basis",
                    "value": "Unique_Values_Cover_Basis"
                },
                {
                    "data_type": "FIELD",
                    "name": "Cover_Basis_Mapping",
                    "value": "Cover Basis Mapping"
                },
                {
                    "data_type": "FIELD",
                    "name": "Effective_From",
                    "value": "Effective From"
                },
                {
                    "data_type": "FIELD",
                    "name": "Effective_To",
                    "value": "Effective To"
                }
            ],
            "code": "\n\ndef prepare_vehicle_schedule_table(Vehicle_Registration_Numbers_And_Cover_Basis, Unique__Values__Cover__Basis, Cover_Basis_Mapping, Effective_From, Effective_To, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tcolumns_order = ['Effective From', \\\n\t                    'Effective To', \\\n\t                    'Vehicle Registration', \\\n\t                    'Cover - Vehicle', \\\n\t                    'Cover - Vehicle - Mapped']\n\t\n\tif Vehicle_Registration_Numbers_And_Cover_Basis == \"[]\":\n\t  return [columns_order]\n\t\n\ttry:\n\t    vehicle_num_and_cover_type_table = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t    vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t\n\t\n\t    vehicle_num_and_cover_type_table_df.rename(columns={\n\t        'Cover Basis' : 'Cover - Vehicle',\n\t        'Vehicle Registration Number' : 'Vehicle Registration' \n\t      }, inplace=True)\n\t\n\t    try:\n\t        Unique__Values__Cover__Basis = json.loads(Unique__Values__Cover__Basis)\n\t    except:\n\t        Unique__Values__Cover__Basis = eval(Unique__Values__Cover__Basis)\n\t\n\t\n\t    if Unique__Values__Cover__Basis:\n\t      try:\n\t        Cover_Basis_Mapping_dct = json.loads(Cover_Basis_Mapping)\n\t        vehicle_num_and_cover_type_table_df['Cover - Vehicle - Mapped'] = vehicle_num_and_cover_type_table_df['Cover - Vehicle'].replace(Cover_Basis_Mapping_dct)\n\t      except:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.copy()\n\t\n\t    vehicle_num_and_cover_type_table_df['Effective From'] = Effective_From\n\t    try:\n\t      vehicle_num_and_cover_type_table_df['Effective To'] = Effective_To\n\t    except:\n\t      vehicle_num_and_cover_type_table_df['Effective To'] = ''\n\t\n\t    for col in columns_order:\n\t        if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t    \n\t    invalid_values = [\"\", \"n/a\", \"null\", \"none\", \"nan\"]\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t\n\t    # if vehicle_num_and_cover_type_table_df.empty and ((Effective_From not in ['','N/A']) or (Effective_To not in ['','N/A'])):\n\t    #   vehicle_num_and_cover_type_table_df.loc[0] = {'Effective From': Effective_From, 'Effective To': Effective_To}\n\t\n\t    vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t    \n\t    if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t        vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t        return vehicle_num_and_cover_type_table_df\n\t    else:\n\t        return [columns_order]\n\texcept:\n\t    return [columns_order]",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tcolumns_order = ['Effective From', \\\n\t                    'Effective To', \\\n\t                    'Vehicle Registration', \\\n\t                    'Cover - Vehicle', \\\n\t                    'Cover - Vehicle - Mapped']\n\t\n\tif Vehicle_Registration_Numbers_And_Cover_Basis == \"[]\":\n\t  return [columns_order]\n\t\n\ttry:\n\t    vehicle_num_and_cover_type_table = json.loads(Vehicle_Registration_Numbers_And_Cover_Basis)\n\t    vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t\n\t\n\t    vehicle_num_and_cover_type_table_df.rename(columns={\n\t        'Cover Basis' : 'Cover - Vehicle',\n\t        'Vehicle Registration Number' : 'Vehicle Registration' \n\t      }, inplace=True)\n\t\n\t    try:\n\t        Unique__Values__Cover__Basis = json.loads(Unique__Values__Cover__Basis)\n\t    except:\n\t        Unique__Values__Cover__Basis = eval(Unique__Values__Cover__Basis)\n\t\n\t\n\t    if Unique__Values__Cover__Basis:\n\t      try:\n\t        Cover_Basis_Mapping_dct = json.loads(Cover_Basis_Mapping)\n\t        vehicle_num_and_cover_type_table_df['Cover - Vehicle - Mapped'] = vehicle_num_and_cover_type_table_df['Cover - Vehicle'].replace(Cover_Basis_Mapping_dct)\n\t      except:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.copy()\n\t\n\t    vehicle_num_and_cover_type_table_df['Effective From'] = Effective_From\n\t    try:\n\t      vehicle_num_and_cover_type_table_df['Effective To'] = Effective_To\n\t    except:\n\t      vehicle_num_and_cover_type_table_df['Effective To'] = ''\n\t\n\t    for col in columns_order:\n\t        if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t    \n\t    invalid_values = [\"\", \"n/a\", \"null\", \"none\", \"nan\"]\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t\n\t    # if vehicle_num_and_cover_type_table_df.empty and ((Effective_From not in ['','N/A']) or (Effective_To not in ['','N/A'])):\n\t    #   vehicle_num_and_cover_type_table_df.loc[0] = {'Effective From': Effective_From, 'Effective To': Effective_To}\n\t\n\t    vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t    \n\t    if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t        vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t        return vehicle_num_and_cover_type_table_df\n\t    else:\n\t        return [columns_order]\n\texcept:\n\t    return [columns_order]",
            "id": 17122,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/49a293e1-4694-44ff-87b0-52eaa930a355_output.json",
            "name": "prepare_vehicle_schedule_table",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/49a293e1-4694-44ff-87b0-52eaa930a355.json",
            "type": "REFINER"
        },
        "17123": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Schedule_Table",
                    "value": "Vehicle Schedule Table"
                }
            ],
            "code": "\n\ndef get_number_of_notifiable_vehicles_count(Vehicle_Schedule_Table, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame( data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\ttry:\n\t  if Vehicle_Schedule_Table == \"[]\" or Vehicle_Schedule_Table == 'n/a':\n\t    return \"0\"\n\t  else:\n\t    Vehicle_Schedule_Table = json.loads(Vehicle_Schedule_Table)\n\t    Vehicle_Schedule_Table = convert_to_dataframe(Vehicle_Schedule_Table)\n\t    return len(Vehicle_Schedule_Table['Vehicle Registration'])\n\texcept:\n\t  return \"0\"",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame( data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\ttry:\n\t  if Vehicle_Schedule_Table == \"[]\" or Vehicle_Schedule_Table == 'n/a':\n\t    return \"0\"\n\t  else:\n\t    Vehicle_Schedule_Table = json.loads(Vehicle_Schedule_Table)\n\t    Vehicle_Schedule_Table = convert_to_dataframe(Vehicle_Schedule_Table)\n\t    return len(Vehicle_Schedule_Table['Vehicle Registration'])\n\texcept:\n\t  return \"0\"",
            "id": 17123,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/fe1f071f-6e83-44f5-b5c9-e772c187a1f4_output.json",
            "name": "get_number_of_notifiable_vehicles_count",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/fe1f071f-6e83-44f5-b5c9-e772c187a1f4.json",
            "type": "REFINER"
        },
        "17124": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Target_Price",
                    "value": "Target Price"
                },
                {
                    "data_type": "FIELD",
                    "name": "Number_of_Notifiable_Vehicles",
                    "value": "Number of Notifiable Vehicles"
                }
            ],
            "code": "\n\ndef get_offering_type(Target_Price, Number_of_Notifiable_Vehicles, context = {}, keys = {}, **kwargs):\n\t# def clean_premium(premium_value):\n\t\n\t#     if premium_value is None:\n\t#         return None\n\t\n\t#     if isinstance(premium_value, (int, float)):\n\t#         return float(premium_value)\n\t\n\t#     try:\n\t#         # Remove currency symbols, commas, and whitespace\n\t#         cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t#         return float(cleaned)\n\t#     except Exception:\n\t#         return None\n\t\n\t# def determine_offering_type(vehicle_count, premium_cleaned=None):\n\t\n\t#     if vehicle_count is None:\n\t#         return \"Unknown\"\n\t#     if vehicle_count <= 19:\n\t#         if premium_cleaned is None or premium_cleaned < 10000:\n\t#             return \"Mini Fleet\"\n\t#     elif 20 <= vehicle_count <= 149:\n\t#         if premium_cleaned is None or 10000 <= premium_cleaned < 250000:\n\t#             return \"Vantage Fleet\"\n\t#     elif vehicle_count >= 150:\n\t#         if premium_cleaned is None or premium_cleaned >= 250000:\n\t#             return \"Mid Corp\"\n\t\n\t#     return \"Unknown\"\n\t\n\t# try:\n\t#   Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\t# except:\n\t#   return \"\"\n\t\n\t# try:\n\t#   premium_cleaned = clean_premium(Target_Price)\n\t#   print(premium_cleaned)\n\t# except:\n\t#   premium_cleaned = None\n\t\n\t# offering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\t# # print(offering_type)\n\t# return offering_type\n\t\n\t\n\t\n\tdef clean_premium(premium_value):\n\t    if premium_value is None:\n\t        return None\n\t    if isinstance(premium_value, (int, float)):\n\t        return float(premium_value)\n\t    try:\n\t        # Remove currency symbols, commas, and whitespace\n\t        # print(premium_value)\n\t        # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t        cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t        print(cleaned)\n\t        return float(cleaned)\n\t    except Exception:\n\t        return None\n\t\n\tdef determine_offering_type(vehicle_count, premium_cleaned=None):    \n\t\n\t    if vehicle_count == 0:\n\t        if premium_cleaned is None or premium_cleaned < 10000:\n\t            return \"Mini Fleet\"\n\t        \n\t        elif 10000 <= premium_cleaned < 250000:\n\t            return \"Vantage Fleet\"\n\t        \n\t        elif premium_cleaned >= 250000:\n\t            return \"Mid Corp\"\n\t\n\t    if vehicle_count <= 19:\n\t            return \"Mini Fleet\"\n\t            \n\t    elif 20 <= vehicle_count <= 149:\n\t            return \"Vantage Fleet\"\n\t        \n\t    elif vehicle_count >= 150:\n\t            return \"Mid Corp\"\n\t\n\t\n\ttry:\n\t    Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\texcept:\n\t    Number_of_Notifiable_Vehicles = 0\n\t\n\ttry:\n\t    premium_cleaned = clean_premium(Target_Price)\n\texcept:\n\t    premium_cleaned = None\n\t\n\toffering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\treturn offering_type\n\t",
            "docstring": null,
            "function_code": "\t# def clean_premium(premium_value):\n\t\n\t#     if premium_value is None:\n\t#         return None\n\t\n\t#     if isinstance(premium_value, (int, float)):\n\t#         return float(premium_value)\n\t\n\t#     try:\n\t#         # Remove currency symbols, commas, and whitespace\n\t#         cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t#         return float(cleaned)\n\t#     except Exception:\n\t#         return None\n\t\n\t# def determine_offering_type(vehicle_count, premium_cleaned=None):\n\t\n\t#     if vehicle_count is None:\n\t#         return \"Unknown\"\n\t#     if vehicle_count <= 19:\n\t#         if premium_cleaned is None or premium_cleaned < 10000:\n\t#             return \"Mini Fleet\"\n\t#     elif 20 <= vehicle_count <= 149:\n\t#         if premium_cleaned is None or 10000 <= premium_cleaned < 250000:\n\t#             return \"Vantage Fleet\"\n\t#     elif vehicle_count >= 150:\n\t#         if premium_cleaned is None or premium_cleaned >= 250000:\n\t#             return \"Mid Corp\"\n\t\n\t#     return \"Unknown\"\n\t\n\t# try:\n\t#   Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\t# except:\n\t#   return \"\"\n\t\n\t# try:\n\t#   premium_cleaned = clean_premium(Target_Price)\n\t#   print(premium_cleaned)\n\t# except:\n\t#   premium_cleaned = None\n\t\n\t# offering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\t# # print(offering_type)\n\t# return offering_type\n\t\n\t\n\t\n\tdef clean_premium(premium_value):\n\t    if premium_value is None:\n\t        return None\n\t    if isinstance(premium_value, (int, float)):\n\t        return float(premium_value)\n\t    try:\n\t        # Remove currency symbols, commas, and whitespace\n\t        # print(premium_value)\n\t        # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t        cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t        print(cleaned)\n\t        return float(cleaned)\n\t    except Exception:\n\t        return None\n\t\n\tdef determine_offering_type(vehicle_count, premium_cleaned=None):    \n\t\n\t    if vehicle_count == 0:\n\t        if premium_cleaned is None or premium_cleaned < 10000:\n\t            return \"Mini Fleet\"\n\t        \n\t        elif 10000 <= premium_cleaned < 250000:\n\t            return \"Vantage Fleet\"\n\t        \n\t        elif premium_cleaned >= 250000:\n\t            return \"Mid Corp\"\n\t\n\t    if vehicle_count <= 19:\n\t            return \"Mini Fleet\"\n\t            \n\t    elif 20 <= vehicle_count <= 149:\n\t            return \"Vantage Fleet\"\n\t        \n\t    elif vehicle_count >= 150:\n\t            return \"Mid Corp\"\n\t\n\t\n\ttry:\n\t    Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\texcept:\n\t    Number_of_Notifiable_Vehicles = 0\n\t\n\ttry:\n\t    premium_cleaned = clean_premium(Target_Price)\n\texcept:\n\t    premium_cleaned = None\n\t\n\toffering_type = determine_offering_type( Number_of_Notifiable_Vehicles, premium_cleaned )\n\treturn offering_type\n\t",
            "id": 17124,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2107d0d7-54a4-4668-8699-67ebac36fc4c_output.json",
            "name": "get_offering_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2107d0d7-54a4-4668-8699-67ebac36fc4c.json",
            "type": "REFINER"
        },
        "17125": {
            "args": [],
            "code": "\n\ndef get_transaction_type(context = {}, keys = {}, **kwargs):\n\treturn \"New Business\"\n\t",
            "docstring": null,
            "function_code": "\treturn \"New Business\"\n\t",
            "id": 17125,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/c9b628be-d85a-482f-ab1f-34b4b50f9493_output.json",
            "name": "get_transaction_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/c9b628be-d85a-482f-ab1f-34b4b50f9493.json",
            "type": "REFINER"
        },
        "17126": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Driver_Merged_Data",
                    "value": "Driver Merged Data"
                },
                {
                    "data_type": "FIELD",
                    "name": "Driver_Type",
                    "value": "Driver Type"
                }
            ],
            "code": "\n\ndef form_driver_party_table(Driver_Merged_Data, Driver_Type, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\tcolumns_order = [ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']\n\t\n\ttry:\n\t  Driver_Data_Merged = json.loads(Driver_Merged_Data)\n\t  Drive_Type = json.loads(Driver_Type)\n\t\n\t  merged_df = []\n\t\n\t  try:\n\t      Driver_Data_Merged = convert_to_dataframe(Driver_Data_Merged)\n\t  except Exception as e:\n\t      print(e)\n\t  try:\n\t      Drive_Type = convert_to_dataframe(Drive_Type)\n\t  except Exception as e:\n\t      merged_df = Driver_Data_Merged.copy()\n\t      print(e)\n\t\n\t  # print(Driver_Data_Merged.columns)\n\t\n\t  try:\n\t      merged_df = pd.merge(Driver_Data_Merged, Drive_Type,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(e)\n\t  \n\t  try:\n\t      \n\t      rename_dict = {\n\t        'Driver Name': 'Driver Name',\n\t        'Driver DOB' : 'Driver D.O.B',\n\t        'Licence Date': 'Drivers Licence - Date obtained',\n\t        'Licence Tenure':'Driver: Years Appropriate Licence Held',\n\t        'Conviction Code': 'Conviction Details',\n\t        'Driver Type': 'Driver Type'\n\t      }\n\t\n\t      safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t      merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t      \n\t      columns_order = [ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']\n\t\n\t      for col in columns_order:\n\t          if col not in merged_df.columns:\n\t              merged_df[col] = ''\n\t     \n\t      existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t                  \n\t      merged_df.replace('N/A', '', inplace=True)\n\t      merged_df.replace('null', '', inplace=True)\n\t      merged_df = merged_df.fillna('')\n\t      merged_df = merged_df[existing_columns]\n\t  \n\t  except Exception as e:\n\t      print(\"Exception in renaming \", e)\n\t      if len(merged_df.columns.tolist()) > 0:\n\t          merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t          return merged_df\n\t      else:\n\t          return [columns_order]\n\t  \n\t  if len(merged_df.columns.tolist()) > 0:\n\t\n\t      merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t      merged_df = merged_df.dropna(how='all')\n\t      merged_df = merged_df.where(pd.notna(merged_df), \"\")\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept:\n\t  return [columns_order]",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\tcolumns_order = [ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']\n\t\n\ttry:\n\t  Driver_Data_Merged = json.loads(Driver_Merged_Data)\n\t  Drive_Type = json.loads(Driver_Type)\n\t\n\t  merged_df = []\n\t\n\t  try:\n\t      Driver_Data_Merged = convert_to_dataframe(Driver_Data_Merged)\n\t  except Exception as e:\n\t      print(e)\n\t  try:\n\t      Drive_Type = convert_to_dataframe(Drive_Type)\n\t  except Exception as e:\n\t      merged_df = Driver_Data_Merged.copy()\n\t      print(e)\n\t\n\t  # print(Driver_Data_Merged.columns)\n\t\n\t  try:\n\t      merged_df = pd.merge(Driver_Data_Merged, Drive_Type,  on='Driver Name', how='outer')\n\t  except Exception as e:\n\t      print(e)\n\t  \n\t  try:\n\t      \n\t      rename_dict = {\n\t        'Driver Name': 'Driver Name',\n\t        'Driver DOB' : 'Driver D.O.B',\n\t        'Licence Date': 'Drivers Licence - Date obtained',\n\t        'Licence Tenure':'Driver: Years Appropriate Licence Held',\n\t        'Conviction Code': 'Conviction Details',\n\t        'Driver Type': 'Driver Type'\n\t      }\n\t\n\t      safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t      merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t      \n\t      columns_order = [ 'Driver Name' , 'Driver D.O.B', \\\n\t                        'Drivers Licence - Date obtained', \\\n\t                        'Driver: Years Appropriate Licence Held', \\\n\t                        'Conviction Details' , 'Driver Type', 'Driver Claims']\n\t\n\t      for col in columns_order:\n\t          if col not in merged_df.columns:\n\t              merged_df[col] = ''\n\t     \n\t      existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t                  \n\t      merged_df.replace('N/A', '', inplace=True)\n\t      merged_df.replace('null', '', inplace=True)\n\t      merged_df = merged_df.fillna('')\n\t      merged_df = merged_df[existing_columns]\n\t  \n\t  except Exception as e:\n\t      print(\"Exception in renaming \", e)\n\t      if len(merged_df.columns.tolist()) > 0:\n\t          merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t          return merged_df\n\t      else:\n\t          return [columns_order]\n\t  \n\t  if len(merged_df.columns.tolist()) > 0:\n\t\n\t      merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t      merged_df = merged_df.dropna(how='all')\n\t      merged_df = merged_df.where(pd.notna(merged_df), \"\")\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept:\n\t  return [columns_order]",
            "id": 17126,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a6ee1af0-9b2b-426e-99a7-b48159dfe31e_output.json",
            "name": "form_driver_party_table",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a6ee1af0-9b2b-426e-99a7-b48159dfe31e.json",
            "type": "REFINER"
        },
        "17127": {
            "args": [],
            "code": "\n\ndef get_business_category(context = {}, keys = {}, **kwargs):\n\treturn \"Mid Market\"",
            "docstring": null,
            "function_code": "\treturn \"Mid Market\"",
            "id": 17127,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/55ec4135-5714-49d3-bfe9-0d3dfe969918_output.json",
            "name": "get_business_category",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/55ec4135-5714-49d3-bfe9-0d3dfe969918.json",
            "type": "REFINER"
        },
        "17128": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Driver_Details",
                    "value": "Driver Details"
                }
            ],
            "code": "\n\ndef get_driver_name_and_dob(Driver_Details, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\t\n\tcolumns = ['Driver Name', 'Driver DOB']\n\ttry:\n\t  table = json.loads(Driver_Details)\n\t  df = convert_to_dataframe(table)\n\t  return [columns] + df[columns].values.tolist()\n\texcept:\n\t  return Driver_Details",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\t\n\tcolumns = ['Driver Name', 'Driver DOB']\n\ttry:\n\t  table = json.loads(Driver_Details)\n\t  df = convert_to_dataframe(table)\n\t  return [columns] + df[columns].values.tolist()\n\texcept:\n\t  return Driver_Details",
            "id": 17128,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/f6e45c6b-7e14-4576-8264-8ed8f49e8278_output.json",
            "name": "get_driver_name_and_dob",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/f6e45c6b-7e14-4576-8264-8ed8f49e8278.json",
            "type": "REFINER"
        },
        "17129": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Driver_Details",
                    "value": "Driver Details"
                }
            ],
            "code": "\n\ndef get_licence_tenure_and_driver_name(Driver_Details, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\t\n\tcolumns = ['Driver Name', 'Licence Date']\n\ttry:\n\t  table = json.loads(Driver_Details)\n\t  df = convert_to_dataframe(table)\n\t  return [columns] + df[columns].values.tolist()\n\texcept:\n\t  return Driver_Details",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t\n\t\n\tcolumns = ['Driver Name', 'Licence Date']\n\ttry:\n\t  table = json.loads(Driver_Details)\n\t  df = convert_to_dataframe(table)\n\t  return [columns] + df[columns].values.tolist()\n\texcept:\n\t  return Driver_Details",
            "id": 17129,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/19ff786d-3fe2-4acc-9898-81d4eba68895_output.json",
            "name": "get_licence_tenure_and_driver_name",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/19ff786d-3fe2-4acc-9898-81d4eba68895.json",
            "type": "REFINER"
        },
        "17130": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_country(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Country\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Country\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "id": 17130,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/dc38f47d-eff0-490d-83bc-111d636a0e8f_output.json",
            "name": "get_country",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/dc38f47d-eff0-490d-83bc-111d636a0e8f.json",
            "type": "REFINER"
        },
        "17131": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_city(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"City\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"City\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "id": 17131,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/fdef8073-dd77-49fb-a70e-57799ac74d39_output.json",
            "name": "get_city",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/fdef8073-dd77-49fb-a70e-57799ac74d39.json",
            "type": "REFINER"
        },
        "17132": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_state(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"State\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"State\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "id": 17132,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/f1088674-f05c-4844-8d29-af2e01dae470_output.json",
            "name": "get_state",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/f1088674-f05c-4844-8d29-af2e01dae470.json",
            "type": "REFINER"
        },
        "17133": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Main Cover Type Mapped@0"
                }
            ],
            "code": "\n\ndef check_cover_type_mapped(previous_line, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
            "id": 17133,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/3a5f3ff5-a60f-4c90-b596-3d47b60fbe0b_output.json",
            "name": "check_cover_type_mapped",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/3a5f3ff5-a60f-4c90-b596-3d47b60fbe0b.json",
            "type": "REFINER"
        },
        "17134": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line2(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
            "id": 17134,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/865ac791-779e-441c-b734-9fe437b1d881_output.json",
            "name": "get_address_line2",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/865ac791-779e-441c-b734-9fe437b1d881.json",
            "type": "REFINER"
        },
        "17135": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line3(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Address Line 3\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\timport json\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Address Line 3\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "id": 17135,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/73423744-ed04-4388-b16a-cdd3f4202c6a_output.json",
            "name": "get_address_line3",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/73423744-ed04-4388-b16a-cdd3f4202c6a.json",
            "type": "REFINER"
        },
        "17136": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_postcode(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Postcode\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Postcode\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "id": 17136,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/cc021052-59e5-4e58-9bf3-fa662f16aca3_output.json",
            "name": "get_postcode",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/cc021052-59e5-4e58-9bf3-fa662f16aca3.json",
            "type": "REFINER"
        },
        "17137": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line1(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
            "id": 17137,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/7af9cd06-d675-4ea4-a766-9c765f0873c6_output.json",
            "name": "get_address_line1",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/7af9cd06-d675-4ea4-a766-9c765f0873c6.json",
            "type": "REFINER"
        },
        "17138": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Party Address Details@0"
                }
            ],
            "code": "\n\ndef clean(previous_line, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\ttry:\n\t  previous_line = eval(previous_line)\n\t  # print(type(previous_line))\n\texcept:\n\t  previous_line = json.loads(previous_line)\n\t  # print(type(previous_line))\n\t\n\treturn previous_line",
            "docstring": null,
            "function_code": "\timport json\n\t\n\ttry:\n\t  previous_line = eval(previous_line)\n\t  # print(type(previous_line))\n\texcept:\n\t  previous_line = json.loads(previous_line)\n\t  # print(type(previous_line))\n\t\n\treturn previous_line",
            "id": 17138,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/3dddb9d8-5662-4543-805a-c4a33588b243_output.json",
            "name": "clean",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/3dddb9d8-5662-4543-805a-c4a33588b243.json",
            "type": "REFINER"
        },
        "17139": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_List",
                    "value": "Excess List"
                }
            ],
            "code": "\n\ndef unnamed_custom_function(Excess_List, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\t\n\ttable = json.loads(Excess_List)\n\t\n\tif Excess_List in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tExcess_List = convert_to_dataframe(table)\n\tunique_vals = Excess_List['Cover On Policy'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\t\n\ttable = json.loads(Excess_List)\n\t\n\tif Excess_List in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tExcess_List = convert_to_dataframe(table)\n\tunique_vals = Excess_List['Cover On Policy'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
            "id": 17139,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/4091bb1f-578e-42b5-857f-34cdf520a1f5_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/4091bb1f-578e-42b5-857f-34cdf520a1f5.json",
            "type": "REFINER"
        },
        "17140": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Policy Dates@0"
                }
            ],
            "code": "\n\ndef business_logic_for_policy_period_end_date(previous_line, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\tfrom datetime import datetime, timedelta\n\t\n\t# Custom function\n\tdef add_one_year_minus_one_day(date_str):\n\t    try:\n\t        input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t        try:\n\t            one_year_later = input_date.replace(year=input_date.year + 1)\n\t        except ValueError:\n\t            temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t            one_year_later = temp_date\n\t        final_date = one_year_later - timedelta(days=1)\n\t        return final_date.strftime(\"%d/%m/%Y\")\n\t    except Exception as e:\n\t        print(e)\n\t        return ''\n\t\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\t\n\ttry:\n\t    previous_line = eval(previous_line)\n\t    dates_df = convert_to_df(previous_line)\n\texcept:\n\t  try:\n\t      previous_line = json.loads(previous_line)\n\t      dates_df = convert_to_df(previous_line)\n\t  except:\n\t      return previous_line\n\t\n\ttry:\n\t\n\t    # Iterate row-wise\n\t    for index, row in dates_df.iterrows():\n\t        start_date = row['Policy Period Start Date']\n\t        \n\t        # Only update Policy Period End Date if Used Inception Date is 'Yes'\n\t        if str(row.get('Used Inception Date', '')).strip().lower() == 'yes':\n\t            dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t            print(\"Applied for \", row[\"Period\"])\n\t\n\t    # Clean up Period column after the loop\n\t    dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    # Convert to list format for output\n\t    final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    return final_df\n\t\n\t    # print(dates_df.columns)\n\t    # Iterate row-wise\n\t    # for index, row in dates_df.iterrows():\n\t    #     start_date = row['Policy Period Start Date']\n\t    #     dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t    #     dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    #     final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    # return final_df\n\t\n\texcept:\n\t    return \"\"",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\tfrom datetime import datetime, timedelta\n\t\n\t# Custom function\n\tdef add_one_year_minus_one_day(date_str):\n\t    try:\n\t        input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t        try:\n\t            one_year_later = input_date.replace(year=input_date.year + 1)\n\t        except ValueError:\n\t            temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t            one_year_later = temp_date\n\t        final_date = one_year_later - timedelta(days=1)\n\t        return final_date.strftime(\"%d/%m/%Y\")\n\t    except Exception as e:\n\t        print(e)\n\t        return ''\n\t\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\t\n\ttry:\n\t    previous_line = eval(previous_line)\n\t    dates_df = convert_to_df(previous_line)\n\texcept:\n\t  try:\n\t      previous_line = json.loads(previous_line)\n\t      dates_df = convert_to_df(previous_line)\n\t  except:\n\t      return previous_line\n\t\n\ttry:\n\t\n\t    # Iterate row-wise\n\t    for index, row in dates_df.iterrows():\n\t        start_date = row['Policy Period Start Date']\n\t        \n\t        # Only update Policy Period End Date if Used Inception Date is 'Yes'\n\t        if str(row.get('Used Inception Date', '')).strip().lower() == 'yes':\n\t            dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t            print(\"Applied for \", row[\"Period\"])\n\t\n\t    # Clean up Period column after the loop\n\t    dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    # Convert to list format for output\n\t    final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    return final_df\n\t\n\t    # print(dates_df.columns)\n\t    # Iterate row-wise\n\t    # for index, row in dates_df.iterrows():\n\t    #     start_date = row['Policy Period Start Date']\n\t    #     dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t    #     dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    #     final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    # return final_df\n\t\n\texcept:\n\t    return \"\"",
            "id": 17140,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/b9d3cef2-e5ae-4bd1-a985-ccffaa59f814_output.json",
            "name": "business_logic_for_policy_period_end_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/b9d3cef2-e5ae-4bd1-a985-ccffaa59f814.json",
            "type": "REFINER"
        },
        "17141": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Policy_Dates",
                    "value": "Policy Dates"
                }
            ],
            "code": "\n\ndef unnamed_custom_function(Policy_Dates, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t    try:\n\t        Policy_Dates = eval(Policy_Dates)    \n\t    except:\n\t        Policy_Dates = json.loads(Policy_Dates)   \n\t\n\t    Policy_Dates = convert_to_df(Policy_Dates)\n\t    Policy_Dates = Policy_Dates.astype(str)\n\t    print(Policy_Dates.columns)\n\t    CCE_Policy_start_dates_List = Policy_Dates[['Policy Period Start Date']].values.tolist()\n\t    filtered_vals = [each_val for each_val in CCE_Policy_start_dates_List if each_val not in [\"N/A\",\"\",None,\"null\"]]\n\t    if len(filtered_vals)>=1:\n\t      return [[\"Policy Period Start Date\"]] + CCE_Policy_start_dates_List\n\t    else:\n\t      CCE_Period_List = Policy_Dates[['Period']].values.tolist()\n\t      return [\"Policy Period Start Date\"] + [CCE_Period_List]\n\texcept Exception as e:\n\t    print(e)\n\t    return \"\"",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t    try:\n\t        Policy_Dates = eval(Policy_Dates)    \n\t    except:\n\t        Policy_Dates = json.loads(Policy_Dates)   \n\t\n\t    Policy_Dates = convert_to_df(Policy_Dates)\n\t    Policy_Dates = Policy_Dates.astype(str)\n\t    print(Policy_Dates.columns)\n\t    CCE_Policy_start_dates_List = Policy_Dates[['Policy Period Start Date']].values.tolist()\n\t    filtered_vals = [each_val for each_val in CCE_Policy_start_dates_List if each_val not in [\"N/A\",\"\",None,\"null\"]]\n\t    if len(filtered_vals)>=1:\n\t      return [[\"Policy Period Start Date\"]] + CCE_Policy_start_dates_List\n\t    else:\n\t      CCE_Period_List = Policy_Dates[['Period']].values.tolist()\n\t      return [\"Policy Period Start Date\"] + [CCE_Period_List]\n\texcept Exception as e:\n\t    print(e)\n\t    return \"\"",
            "id": 17141,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8d11abe7-9070-41c2-88df-5fddb986d73d_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8d11abe7-9070-41c2-88df-5fddb986d73d.json",
            "type": "REFINER"
        },
        "17142": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_List",
                    "value": "Excess List"
                },
                {
                    "data_type": "FIELD",
                    "name": "cover_mapping",
                    "value": "cover mapping"
                },
                {
                    "data_type": "FIELD",
                    "name": "CCE_Data",
                    "value": "CCE Data"
                },
                {
                    "data_type": "FIELD",
                    "name": "Policy_Dates",
                    "value": "Policy Dates"
                },
                {
                    "data_type": "FIELD",
                    "name": "Year",
                    "value": "Year"
                }
            ],
            "code": "\n\ndef from_CCE_Table(Excess_List, cover_mapping, CCE_Data, Policy_Dates, Year, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            # print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\ttry:\n\t  try:\n\t      try:\n\t          Excess_List = json.loads(Excess_List)\n\t      except:\n\t          Excess_List = eval(Excess_List)\n\t\n\t      Excess_List = convert_to_dataframe(Excess_List)\n\t      if \"Period\" in Excess_List.columns:\n\t          Excess_List[\"Period\"] = Excess_List[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  try:\n\t      try:\n\t          CCE_Data = json.loads(CCE_Data)\n\t      except:\n\t          CCE_Data = eval(CCE_Data)\n\t\n\t      CCE_Data = convert_to_dataframe(CCE_Data)\n\t      if \"Period\" in CCE_Data.columns:\n\t          CCE_Data[\"Period\"] = CCE_Data[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  try:\n\t      try:\n\t          Policy_Dates = json.loads(Policy_Dates)\n\t      except:\n\t          Policy_Dates = eval(Policy_Dates)\n\t\n\t      Policy_Dates = convert_to_dataframe(Policy_Dates)\n\t      if \"Period\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Period\"] = Policy_Dates[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      if \"Policy Period Start Date\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Policy Period Start Date\"] = Policy_Dates[\"Policy Period Start Date\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  try:\n\t      try:\n\t          Year = json.loads(Year)\n\t      except:\n\t          Year = eval(Year)\n\t      Year = convert_to_dataframe(Year)\n\t  except Exception as e:\n\t      print(e)\n\t\n\t\n\t  ## Merging the data to form final CCE table\n\t  try:\n\t      CCE_Data['Period'] = CCE_Data['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      Excess_List['Period'] = Excess_List['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      merged_df1 = pd.merge(CCE_Data, Excess_List,  on='Period', how='inner')\n\t  except:\n\t      merged_df1 = CCE_Data.copy()\n\t      \n\t      print(\"Exception in merging claims info\")\n\t  \n\t  # print(merged_df1)\n\t  \n\t  # merged_df = [merged_df1.columns.tolist()] + merged_df1.values.tolist()\n\t  # return merged_df\n\t  \n\t  try:\n\t      merged_df2 = pd.merge(merged_df1, Policy_Dates,  on='Period', how='inner')\n\t  except Exception as e:\n\t      merged_df2 = merged_df1.copy()\n\t      print(\"Exception in merging policy info\")\n\t\n\t  # merged_df = [merged_df2.columns.tolist()] + merged_df2.values.tolist()\n\t  # return merged_df\n\t  \n\t  # print(merged_df2['Period'])\n\t  # print(Insurer_Data['Period'])\n\t  \n\t  try:\n\t      merged_df3 = pd.merge(merged_df2, Insurer_Data,  on='Period', how='inner')\n\t  except:\n\t      merged_df3 = merged_df2.copy()\n\t      print(\"Exception in merging all details\")\n\t  # print(merged_df3['Period'])\n\t  try:    \n\t      merged_df = pd.merge(merged_df3, Year,  on='Policy Period Start Date', how='inner')\n\t  except:\n\t      merged_df = merged_df3.copy()\n\t      print(\"Exception in merging Year info\")\n\t  \n\t  \n\t  try:\n\t    cover_mapping = json.loads(cover_mapping)\n\t    merged_df['Cover on Policy - Mapped'] = merged_df['Cover On Policy'].map(cover_mapping)\n\t  except Exception as e:\n\t    print(e)\n\t    merged_df = merged_df.copy()\n\t\n\t  rename_dict = {\n\t      'AD Excess': 'Excess: AD',\n\t      'Theft Excess': 'Excess: Theft',\n\t      'Fire Excess': 'Excess: Fire',\n\t      'WS Excess': 'Excess: WS',\n\t      'Cover On Policy': 'Cover on Policy',\n\t      'Vehicle Years Earned': 'Vehicle Years Earned',\n\t      'Claim Count': 'Claim Count: All',\n\t      # 'no of claims windscreen': 'Claim Count: WS',\n\t      'Incurreds Paid AD WS': 'Incurreds - Paid: AD&WS',\n\t      'Incurreds Paid FT': 'Incurreds - Paid: FT',\n\t      'Incurreds Paid TP': 'Incurreds - Paid: TP',\n\t      'Incurreds Outstanding AD WS': 'Incurreds - Outstanding: AD&WS',\n\t      'Incurreds Outstanding FT': 'Incurreds - Outstanding: FT',\n\t      'Incurreds Outstanding TP': 'Incurreds - Outstanding: TP',\n\t      'Total Incurred': 'Total Incurred Paid +  Outstanding',\n\t      'Policy Period Start Date': 'Policy Year Start Date',\n\t      'Policy Period End Date': 'Policy Year End Date'\n\t  }\n\t\n\t  safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t  merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t\n\t  try:\n\t      merged_df['Claim Count: WS'] = ''\n\t  except Exception as e:\n\t    print(e)\n\t  \n\t\n\t  columns_order = [ 'Year', 'Policy Year Start Date', \\\n\t                'Policy Year End Date', \n\t                'Insurer', \n\t                'Excess: AD',\\\n\t                'Excess: Fire', \\\n\t                'Excess: Theft', \\\n\t                'Excess: WS', \\\n\t                'Cover on Policy', \\\n\t                'Cover on Policy - Mapped',\n\t                 'Vehicle Years Earned', \n\t                'Claim Count: All', \\\n\t                'Claim Count: WS', \\\n\t                'Incurreds - Paid: AD&WS', \\\n\t                'Incurreds - Paid: FT', \\\n\t                'Incurreds - Paid: TP', \n\t                'Incurreds - Outstanding: AD&WS', \\\n\t                'Incurreds - Outstanding: FT' ,\n\t                'Incurreds - Outstanding: TP', \\\n\t                'Total Incurred Paid +  Outstanding']\n\t\n\t  # existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t               \n\t  # print(merged_df.columns)\n\t  merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t  merged_df = merged_df.dropna(how='all')\n\t  \n\t  merged_df.replace('N/A', '', inplace=True)\n\t  merged_df = merged_df.fillna('')\n\t\n\t  # merged_df = merged_df[existing_columns]\n\t\n\t  for col in columns_order:\n\t    if col not in merged_df.columns:\n\t      merged_df[col] = '' \n\t\n\t  merged_df = merged_df.drop_duplicates()\n\t  if len(merged_df.columns.tolist()) > 0:\n\t\n\t      merged_df = merged_df[columns_order]\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept Exception as e:\n\t  print(e)\n\t  return [columns_order]",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            # print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\ttry:\n\t  try:\n\t      try:\n\t          Excess_List = json.loads(Excess_List)\n\t      except:\n\t          Excess_List = eval(Excess_List)\n\t\n\t      Excess_List = convert_to_dataframe(Excess_List)\n\t      if \"Period\" in Excess_List.columns:\n\t          Excess_List[\"Period\"] = Excess_List[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  try:\n\t      try:\n\t          CCE_Data = json.loads(CCE_Data)\n\t      except:\n\t          CCE_Data = eval(CCE_Data)\n\t\n\t      CCE_Data = convert_to_dataframe(CCE_Data)\n\t      if \"Period\" in CCE_Data.columns:\n\t          CCE_Data[\"Period\"] = CCE_Data[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  try:\n\t      try:\n\t          Policy_Dates = json.loads(Policy_Dates)\n\t      except:\n\t          Policy_Dates = eval(Policy_Dates)\n\t\n\t      Policy_Dates = convert_to_dataframe(Policy_Dates)\n\t      if \"Period\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Period\"] = Policy_Dates[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      if \"Policy Period Start Date\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Policy Period Start Date\"] = Policy_Dates[\"Policy Period Start Date\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  try:\n\t      try:\n\t          Year = json.loads(Year)\n\t      except:\n\t          Year = eval(Year)\n\t      Year = convert_to_dataframe(Year)\n\t  except Exception as e:\n\t      print(e)\n\t\n\t\n\t  ## Merging the data to form final CCE table\n\t  try:\n\t      CCE_Data['Period'] = CCE_Data['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      Excess_List['Period'] = Excess_List['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      merged_df1 = pd.merge(CCE_Data, Excess_List,  on='Period', how='inner')\n\t  except:\n\t      merged_df1 = CCE_Data.copy()\n\t      \n\t      print(\"Exception in merging claims info\")\n\t  \n\t  # print(merged_df1)\n\t  \n\t  # merged_df = [merged_df1.columns.tolist()] + merged_df1.values.tolist()\n\t  # return merged_df\n\t  \n\t  try:\n\t      merged_df2 = pd.merge(merged_df1, Policy_Dates,  on='Period', how='inner')\n\t  except Exception as e:\n\t      merged_df2 = merged_df1.copy()\n\t      print(\"Exception in merging policy info\")\n\t\n\t  # merged_df = [merged_df2.columns.tolist()] + merged_df2.values.tolist()\n\t  # return merged_df\n\t  \n\t  # print(merged_df2['Period'])\n\t  # print(Insurer_Data['Period'])\n\t  \n\t  try:\n\t      merged_df3 = pd.merge(merged_df2, Insurer_Data,  on='Period', how='inner')\n\t  except:\n\t      merged_df3 = merged_df2.copy()\n\t      print(\"Exception in merging all details\")\n\t  # print(merged_df3['Period'])\n\t  try:    \n\t      merged_df = pd.merge(merged_df3, Year,  on='Policy Period Start Date', how='inner')\n\t  except:\n\t      merged_df = merged_df3.copy()\n\t      print(\"Exception in merging Year info\")\n\t  \n\t  \n\t  try:\n\t    cover_mapping = json.loads(cover_mapping)\n\t    merged_df['Cover on Policy - Mapped'] = merged_df['Cover On Policy'].map(cover_mapping)\n\t  except Exception as e:\n\t    print(e)\n\t    merged_df = merged_df.copy()\n\t\n\t  rename_dict = {\n\t      'AD Excess': 'Excess: AD',\n\t      'Theft Excess': 'Excess: Theft',\n\t      'Fire Excess': 'Excess: Fire',\n\t      'WS Excess': 'Excess: WS',\n\t      'Cover On Policy': 'Cover on Policy',\n\t      'Vehicle Years Earned': 'Vehicle Years Earned',\n\t      'Claim Count': 'Claim Count: All',\n\t      # 'no of claims windscreen': 'Claim Count: WS',\n\t      'Incurreds Paid AD WS': 'Incurreds - Paid: AD&WS',\n\t      'Incurreds Paid FT': 'Incurreds - Paid: FT',\n\t      'Incurreds Paid TP': 'Incurreds - Paid: TP',\n\t      'Incurreds Outstanding AD WS': 'Incurreds - Outstanding: AD&WS',\n\t      'Incurreds Outstanding FT': 'Incurreds - Outstanding: FT',\n\t      'Incurreds Outstanding TP': 'Incurreds - Outstanding: TP',\n\t      'Total Incurred': 'Total Incurred Paid +  Outstanding',\n\t      'Policy Period Start Date': 'Policy Year Start Date',\n\t      'Policy Period End Date': 'Policy Year End Date'\n\t  }\n\t\n\t  safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t  merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t\n\t  try:\n\t      merged_df['Claim Count: WS'] = ''\n\t  except Exception as e:\n\t    print(e)\n\t  \n\t\n\t  columns_order = [ 'Year', 'Policy Year Start Date', \\\n\t                'Policy Year End Date', \n\t                'Insurer', \n\t                'Excess: AD',\\\n\t                'Excess: Fire', \\\n\t                'Excess: Theft', \\\n\t                'Excess: WS', \\\n\t                'Cover on Policy', \\\n\t                'Cover on Policy - Mapped',\n\t                 'Vehicle Years Earned', \n\t                'Claim Count: All', \\\n\t                'Claim Count: WS', \\\n\t                'Incurreds - Paid: AD&WS', \\\n\t                'Incurreds - Paid: FT', \\\n\t                'Incurreds - Paid: TP', \n\t                'Incurreds - Outstanding: AD&WS', \\\n\t                'Incurreds - Outstanding: FT' ,\n\t                'Incurreds - Outstanding: TP', \\\n\t                'Total Incurred Paid +  Outstanding']\n\t\n\t  # existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t               \n\t  # print(merged_df.columns)\n\t  merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t  merged_df = merged_df.dropna(how='all')\n\t  \n\t  merged_df.replace('N/A', '', inplace=True)\n\t  merged_df = merged_df.fillna('')\n\t\n\t  # merged_df = merged_df[existing_columns]\n\t\n\t  for col in columns_order:\n\t    if col not in merged_df.columns:\n\t      merged_df[col] = '' \n\t\n\t  merged_df = merged_df.drop_duplicates()\n\t  if len(merged_df.columns.tolist()) > 0:\n\t\n\t      merged_df = merged_df[columns_order]\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept Exception as e:\n\t  print(e)\n\t  return [columns_order]",
            "id": 17142,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/acd3646b-c0c8-4609-b8b3-bc1fb7f0a952_output.json",
            "name": "from_CCE_Table",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/acd3646b-c0c8-4609-b8b3-bc1fb7f0a952.json",
            "type": "REFINER"
        },
        "17143": {
            "args": [],
            "code": "\n\ndef unnamed_custom_function(context = {}, keys = {}, **kwargs):\n\tdata = [\n\t  {\n\t    \"trade\": \"General Manufacturing\",\n\t    \"aliases\": [\"steel\", \"metal\", \"samples\", \"manufacturing of glass\", \"manufacturing of furniture\", \"manufacturing\", \"general manufacturing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Fleet - Unclassified\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Contractor\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Builders\",\n\t    \"aliases\": [\"Building Contractors\", \"house builders\", \"builders\"]\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers UK only\",\n\t    \"aliases\": [\"haulage contractors - uk only\", \"hauliers uk only\"]\n\t  },\n\t  {\n\t    \"trade\": \"Electrician\",\n\t    \"aliases\": [\"Electrical Contractors\", \"electrical installation\", \"electrical testing\", \"electrician\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plumbing & Heating Engineer\",\n\t    \"aliases\": [\"plumbing & heating contractors\", \"gas servicing\", \"boiler maintenance\", \"plumber\", \"heating engineer\"]\n\t  },\n\t  {\n\t    \"trade\": \"Engineering\",\n\t    \"aliases\": [ \"engineers\", \"mechanical engineers\", \"engineering\", \"electrical engineers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Civil engineering\",\n\t    \"aliases\": [\"civil engineering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Scaffolding Contractors\",\n\t    \"aliases\": [\"scaffolders\", \"scaffolding contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Wholesale (non food and drink)\",\n\t    \"aliases\": [\"wholesalers of electrical components\", \"building materials\", \"wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Cleaning Contractors\",\n\t    \"aliases\": [\"domestic cleaners\", \"office cleaners\", \"cleaning contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plant Hire\",\n\t    \"aliases\": [\"plant hire operator\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Wholesale\",\n\t    \"aliases\": [\"cash & carry\", \"beer & wine wholesalers\", \"food wholesalers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Retail (non food and drink)\",\n\t    \"aliases\": [\"department store\", \"shopping centre\"]\n\t  },\n\t  {\n\t    \"trade\": \"Property Owners\",\n\t    \"aliases\": [\"landlords\", \"commercial property owners\"]\n\t  },\n\t  {\n\t    \"trade\": \"Telecommunications & IT\",\n\t    \"aliases\": [\"telecommunications installation\", \"it installation\", \"it services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Double Glazing\",\n\t    \"aliases\": [\"double glazing manufacture\", \"double glazing installation\"]\n\t  },\n\t  {\n\t    \"trade\": \"Landscape Gardener\",\n\t    \"aliases\": [\"gardening\", \"landscape gardener\"]\n\t  },\n\t  {\n\t    \"trade\": \"Other Prof/Sci/Tech\",\n\t    \"aliases\": [\"laboratory\"]\n\t  },\n\t  {\n\t    \"trade\": \"Removal Contractor\",\n\t    \"aliases\": [\"removals and storage\"]\n\t  },\n\t  {\n\t    \"trade\": \"Security and investigation\",\n\t    \"aliases\": [\"private detectives\", \"security guarding\", \"security services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Builders Merchant\",\n\t    \"aliases\": [\"building supplies\", \"suppliers of aggregates\"]\n\t  },\n\t  {\n\t    \"trade\": \"Service Engineers\",\n\t    \"aliases\": [\"mechanical servicing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Asphalters/Pavers/Engineers\",\n\t    \"aliases\": [\"road maintenance\", \"road surfacing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Flooring and Carpet\",\n\t    \"aliases\": [\"carpet fitters\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure Industry\",\n\t    \"aliases\": [\"nightclub\", \"pub\", \"leisure centre\", \"gym\"]\n\t  },\n\t  {\n\t    \"trade\": \"Social Religious or Charitable\",\n\t    \"aliases\": [\"charity\"]\n\t  },\n\t  {\n\t    \"trade\": \"Business Services\",\n\t    \"aliases\": [\"document storage\", \"administration\", \"consultants\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Services\",\n\t    \"aliases\": [\"food delivery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Retail\",\n\t    \"aliases\": [\"restaurant\", \"pub\", \"takeaway\"]\n\t  },\n\t  {\n\t    \"trade\": \"Estate Agent\",\n\t    \"aliases\": [\"lettings agents\", \"estate agent\"]\n\t  },\n\t  {\n\t    \"trade\": \"Furniture Sale and Manufacture\",\n\t    \"aliases\": [\"furniture retail\", \"furniture showroom\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure (Hotel, clubs & pubs)\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Shop Fitting\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"NHS Trust\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Import/Export\",\n\t    \"aliases\": [\"Import\", \"Export\"]\n\t  },\n\t  {\n\t    \"trade\": \"Catering\",\n\t    \"aliases\": [\"licensed catering\", \"unlicensed catering\", \"outside catering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Residential Care\",\n\t    \"aliases\": [\"care homes\", \"retirement homes\"]\n\t  },\n\t  {\n\t    \"trade\": \"Manufacturing Timber/Furniture\",\n\t    \"aliases\": [\"joinery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Printers and publishers\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Demolition Contractors\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Housing Association\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Timber Merchant\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Private Ambulance Service\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers Overseas\",\n\t    \"aliases\": [\"haulage contractors - overseas\", \"hauliers overseas\"]\n\t  },\n\t  {\n\t    \"trade\": \"Textiles & Clothing\",\n\t    \"aliases\": [\"clothing manufacturing\", \"textile manufacturing\", \"clothing retail\"]\n\t  },\n\t  {\n\t    \"trade\": \"Automotive Industry\",\n\t    \"aliases\": [\"vehicle manufacturing\", \"vehicle repairs\", \"vehicle parts manufacturing\", \"vehicle parts wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Pharmaceutical\",\n\t    \"aliases\": [\"pharmacy\", \"medical laboratories\"]\n\t  },\n\t  {\n\t    \"trade\": \"Farmer\",\n\t    \"aliases\": [\"dairy farmers\", \"arable farmers\", \"livestock farmers\"]\n\t  }\n\t]\n\t\n\treturn data\n\t\n\t",
            "docstring": null,
            "function_code": "\tdata = [\n\t  {\n\t    \"trade\": \"General Manufacturing\",\n\t    \"aliases\": [\"steel\", \"metal\", \"samples\", \"manufacturing of glass\", \"manufacturing of furniture\", \"manufacturing\", \"general manufacturing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Fleet - Unclassified\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Contractor\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Builders\",\n\t    \"aliases\": [\"Building Contractors\", \"house builders\", \"builders\"]\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers UK only\",\n\t    \"aliases\": [\"haulage contractors - uk only\", \"hauliers uk only\"]\n\t  },\n\t  {\n\t    \"trade\": \"Electrician\",\n\t    \"aliases\": [\"Electrical Contractors\", \"electrical installation\", \"electrical testing\", \"electrician\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plumbing & Heating Engineer\",\n\t    \"aliases\": [\"plumbing & heating contractors\", \"gas servicing\", \"boiler maintenance\", \"plumber\", \"heating engineer\"]\n\t  },\n\t  {\n\t    \"trade\": \"Engineering\",\n\t    \"aliases\": [ \"engineers\", \"mechanical engineers\", \"engineering\", \"electrical engineers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Civil engineering\",\n\t    \"aliases\": [\"civil engineering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Scaffolding Contractors\",\n\t    \"aliases\": [\"scaffolders\", \"scaffolding contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Wholesale (non food and drink)\",\n\t    \"aliases\": [\"wholesalers of electrical components\", \"building materials\", \"wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Cleaning Contractors\",\n\t    \"aliases\": [\"domestic cleaners\", \"office cleaners\", \"cleaning contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plant Hire\",\n\t    \"aliases\": [\"plant hire operator\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Wholesale\",\n\t    \"aliases\": [\"cash & carry\", \"beer & wine wholesalers\", \"food wholesalers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Retail (non food and drink)\",\n\t    \"aliases\": [\"department store\", \"shopping centre\"]\n\t  },\n\t  {\n\t    \"trade\": \"Property Owners\",\n\t    \"aliases\": [\"landlords\", \"commercial property owners\"]\n\t  },\n\t  {\n\t    \"trade\": \"Telecommunications & IT\",\n\t    \"aliases\": [\"telecommunications installation\", \"it installation\", \"it services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Double Glazing\",\n\t    \"aliases\": [\"double glazing manufacture\", \"double glazing installation\"]\n\t  },\n\t  {\n\t    \"trade\": \"Landscape Gardener\",\n\t    \"aliases\": [\"gardening\", \"landscape gardener\"]\n\t  },\n\t  {\n\t    \"trade\": \"Other Prof/Sci/Tech\",\n\t    \"aliases\": [\"laboratory\"]\n\t  },\n\t  {\n\t    \"trade\": \"Removal Contractor\",\n\t    \"aliases\": [\"removals and storage\"]\n\t  },\n\t  {\n\t    \"trade\": \"Security and investigation\",\n\t    \"aliases\": [\"private detectives\", \"security guarding\", \"security services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Builders Merchant\",\n\t    \"aliases\": [\"building supplies\", \"suppliers of aggregates\"]\n\t  },\n\t  {\n\t    \"trade\": \"Service Engineers\",\n\t    \"aliases\": [\"mechanical servicing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Asphalters/Pavers/Engineers\",\n\t    \"aliases\": [\"road maintenance\", \"road surfacing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Flooring and Carpet\",\n\t    \"aliases\": [\"carpet fitters\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure Industry\",\n\t    \"aliases\": [\"nightclub\", \"pub\", \"leisure centre\", \"gym\"]\n\t  },\n\t  {\n\t    \"trade\": \"Social Religious or Charitable\",\n\t    \"aliases\": [\"charity\"]\n\t  },\n\t  {\n\t    \"trade\": \"Business Services\",\n\t    \"aliases\": [\"document storage\", \"administration\", \"consultants\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Services\",\n\t    \"aliases\": [\"food delivery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Retail\",\n\t    \"aliases\": [\"restaurant\", \"pub\", \"takeaway\"]\n\t  },\n\t  {\n\t    \"trade\": \"Estate Agent\",\n\t    \"aliases\": [\"lettings agents\", \"estate agent\"]\n\t  },\n\t  {\n\t    \"trade\": \"Furniture Sale and Manufacture\",\n\t    \"aliases\": [\"furniture retail\", \"furniture showroom\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure (Hotel, clubs & pubs)\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Shop Fitting\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"NHS Trust\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Import/Export\",\n\t    \"aliases\": [\"Import\", \"Export\"]\n\t  },\n\t  {\n\t    \"trade\": \"Catering\",\n\t    \"aliases\": [\"licensed catering\", \"unlicensed catering\", \"outside catering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Residential Care\",\n\t    \"aliases\": [\"care homes\", \"retirement homes\"]\n\t  },\n\t  {\n\t    \"trade\": \"Manufacturing Timber/Furniture\",\n\t    \"aliases\": [\"joinery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Printers and publishers\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Demolition Contractors\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Housing Association\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Timber Merchant\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Private Ambulance Service\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers Overseas\",\n\t    \"aliases\": [\"haulage contractors - overseas\", \"hauliers overseas\"]\n\t  },\n\t  {\n\t    \"trade\": \"Textiles & Clothing\",\n\t    \"aliases\": [\"clothing manufacturing\", \"textile manufacturing\", \"clothing retail\"]\n\t  },\n\t  {\n\t    \"trade\": \"Automotive Industry\",\n\t    \"aliases\": [\"vehicle manufacturing\", \"vehicle repairs\", \"vehicle parts manufacturing\", \"vehicle parts wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Pharmaceutical\",\n\t    \"aliases\": [\"pharmacy\", \"medical laboratories\"]\n\t  },\n\t  {\n\t    \"trade\": \"Farmer\",\n\t    \"aliases\": [\"dairy farmers\", \"arable farmers\", \"livestock farmers\"]\n\t  }\n\t]\n\t\n\treturn data\n\t\n\t",
            "id": 17143,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/15ec1038-63d9-49ac-9c28-8d5ce6e0648b_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/15ec1038-63d9-49ac-9c28-8d5ce6e0648b.json",
            "type": "REFINER"
        },
        "17144": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "AXA Trade Description@0"
                },
                {
                    "data_type": "FIELD",
                    "name": "Business_Description",
                    "value": "Business Description"
                }
            ],
            "code": "\n\ndef clean(previous_line, Business_Description, context = {}, keys = {}, **kwargs):\n\tif Business_Description:\n\t  if Business_Description == \"\":\n\t      return \"\"\n\t  else:\n\t      return previous_line\n\telse:\n\t    return \"\"",
            "docstring": null,
            "function_code": "\tif Business_Description:\n\t  if Business_Description == \"\":\n\t      return \"\"\n\t  else:\n\t      return previous_line\n\telse:\n\t    return \"\"",
            "id": 17144,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/10f626d5-c4bd-4210-a2fa-4aad798e40a1_output.json",
            "name": "clean",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/10f626d5-c4bd-4210-a2fa-4aad798e40a1.json",
            "type": "REFINER"
        },
        "17145": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "AXA Trade Description@1"
                },
                {
                    "data_type": "FIELD",
                    "name": "Trade_Descriptions",
                    "value": "Trade Descriptions"
                }
            ],
            "code": "\n\ndef unnamed_custom_function(previous_line, Trade_Descriptions, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tprint(repr(previous_line))\n\tif previous_line == '\"\"' or previous_line == '' or previous_line == \"\":\n\t    return \"\"\n\t\n\tif previous_line:\n\t  try:\n\t      Trade_Descriptions = eval(Trade_Descriptions)\n\t  except:\n\t      try:\n\t          Trade_Descriptions = json.loads(Trade_Descriptions)\n\t      except:\n\t          # return \"Fleet - Unclassified\"\n\t          return \"Fleet - Unclassified\"\n\t\n\t  Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t    \n\t  if previous_line.lower() in Trade_Descriptions_ls:\n\t      return previous_line\n\t  else:\n\t      # return \"Fleet - Unclassified\"\n\t      return \"Fleet - Unclassified\"\n\telse:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tprint(repr(previous_line))\n\tif previous_line == '\"\"' or previous_line == '' or previous_line == \"\":\n\t    return \"\"\n\t\n\tif previous_line:\n\t  try:\n\t      Trade_Descriptions = eval(Trade_Descriptions)\n\t  except:\n\t      try:\n\t          Trade_Descriptions = json.loads(Trade_Descriptions)\n\t      except:\n\t          # return \"Fleet - Unclassified\"\n\t          return \"Fleet - Unclassified\"\n\t\n\t  Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t    \n\t  if previous_line.lower() in Trade_Descriptions_ls:\n\t      return previous_line\n\t  else:\n\t      # return \"Fleet - Unclassified\"\n\t      return \"Fleet - Unclassified\"\n\telse:\n\t  return \"\"",
            "id": 17145,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/4d3181b6-37c6-44e4-af34-826d3ed13323_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/4d3181b6-37c6-44e4-af34-826d3ed13323.json",
            "type": "REFINER"
        },
        "17146": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Incepts On@1"
                },
                {
                    "data_type": "FIELD",
                    "name": "Expiry_Date",
                    "value": "Expiry Date"
                }
            ],
            "code": "\n\ndef form_inception_date_with_expiry_date(previous_line, Expiry_Date, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime, timedelta\n\t\n\tdef get_new_date(incepts_on, expiry_date):\n\t    # If both are empty, return ''\n\t    if not incepts_on and not expiry_date:\n\t        return ''\n\t    \n\t    # Convert to datetime objects if not empty\n\t    expiry_dt = datetime.strptime(expiry_date, \"%d/%m/%Y\") if expiry_date else ''\n\t    incepts_dt = datetime.strptime(incepts_on, \"%d/%m/%Y\") if incepts_on else ''\n\t    \n\t    # 1) If Incepts On is empty, return Expiry Date + 1 day\n\t    if not incepts_on:\n\t        return (expiry_dt + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n\t    \n\t    # 2) If Incepts On and Expiry Date are the same, return Incepts On\n\t    if expiry_date and incepts_dt == expiry_dt:\n\t        return incepts_on\n\t    \n\t    # 3) If Incepts On is non-empty, return Incepts On\n\t    return incepts_on\n\t\n\ttry:\n\t    # previous_line = str(previous_line)\n\t    # Expiry_Date = str(Expiry_Date)\n\t    \n\t    previous_line = previous_line.replace('\"\"', '')\n\t    Expiry_Date = Expiry_Date.replace('\"\"', '')\n\t\n\t    result = get_new_date(previous_line, Expiry_Date)\n\t    \n\t    return result\n\texcept Exception as e:\n\t    print(e)\n\t    return \"\"",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime, timedelta\n\t\n\tdef get_new_date(incepts_on, expiry_date):\n\t    # If both are empty, return ''\n\t    if not incepts_on and not expiry_date:\n\t        return ''\n\t    \n\t    # Convert to datetime objects if not empty\n\t    expiry_dt = datetime.strptime(expiry_date, \"%d/%m/%Y\") if expiry_date else ''\n\t    incepts_dt = datetime.strptime(incepts_on, \"%d/%m/%Y\") if incepts_on else ''\n\t    \n\t    # 1) If Incepts On is empty, return Expiry Date + 1 day\n\t    if not incepts_on:\n\t        return (expiry_dt + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n\t    \n\t    # 2) If Incepts On and Expiry Date are the same, return Incepts On\n\t    if expiry_date and incepts_dt == expiry_dt:\n\t        return incepts_on\n\t    \n\t    # 3) If Incepts On is non-empty, return Incepts On\n\t    return incepts_on\n\t\n\ttry:\n\t    # previous_line = str(previous_line)\n\t    # Expiry_Date = str(Expiry_Date)\n\t    \n\t    previous_line = previous_line.replace('\"\"', '')\n\t    Expiry_Date = Expiry_Date.replace('\"\"', '')\n\t\n\t    result = get_new_date(previous_line, Expiry_Date)\n\t    \n\t    return result\n\texcept Exception as e:\n\t    print(e)\n\t    return \"\"",
            "id": 17146,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/ad781ae0-f5fb-419c-b067-f09faf073bb3_output.json",
            "name": "form_inception_date_with_expiry_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/ad781ae0-f5fb-419c-b067-f09faf073bb3.json",
            "type": "REFINER"
        },
        "17147": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Main Cover Type@1"
                }
            ],
            "code": "\n\ndef check_cover_type(previous_line, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
            "id": 17147,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/aa061384-f9de-4b55-88b9-d708db07954e_output.json",
            "name": "check_cover_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/aa061384-f9de-4b55-88b9-d708db07954e.json",
            "type": "REFINER"
        },
        "17148": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Incepts_On",
                    "value": "Incepts On"
                }
            ],
            "code": "\n\ndef get_inception_date(Incepts_On, context = {}, keys = {}, **kwargs):\n\tif Incepts_On :\n\t  return Incepts_On\n\telse:\n\t  return ''",
            "docstring": null,
            "function_code": "\tif Incepts_On :\n\t  return Incepts_On\n\telse:\n\t  return ''",
            "id": 17148,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8056d654-03d5-4c26-a9b5-abfd4a9c3ef5_output.json",
            "name": "get_inception_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8056d654-03d5-4c26-a9b5-abfd4a9c3ef5.json",
            "type": "REFINER"
        },
        "17149": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "CCE_Data",
                    "value": "CCE Data"
                },
                {
                    "data_type": "FIELD",
                    "name": "CCE_Table",
                    "value": "CCE Table"
                }
            ],
            "code": "\n\ndef get_years_count_from_CCE(CCE_Data, CCE_Table, context = {}, keys = {}, **kwargs):\n\t\n\timport pandas as pd\n\timport json\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t    try:\n\t        CCE_Table = eval(CCE_Table)\n\t    except:\n\t        CCE_Table = json.loads(CCE_Table)\n\t    try:\n\t        CCE_Table = convert_to_dataframe(CCE_Table)\n\t        CCE_Table_rows = CCE_Table.shape[0]\n\t        return CCE_Table_rows\n\t    except:\n\t        return 0\n\texcept:\n\t    return \"Exception in code\"",
            "docstring": null,
            "function_code": "\t\n\timport pandas as pd\n\timport json\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t    try:\n\t        CCE_Table = eval(CCE_Table)\n\t    except:\n\t        CCE_Table = json.loads(CCE_Table)\n\t    try:\n\t        CCE_Table = convert_to_dataframe(CCE_Table)\n\t        CCE_Table_rows = CCE_Table.shape[0]\n\t        return CCE_Table_rows\n\t    except:\n\t        return 0\n\texcept:\n\t    return \"Exception in code\"",
            "id": 17149,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/aca56f8b-8e63-4c8d-b852-fdb50c801233_output.json",
            "name": "get_years_count_from_CCE",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/aca56f8b-8e63-4c8d-b852-fdb50c801233.json",
            "type": "REFINER"
        },
        "17150": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "CCE_Data",
                    "value": "CCE Data"
                },
                {
                    "data_type": "FIELD",
                    "name": "Policy_Dates",
                    "value": "Policy Dates"
                }
            ],
            "code": "\n\ndef get_period_list(CCE_Data, Policy_Dates, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t    try:\n\t        Policy_Dates = eval(Policy_Dates)    \n\t    except:\n\t        Policy_Dates = json.loads(Policy_Dates)   \n\t\n\t    Policy_Dates = convert_to_df(Policy_Dates)\n\t    Policy_Dates = Policy_Dates.astype(str)\n\t    print(Policy_Dates.columns)\n\t    CCE_Policy_start_dates_List = Policy_Dates[['Policy Period Start Date']].values.tolist()\n\t    filtered_vals = [each_val for each_val in CCE_Policy_start_dates_List if each_val not in [\"N/A\",\"\",None,\"null\"]]\n\t    if len(filtered_vals)>=1:\n\t      return [[\"Policy Period Start Date\"]] + CCE_Policy_start_dates_List\n\t    else:\n\t      CCE_Period_List = Policy_Dates[['Period']].values.tolist()\n\t      return [\"Policy Period Start Date\"] + [CCE_Period_List]\n\texcept Exception as e:\n\t    print(e)\n\t    return \"\"",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t    try:\n\t        Policy_Dates = eval(Policy_Dates)    \n\t    except:\n\t        Policy_Dates = json.loads(Policy_Dates)   \n\t\n\t    Policy_Dates = convert_to_df(Policy_Dates)\n\t    Policy_Dates = Policy_Dates.astype(str)\n\t    print(Policy_Dates.columns)\n\t    CCE_Policy_start_dates_List = Policy_Dates[['Policy Period Start Date']].values.tolist()\n\t    filtered_vals = [each_val for each_val in CCE_Policy_start_dates_List if each_val not in [\"N/A\",\"\",None,\"null\"]]\n\t    if len(filtered_vals)>=1:\n\t      return [[\"Policy Period Start Date\"]] + CCE_Policy_start_dates_List\n\t    else:\n\t      CCE_Period_List = Policy_Dates[['Period']].values.tolist()\n\t      return [\"Policy Period Start Date\"] + [CCE_Period_List]\n\texcept Exception as e:\n\t    print(e)\n\t    return \"\"",
            "id": 17150,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/63bffd27-74c9-4931-9633-a97487e35650_output.json",
            "name": "get_period_list",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/63bffd27-74c9-4931-9633-a97487e35650.json",
            "type": "REFINER"
        },
        "17151": {
            "args": [],
            "code": "\n\ndef get_transaction_type(context = {}, keys = {}, **kwargs):\n\treturn \"New Business\"\n\t",
            "docstring": null,
            "function_code": "\treturn \"New Business\"\n\t",
            "id": 17151,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/be13889c-b9d8-4b1e-911a-71aeaa1ab396_output.json",
            "name": "get_transaction_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/be13889c-b9d8-4b1e-911a-71aeaa1ab396.json",
            "type": "REFINER"
        },
        "17152": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_List",
                    "value": "Excess List"
                },
                {
                    "data_type": "FIELD",
                    "name": "CCE_Data",
                    "value": "CCE Data"
                },
                {
                    "data_type": "FIELD",
                    "name": "Year",
                    "value": "Year"
                },
                {
                    "data_type": "FIELD",
                    "name": "cover_mapping",
                    "value": "cover mapping"
                },
                {
                    "data_type": "FIELD",
                    "name": "Policy_Dates",
                    "value": "Policy Dates"
                }
            ],
            "code": "\n\ndef form_CCE_table(Excess_List, CCE_Data, Year, cover_mapping, Policy_Dates, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            # print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\ttry:\n\t  try:\n\t      Excess_List = json.loads(Excess_List)\n\t      Excess_List = convert_to_dataframe(Excess_List)\n\t      if \"Period\" in Excess_List.columns:\n\t          Excess_List[\"Period\"] = Excess_List[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t  try:\n\t      CCE_Data = json.loads(CCE_Data)\n\t      CCE_Data = convert_to_dataframe(CCE_Data)\n\t      if \"Period\" in CCE_Data.columns:\n\t          CCE_Data[\"Period\"] = CCE_Data[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t  try:\n\t      Policy_Dates = json.loads(Policy_Dates)\n\t      Policy_Dates = convert_to_dataframe(Policy_Dates)\n\t      if \"Period\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Period\"] = Policy_Dates[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      if \"Policy Period Start Date\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Policy Period Start Date\"] = Policy_Dates[\"Policy Period Start Date\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  # try:\n\t  #     Insurer_Data = json.loads(Insurer_Data)\n\t  #     Insurer_Data = convert_to_dataframe(Insurer_Data)\n\t  # except Exception as e:\n\t  #     print(e)\n\t\n\t  try:\n\t      Year = json.loads(Year)\n\t      Year = convert_to_dataframe(Year)\n\t  except Exception as e:\n\t      print(e)\n\t\n\t\n\t  ## Merging the data to form final CCE table\n\t  try:\n\t      CCE_Data['Period'] = CCE_Data['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      Excess_List['Period'] = Excess_List['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      merged_df1 = pd.merge(CCE_Data, Excess_List,  on='Period', how='inner')\n\t  except:\n\t      merged_df1 = CCE_Data.copy()\n\t      \n\t      print(\"Exception in merging claims info\")\n\t  \n\t  # print(merged_df1)\n\t  \n\t  # merged_df = [merged_df1.columns.tolist()] + merged_df1.values.tolist()\n\t  # return merged_df\n\t  \n\t  try:\n\t      merged_df2 = pd.merge(merged_df1, Policy_Dates,  on='Period', how='inner')\n\t  except Exception as e:\n\t      merged_df2 = merged_df1.copy()\n\t      print(\"Exception in merging policy info\")\n\t\n\t  \n\t\n\t  # merged_df = [merged_df2.columns.tolist()] + merged_df2.values.tolist()\n\t  # return merged_df\n\t  \n\t  # print(merged_df2['Period'])\n\t  # print(Insurer_Data['Period'])\n\t  \n\t  # try:\n\t  #     merged_df3 = pd.merge(merged_df2, Insurer_Data,  on='Period', how='inner')\n\t  # except:\n\t  #     merged_df3 = merged_df2.copy()\n\t  #     print(\"Exception in merging all details\")\n\t\n\t  \n\t  try:    \n\t      merged_df = pd.merge(merged_df2, Year,  on='Policy Period Start Date', how='inner')\n\t  except:\n\t      merged_df = merged_df2.copy()\n\t      print(\"Exception in merging Year info\")\n\t\n\t  try:\n\t      merged_df = merged_df.sort_values(\n\t      by=\"Policy Period Start Date\",  # replace with your actual column name\n\t      ascending=False\n\t      )\n\t  except:\n\t      merged_df = merged_df.copy()\n\t\n\t  try:\n\t      # Assign rank (1 = most recent, increasing for older dates)\n\t      merged_df[\"Year\"] = merged_df[\"Policy Period Start Date\"].rank(\n\t          method=\"dense\", ascending=False\n\t      ).astype(int)\n\t  except:\n\t      merged_df = merged_df.copy()\n\t\n\t\n\t  try:\n\t      merged_df = merged_df.sort_values(\n\t      by=\"Policy Period Start Date\",  # replace with your actual column name\n\t      ascending=True\n\t      )\n\t  except:\n\t      merged_df = merged_df.copy()\n\t\n\t  \n\t\n\t\n\t  \n\t  try:\n\t    cover_mapping = json.loads(cover_mapping)\n\t    merged_df['Cover on Policy - Mapped'] = merged_df['Cover On Policy'].map(cover_mapping)\n\t  except Exception as e:\n\t    print(e)\n\t    merged_df = merged_df.copy()\n\t\n\t  rename_dict = {\n\t      'AD Excess': 'Excess: AD',\n\t      'Theft Excess': 'Excess: Theft',\n\t      'Fire Excess': 'Excess: Fire',\n\t      'WS Excess': 'Excess: WS',\n\t      'Cover On Policy': 'Cover on Policy',\n\t      'Vehicle Years Earned': 'Vehicle Years Earned',\n\t      'Claim Count': 'Claim Count: All',\n\t      # 'no of claims windscreen': 'Claim Count: WS',\n\t      'Incurreds Paid AD WS': 'Incurreds - Paid: AD&WS',\n\t      'Incurreds Paid FT': 'Incurreds - Paid: FT',\n\t      'Incurreds Paid TP': 'Incurreds - Paid: TP',\n\t      'Incurreds Outstanding AD WS': 'Incurreds - Outstanding: AD&WS',\n\t      'Incurreds Outstanding FT': 'Incurreds - Outstanding: FT',\n\t      'Incurreds Outstanding TP': 'Incurreds - Outstanding: TP',\n\t      'Total Incurred': 'Total Incurred Paid +  Outstanding',\n\t      'Policy Period Start Date': 'Policy Year Start Date',\n\t      'Policy Period End Date': 'Policy Year End Date'\n\t  }\n\t\n\t  safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t  merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t\n\t  try:\n\t      merged_df['Claim Count: WS'] = ''\n\t  except Exception as e:\n\t    print(e)\n\t  \n\t\n\t  columns_order = [ 'Year', 'Policy Year Start Date', \\\n\t                'Policy Year End Date', \n\t                'Insurer', \n\t                'Excess: AD',\\\n\t                'Excess: Fire', \\\n\t                'Excess: Theft', \\\n\t                'Excess: WS', \\\n\t                'Cover on Policy', \\\n\t                'Cover on Policy - Mapped',\n\t                 'Vehicle Years Earned', \n\t                'Claim Count: All', \\\n\t                'Claim Count: WS', \\\n\t                'Incurreds - Paid: AD&WS', \\\n\t                'Incurreds - Paid: FT', \\\n\t                'Incurreds - Paid: TP', \n\t                'Incurreds - Outstanding: AD&WS', \\\n\t                'Incurreds - Outstanding: FT' ,\n\t                'Incurreds - Outstanding: TP', \\\n\t                'Total Incurred Paid +  Outstanding']\n\t\n\t  # existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t               \n\t  # print(merged_df.columns)\n\t  merged_df.replace('N/A', '', inplace=True)\n\t\n\t  merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t  merged_df = merged_df.dropna(how='all')\n\t  \n\t  merged_df = merged_df.fillna('')\n\t\n\t  # merged_df = merged_df[existing_columns]\n\t\n\t  for col in columns_order:\n\t    if col not in merged_df.columns:\n\t      merged_df[col] = '' \n\t\n\t\n\t  if len(merged_df.columns.tolist()) > 0:\n\t      merged_df = merged_df[columns_order]\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept Exception as e:\n\t  print(e)\n\t  return [columns_order]",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        \n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assumed as header\n\t            # print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\ttry:\n\t  try:\n\t      Excess_List = json.loads(Excess_List)\n\t      Excess_List = convert_to_dataframe(Excess_List)\n\t      if \"Period\" in Excess_List.columns:\n\t          Excess_List[\"Period\"] = Excess_List[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t  try:\n\t      CCE_Data = json.loads(CCE_Data)\n\t      CCE_Data = convert_to_dataframe(CCE_Data)\n\t      if \"Period\" in CCE_Data.columns:\n\t          CCE_Data[\"Period\"] = CCE_Data[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t  try:\n\t      Policy_Dates = json.loads(Policy_Dates)\n\t      Policy_Dates = convert_to_dataframe(Policy_Dates)\n\t      if \"Period\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Period\"] = Policy_Dates[\"Period\"].str.replace(\" \", \"\", regex=False)\n\t      if \"Policy Period Start Date\" in Policy_Dates.columns:\n\t          Policy_Dates[\"Policy Period Start Date\"] = Policy_Dates[\"Policy Period Start Date\"].str.replace(\" \", \"\", regex=False)\n\t      \n\t  except Exception as e:\n\t      print(e)\n\t\n\t  # try:\n\t  #     Insurer_Data = json.loads(Insurer_Data)\n\t  #     Insurer_Data = convert_to_dataframe(Insurer_Data)\n\t  # except Exception as e:\n\t  #     print(e)\n\t\n\t  try:\n\t      Year = json.loads(Year)\n\t      Year = convert_to_dataframe(Year)\n\t  except Exception as e:\n\t      print(e)\n\t\n\t\n\t  ## Merging the data to form final CCE table\n\t  try:\n\t      CCE_Data['Period'] = CCE_Data['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      Excess_List['Period'] = Excess_List['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t      merged_df1 = pd.merge(CCE_Data, Excess_List,  on='Period', how='inner')\n\t  except:\n\t      merged_df1 = CCE_Data.copy()\n\t      \n\t      print(\"Exception in merging claims info\")\n\t  \n\t  # print(merged_df1)\n\t  \n\t  # merged_df = [merged_df1.columns.tolist()] + merged_df1.values.tolist()\n\t  # return merged_df\n\t  \n\t  try:\n\t      merged_df2 = pd.merge(merged_df1, Policy_Dates,  on='Period', how='inner')\n\t  except Exception as e:\n\t      merged_df2 = merged_df1.copy()\n\t      print(\"Exception in merging policy info\")\n\t\n\t  \n\t\n\t  # merged_df = [merged_df2.columns.tolist()] + merged_df2.values.tolist()\n\t  # return merged_df\n\t  \n\t  # print(merged_df2['Period'])\n\t  # print(Insurer_Data['Period'])\n\t  \n\t  # try:\n\t  #     merged_df3 = pd.merge(merged_df2, Insurer_Data,  on='Period', how='inner')\n\t  # except:\n\t  #     merged_df3 = merged_df2.copy()\n\t  #     print(\"Exception in merging all details\")\n\t\n\t  \n\t  try:    \n\t      merged_df = pd.merge(merged_df2, Year,  on='Policy Period Start Date', how='inner')\n\t  except:\n\t      merged_df = merged_df2.copy()\n\t      print(\"Exception in merging Year info\")\n\t\n\t  try:\n\t      merged_df = merged_df.sort_values(\n\t      by=\"Policy Period Start Date\",  # replace with your actual column name\n\t      ascending=False\n\t      )\n\t  except:\n\t      merged_df = merged_df.copy()\n\t\n\t  try:\n\t      # Assign rank (1 = most recent, increasing for older dates)\n\t      merged_df[\"Year\"] = merged_df[\"Policy Period Start Date\"].rank(\n\t          method=\"dense\", ascending=False\n\t      ).astype(int)\n\t  except:\n\t      merged_df = merged_df.copy()\n\t\n\t\n\t  try:\n\t      merged_df = merged_df.sort_values(\n\t      by=\"Policy Period Start Date\",  # replace with your actual column name\n\t      ascending=True\n\t      )\n\t  except:\n\t      merged_df = merged_df.copy()\n\t\n\t  \n\t\n\t\n\t  \n\t  try:\n\t    cover_mapping = json.loads(cover_mapping)\n\t    merged_df['Cover on Policy - Mapped'] = merged_df['Cover On Policy'].map(cover_mapping)\n\t  except Exception as e:\n\t    print(e)\n\t    merged_df = merged_df.copy()\n\t\n\t  rename_dict = {\n\t      'AD Excess': 'Excess: AD',\n\t      'Theft Excess': 'Excess: Theft',\n\t      'Fire Excess': 'Excess: Fire',\n\t      'WS Excess': 'Excess: WS',\n\t      'Cover On Policy': 'Cover on Policy',\n\t      'Vehicle Years Earned': 'Vehicle Years Earned',\n\t      'Claim Count': 'Claim Count: All',\n\t      # 'no of claims windscreen': 'Claim Count: WS',\n\t      'Incurreds Paid AD WS': 'Incurreds - Paid: AD&WS',\n\t      'Incurreds Paid FT': 'Incurreds - Paid: FT',\n\t      'Incurreds Paid TP': 'Incurreds - Paid: TP',\n\t      'Incurreds Outstanding AD WS': 'Incurreds - Outstanding: AD&WS',\n\t      'Incurreds Outstanding FT': 'Incurreds - Outstanding: FT',\n\t      'Incurreds Outstanding TP': 'Incurreds - Outstanding: TP',\n\t      'Total Incurred': 'Total Incurred Paid +  Outstanding',\n\t      'Policy Period Start Date': 'Policy Year Start Date',\n\t      'Policy Period End Date': 'Policy Year End Date'\n\t  }\n\t\n\t  safe_rename_dict = {k: v for k, v in rename_dict.items() if k in merged_df.columns}\n\t\n\t  merged_df.rename(columns=safe_rename_dict, inplace=True)\n\t\n\t  try:\n\t      merged_df['Claim Count: WS'] = ''\n\t  except Exception as e:\n\t    print(e)\n\t  \n\t\n\t  columns_order = [ 'Year', 'Policy Year Start Date', \\\n\t                'Policy Year End Date', \n\t                'Insurer', \n\t                'Excess: AD',\\\n\t                'Excess: Fire', \\\n\t                'Excess: Theft', \\\n\t                'Excess: WS', \\\n\t                'Cover on Policy', \\\n\t                'Cover on Policy - Mapped',\n\t                 'Vehicle Years Earned', \n\t                'Claim Count: All', \\\n\t                'Claim Count: WS', \\\n\t                'Incurreds - Paid: AD&WS', \\\n\t                'Incurreds - Paid: FT', \\\n\t                'Incurreds - Paid: TP', \n\t                'Incurreds - Outstanding: AD&WS', \\\n\t                'Incurreds - Outstanding: FT' ,\n\t                'Incurreds - Outstanding: TP', \\\n\t                'Total Incurred Paid +  Outstanding']\n\t\n\t  # existing_columns = [col for col in columns_order if col in merged_df.columns]           \n\t               \n\t  # print(merged_df.columns)\n\t  merged_df.replace('N/A', '', inplace=True)\n\t\n\t  merged_df = merged_df.replace({None: np.nan, \"\": np.nan})\n\t  merged_df = merged_df.dropna(how='all')\n\t  \n\t  merged_df = merged_df.fillna('')\n\t\n\t  # merged_df = merged_df[existing_columns]\n\t\n\t  for col in columns_order:\n\t    if col not in merged_df.columns:\n\t      merged_df[col] = '' \n\t\n\t\n\t  if len(merged_df.columns.tolist()) > 0:\n\t      merged_df = merged_df[columns_order]\n\t      merged_df = [merged_df.columns.tolist()] + merged_df.values.tolist()\n\t      return merged_df\n\t  else:\n\t      return [columns_order]\n\texcept Exception as e:\n\t  print(e)\n\t  return [columns_order]",
            "id": 17152,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/50ff88af-5163-44be-9002-073fd2b3b72a_output.json",
            "name": "form_CCE_table",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/50ff88af-5163-44be-9002-073fd2b3b72a.json",
            "type": "REFINER"
        },
        "17153": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Incepts_On",
                    "value": "Incepts On"
                }
            ],
            "code": "\n\ndef unnamed_custom_function(Incepts_On, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime, timedelta\n\t\n\tdef add_one_year_minus_one_day(date_str):\n\t    # Parse the input date (DD/MM/YYYY)\n\t    input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t    try:\n\t        # Try to add one year directly\n\t        one_year_later = input_date.replace(year=input_date.year + 1)\n\t    except ValueError:\n\t        # Handle Feb 29 (leap year issue) and other invalid dates\n\t        temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t        one_year_later = temp_date\n\t    # Subtract one day\n\t    final_date = one_year_later - timedelta(days=1)\n\t    # Return formatted date as DD/MM/YYYY with leading zeros\n\t    return final_date.strftime(\"%d/%m/%Y\")\n\t\n\ttry:\n\t    expiry_date = add_one_year_minus_one_day(Incepts_On)\n\t    return expiry_date\n\texcept Exception as e:\n\t    return \"\"\n\t\n\t",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime, timedelta\n\t\n\tdef add_one_year_minus_one_day(date_str):\n\t    # Parse the input date (DD/MM/YYYY)\n\t    input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t    try:\n\t        # Try to add one year directly\n\t        one_year_later = input_date.replace(year=input_date.year + 1)\n\t    except ValueError:\n\t        # Handle Feb 29 (leap year issue) and other invalid dates\n\t        temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t        one_year_later = temp_date\n\t    # Subtract one day\n\t    final_date = one_year_later - timedelta(days=1)\n\t    # Return formatted date as DD/MM/YYYY with leading zeros\n\t    return final_date.strftime(\"%d/%m/%Y\")\n\t\n\ttry:\n\t    expiry_date = add_one_year_minus_one_day(Incepts_On)\n\t    return expiry_date\n\texcept Exception as e:\n\t    return \"\"\n\t\n\t",
            "id": 17153,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/42633144-f74d-4061-a4ac-76ecf81c5629_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/42633144-f74d-4061-a4ac-76ecf81c5629.json",
            "type": "REFINER"
        },
        "17154": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Expires_On",
                    "value": "Expires On"
                }
            ],
            "code": "\n\ndef get_expiry_date(Expires_On, context = {}, keys = {}, **kwargs):\n\treturn Expires_On",
            "docstring": null,
            "function_code": "\treturn Expires_On",
            "id": 17154,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/0127a7f4-acb8-4d46-97a0-4afb22dc69a0_output.json",
            "name": "get_expiry_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/0127a7f4-acb8-4d46-97a0-4afb22dc69a0.json",
            "type": "REFINER"
        },
        "17155": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Year@0"
                }
            ],
            "code": "\n\ndef refine_result(previous_line, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tcolumns_order = [\"Policy Period Start Date\", \"Year\"]\n\t\n\tprint(previous_line)\n\ttry:\n\t    if previous_line in [\"N/A\", \"[]\", \"\",  None, \"null\"]:\n\t      return [columns_order]\n\t    try:\n\t        df = eval(previous_line)    \n\t    except:\n\t        df = json.loads(previous_line)   \n\t\n\t    df = convert_to_df(df)\n\t    df = df.astype(str)\n\t    if len(df.columns.tolist()) > 0:\n\t        return [df.columns.tolist()] + df.values.tolist()\n\t    else:\n\t        return [columns_order]\n\texcept Exception as e:\n\t    print(e)\n\t    return [columns_order]",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tcolumns_order = [\"Policy Period Start Date\", \"Year\"]\n\t\n\tprint(previous_line)\n\ttry:\n\t    if previous_line in [\"N/A\", \"[]\", \"\",  None, \"null\"]:\n\t      return [columns_order]\n\t    try:\n\t        df = eval(previous_line)    \n\t    except:\n\t        df = json.loads(previous_line)   \n\t\n\t    df = convert_to_df(df)\n\t    df = df.astype(str)\n\t    if len(df.columns.tolist()) > 0:\n\t        return [df.columns.tolist()] + df.values.tolist()\n\t    else:\n\t        return [columns_order]\n\texcept Exception as e:\n\t    print(e)\n\t    return [columns_order]",
            "id": 17155,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a3e52338-cea1-4b4c-9853-cdefea183906_output.json",
            "name": "refine_result",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a3e52338-cea1-4b4c-9853-cdefea183906.json",
            "type": "REFINER"
        },
        "17156": {
            "args": [],
            "code": "\n\ndef get_business_category(context = {}, keys = {}, **kwargs):\n\treturn \"Mid Market\"",
            "docstring": null,
            "function_code": "\treturn \"Mid Market\"",
            "id": 17156,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/715dda7c-ee41-4c83-941c-fa9f5fa793c5_output.json",
            "name": "get_business_category",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/715dda7c-ee41-4c83-941c-fa9f5fa793c5.json",
            "type": "REFINER"
        },
        "17157": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_List",
                    "value": "Excess List"
                }
            ],
            "code": "\n\ndef get_unique_values_from_cover_vehicle(Excess_List, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\t\n\ttable = json.loads(Excess_List)\n\t\n\tif Excess_List in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tExcess_List = convert_to_dataframe(table)\n\tunique_vals = Excess_List['Cover On Policy'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\t\n\ttable = json.loads(Excess_List)\n\t\n\tif Excess_List in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tExcess_List = convert_to_dataframe(table)\n\tunique_vals = Excess_List['Cover On Policy'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
            "id": 17157,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/27f2ae79-e1fb-4d95-a90d-04339f0b3387_output.json",
            "name": "get_unique_values_from_cover_vehicle",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/27f2ae79-e1fb-4d95-a90d-04339f0b3387.json",
            "type": "REFINER"
        },
        "17158": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Main Cover Type Mapped@0"
                }
            ],
            "code": "\n\ndef check_cover_type_mapped(previous_line, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
            "id": 17158,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2100afba-56b7-4c65-a46e-e4bcd687e927_output.json",
            "name": "check_cover_type_mapped",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2100afba-56b7-4c65-a46e-e4bcd687e927.json",
            "type": "REFINER"
        },
        "17159": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line2(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t# import json\n\t# key = \"Line_2\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
            "docstring": null,
            "function_code": "\t# import json\n\t# key = \"Line_2\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
            "id": 17159,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/41a0e4ed-4974-4ec4-8a9a-f86f0bb2d734_output.json",
            "name": "get_address_line2",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/41a0e4ed-4974-4ec4-8a9a-f86f0bb2d734.json",
            "type": "REFINER"
        },
        "17160": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line3(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t# import json\n\t# key = \"Line_3\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Address Line 3\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\t# import json\n\t# key = \"Line_3\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Address Line 3\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "id": 17160,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/e0e9b876-e813-4087-8d5c-4e941390f8d7_output.json",
            "name": "get_address_line3",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/e0e9b876-e813-4087-8d5c-4e941390f8d7.json",
            "type": "REFINER"
        },
        "17161": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t# import json\n\t# key = \"Country\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Country\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\t# import json\n\t# key = \"Country\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Country\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "id": 17161,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/d4fb0f9f-a986-4360-9de0-9cca634d5097_output.json",
            "name": "get_address_line",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/d4fb0f9f-a986-4360-9de0-9cca634d5097.json",
            "type": "REFINER"
        },
        "17162": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t# import json\n\t# key = \"County_State\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"State\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\t# import json\n\t# key = \"County_State\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"State\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "id": 17162,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/fafe17be-052c-480d-815b-bb75b9fa4236_output.json",
            "name": "get_address_line",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/fafe17be-052c-480d-815b-bb75b9fa4236.json",
            "type": "REFINER"
        },
        "17163": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t# import json\n\t# key = \"Town_City\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"City\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\t# import json\n\t# key = \"Town_City\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"City\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "id": 17163,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/6051f69f-2762-4548-88b2-568ad2dc398a_output.json",
            "name": "get_address_line",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/6051f69f-2762-4548-88b2-568ad2dc398a.json",
            "type": "REFINER"
        },
        "17164": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t# import json\n\t# key = \"County_State\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Postcode\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\t# import json\n\t# key = \"County_State\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\ttry:\n\t  val = get_value(Party_Address_Details, \"Postcode\")\n\t  return val\n\texcept:\n\t  return \"\"",
            "id": 17164,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/6e789b2b-ea5c-4a9a-ae6b-75a66af78baf_output.json",
            "name": "get_address_line",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/6e789b2b-ea5c-4a9a-ae6b-75a66af78baf.json",
            "type": "REFINER"
        },
        "17165": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line1(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t# import json\n\t# key = \"Line_1\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
            "docstring": null,
            "function_code": "\t# import json\n\t# key = \"Line_1\"\n\t# def extract_first_json_object(text):\n\t#   start = text.find('{')\n\t#   if start == -1:\n\t#     return None\n\t#   brace_count = 0\n\t#   end = None\n\t#   for i in range(start, len(text)):\n\t#     if text[i] == '{':\n\t#         brace_count += 1\n\t#     elif text[i] == '}':\n\t#         brace_count -= 1\n\t#         if brace_count == 0:\n\t#           end = i\n\t#           break\n\t#   if end is None:\n\t#       return None\n\t#   return text[start:end+1]\n\t\n\t\n\t# if not Party_Address_Intermediate_Object:\n\t#   return None\n\t# json_part = extract_first_json_object(Party_Address_Intermediate_Object)\n\t# if json_part is None:\n\t#   return None\n\t# try:\n\t#   data = json.loads(json_part)\n\t#   value = data.get(key)\n\t#   return str(value) if value is not None else None\n\t# except json.JSONDecodeError:\n\t#   return None\n\t\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
            "id": 17165,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/3ad831bf-7d05-4fe5-bf20-80ab485cf26a_output.json",
            "name": "get_address_line1",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/3ad831bf-7d05-4fe5-bf20-80ab485cf26a.json",
            "type": "REFINER"
        },
        "17166": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Policy Dates@0"
                }
            ],
            "code": "\n\ndef business_logic_for_policy_inception_date(previous_line, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\tfrom datetime import datetime, timedelta\n\t\n\t# Custom function\n\tdef add_one_year_minus_one_day(date_str):\n\t    try:\n\t        input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t        try:\n\t            one_year_later = input_date.replace(year=input_date.year + 1)\n\t        except ValueError:\n\t            temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t            one_year_later = temp_date\n\t        final_date = one_year_later - timedelta(days=1)\n\t        return final_date.strftime(\"%d/%m/%Y\")\n\t    except Exception as e:\n\t        print(e)\n\t        return ''\n\t\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\t\n\ttry:\n\t    previous_line = eval(previous_line)\n\t    dates_df = convert_to_df(previous_line)\n\texcept:\n\t  try:\n\t      previous_line = json.loads(previous_line)\n\t      dates_df = convert_to_df(previous_line)\n\t  except:\n\t      return previous_line\n\t\n\ttry:\n\t\n\t    # Iterate row-wise\n\t    for index, row in dates_df.iterrows():\n\t        start_date = row['Policy Period Start Date']\n\t        \n\t        # Only update Policy Period End Date if Used Inception Date is 'Yes'\n\t        if str(row.get('Used Inception Date', '')).strip().lower() == 'yes':\n\t            dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t            print(\"Applied for \", row[\"Period\"])\n\t\n\t    # Clean up Period column after the loop\n\t    dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    # Convert to list format for output\n\t    final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    return final_df\n\t\n\t    # print(dates_df.columns)\n\t    # Iterate row-wise\n\t    # for index, row in dates_df.iterrows():\n\t    #     start_date = row['Policy Period Start Date']\n\t    #     dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t    #     dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    #     final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    # return final_df\n\t\n\texcept:\n\t    return \"\"",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\tfrom datetime import datetime, timedelta\n\t\n\t# Custom function\n\tdef add_one_year_minus_one_day(date_str):\n\t    try:\n\t        input_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n\t        try:\n\t            one_year_later = input_date.replace(year=input_date.year + 1)\n\t        except ValueError:\n\t            temp_date = input_date + (datetime(input_date.year + 1, 3, 1) - datetime(input_date.year, 3, 1))\n\t            one_year_later = temp_date\n\t        final_date = one_year_later - timedelta(days=1)\n\t        return final_date.strftime(\"%d/%m/%Y\")\n\t    except Exception as e:\n\t        print(e)\n\t        return ''\n\t\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\t\n\ttry:\n\t    previous_line = eval(previous_line)\n\t    dates_df = convert_to_df(previous_line)\n\texcept:\n\t  try:\n\t      previous_line = json.loads(previous_line)\n\t      dates_df = convert_to_df(previous_line)\n\t  except:\n\t      return previous_line\n\t\n\ttry:\n\t\n\t    # Iterate row-wise\n\t    for index, row in dates_df.iterrows():\n\t        start_date = row['Policy Period Start Date']\n\t        \n\t        # Only update Policy Period End Date if Used Inception Date is 'Yes'\n\t        if str(row.get('Used Inception Date', '')).strip().lower() == 'yes':\n\t            dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t            print(\"Applied for \", row[\"Period\"])\n\t\n\t    # Clean up Period column after the loop\n\t    dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    # Convert to list format for output\n\t    final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    return final_df\n\t\n\t    # print(dates_df.columns)\n\t    # Iterate row-wise\n\t    # for index, row in dates_df.iterrows():\n\t    #     start_date = row['Policy Period Start Date']\n\t    #     dates_df.at[index, 'Policy Period End Date'] = add_one_year_minus_one_day(start_date)\n\t    #     dates_df['Period'] = dates_df['Period'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n\t\n\t    #     final_df = [dates_df.columns.tolist()] + dates_df.values.tolist()\n\t    # return final_df\n\t\n\texcept:\n\t    return \"\"",
            "id": 17166,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/ed218f77-4497-4f08-90b9-007c58b5d653_output.json",
            "name": "business_logic_for_policy_inception_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/ed218f77-4497-4f08-90b9-007c58b5d653.json",
            "type": "REFINER"
        },
        "17167": {
            "args": [],
            "code": "\n\ndef unnamed_custom_function(context = {}, keys = {}, **kwargs):\n\tdata = [\n\t  {\n\t    \"trade\": \"General Manufacturing\",\n\t    \"aliases\": [\"steel\", \"metal\", \"samples\", \"manufacturing of glass\", \"manufacturing of furniture\", \"manufacturing\", \"general manufacturing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Fleet - Unclassified\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Contractor\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Builders\",\n\t    \"aliases\": [\"Building Contractors\", \"house builders\", \"builders\"]\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers UK only\",\n\t    \"aliases\": [\"haulage contractors - uk only\", \"hauliers uk only\"]\n\t  },\n\t  {\n\t    \"trade\": \"Electrician\",\n\t    \"aliases\": [\"Electrical Contractors\", \"electrical installation\", \"electrical testing\", \"electrician\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plumbing & Heating Engineer\",\n\t    \"aliases\": [\"plumbing & heating contractors\", \"gas servicing\", \"boiler maintenance\", \"plumber\", \"heating engineer\"]\n\t  },\n\t  {\n\t    \"trade\": \"Engineering\",\n\t    \"aliases\": [ \"engineers\", \"mechanical engineers\", \"engineering\", \"electrical engineers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Civil engineering\",\n\t    \"aliases\": [\"civil engineering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Scaffolding Contractors\",\n\t    \"aliases\": [\"scaffolders\", \"scaffolding contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Wholesale (non food and drink)\",\n\t    \"aliases\": [\"wholesalers of electrical components\", \"building materials\", \"wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Cleaning Contractors\",\n\t    \"aliases\": [\"domestic cleaners\", \"office cleaners\", \"cleaning contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plant Hire\",\n\t    \"aliases\": [\"plant hire operator\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Wholesale\",\n\t    \"aliases\": [\"cash & carry\", \"beer & wine wholesalers\", \"food wholesalers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Retail (non food and drink)\",\n\t    \"aliases\": [\"department store\", \"shopping centre\"]\n\t  },\n\t  {\n\t    \"trade\": \"Property Owners\",\n\t    \"aliases\": [\"landlords\", \"commercial property owners\"]\n\t  },\n\t  {\n\t    \"trade\": \"Telecommunications & IT\",\n\t    \"aliases\": [\"telecommunications installation\", \"it installation\", \"it services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Double Glazing\",\n\t    \"aliases\": [\"double glazing manufacture\", \"double glazing installation\"]\n\t  },\n\t  {\n\t    \"trade\": \"Landscape Gardener\",\n\t    \"aliases\": [\"gardening\", \"landscape gardener\"]\n\t  },\n\t  {\n\t    \"trade\": \"Other Prof/Sci/Tech\",\n\t    \"aliases\": [\"laboratory\"]\n\t  },\n\t  {\n\t    \"trade\": \"Removal Contractor\",\n\t    \"aliases\": [\"removals and storage\"]\n\t  },\n\t  {\n\t    \"trade\": \"Security and investigation\",\n\t    \"aliases\": [\"private detectives\", \"security guarding\", \"security services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Builders Merchant\",\n\t    \"aliases\": [\"building supplies\", \"suppliers of aggregates\"]\n\t  },\n\t  {\n\t    \"trade\": \"Service Engineers\",\n\t    \"aliases\": [\"mechanical servicing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Asphalters/Pavers/Engineers\",\n\t    \"aliases\": [\"road maintenance\", \"road surfacing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Flooring and Carpet\",\n\t    \"aliases\": [\"carpet fitters\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure Industry\",\n\t    \"aliases\": [\"nightclub\", \"pub\", \"leisure centre\", \"gym\"]\n\t  },\n\t  {\n\t    \"trade\": \"Social Religious or Charitable\",\n\t    \"aliases\": [\"charity\"]\n\t  },\n\t  {\n\t    \"trade\": \"Business Services\",\n\t    \"aliases\": [\"document storage\", \"administration\", \"consultants\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Services\",\n\t    \"aliases\": [\"food delivery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Retail\",\n\t    \"aliases\": [\"restaurant\", \"pub\", \"takeaway\"]\n\t  },\n\t  {\n\t    \"trade\": \"Estate Agent\",\n\t    \"aliases\": [\"lettings agents\", \"estate agent\"]\n\t  },\n\t  {\n\t    \"trade\": \"Furniture Sale and Manufacture\",\n\t    \"aliases\": [\"furniture retail\", \"furniture showroom\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure (Hotel, clubs & pubs)\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Shop Fitting\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"NHS Trust\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Import/Export\",\n\t    \"aliases\": [\"Import\", \"Export\"]\n\t  },\n\t  {\n\t    \"trade\": \"Catering\",\n\t    \"aliases\": [\"licensed catering\", \"unlicensed catering\", \"outside catering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Residential Care\",\n\t    \"aliases\": [\"care homes\", \"retirement homes\"]\n\t  },\n\t  {\n\t    \"trade\": \"Manufacturing Timber/Furniture\",\n\t    \"aliases\": [\"joinery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Printers and publishers\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Demolition Contractors\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Housing Association\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Timber Merchant\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Private Ambulance Service\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers Overseas\",\n\t    \"aliases\": [\"haulage contractors - overseas\", \"hauliers overseas\"]\n\t  },\n\t  {\n\t    \"trade\": \"Textiles & Clothing\",\n\t    \"aliases\": [\"clothing manufacturing\", \"textile manufacturing\", \"clothing retail\"]\n\t  },\n\t  {\n\t    \"trade\": \"Automotive Industry\",\n\t    \"aliases\": [\"vehicle manufacturing\", \"vehicle repairs\", \"vehicle parts manufacturing\", \"vehicle parts wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Pharmaceutical\",\n\t    \"aliases\": [\"pharmacy\", \"medical laboratories\"]\n\t  },\n\t  {\n\t    \"trade\": \"Farmer\",\n\t    \"aliases\": [\"dairy farmers\", \"arable farmers\", \"livestock farmers\"]\n\t  }\n\t]\n\t\n\treturn data",
            "docstring": null,
            "function_code": "\tdata = [\n\t  {\n\t    \"trade\": \"General Manufacturing\",\n\t    \"aliases\": [\"steel\", \"metal\", \"samples\", \"manufacturing of glass\", \"manufacturing of furniture\", \"manufacturing\", \"general manufacturing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Fleet - Unclassified\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Contractor\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Builders\",\n\t    \"aliases\": [\"Building Contractors\", \"house builders\", \"builders\"]\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers UK only\",\n\t    \"aliases\": [\"haulage contractors - uk only\", \"hauliers uk only\"]\n\t  },\n\t  {\n\t    \"trade\": \"Electrician\",\n\t    \"aliases\": [\"Electrical Contractors\", \"electrical installation\", \"electrical testing\", \"electrician\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plumbing & Heating Engineer\",\n\t    \"aliases\": [\"plumbing & heating contractors\", \"gas servicing\", \"boiler maintenance\", \"plumber\", \"heating engineer\"]\n\t  },\n\t  {\n\t    \"trade\": \"Engineering\",\n\t    \"aliases\": [ \"engineers\", \"mechanical engineers\", \"engineering\", \"electrical engineers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Civil engineering\",\n\t    \"aliases\": [\"civil engineering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Scaffolding Contractors\",\n\t    \"aliases\": [\"scaffolders\", \"scaffolding contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Wholesale (non food and drink)\",\n\t    \"aliases\": [\"wholesalers of electrical components\", \"building materials\", \"wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Cleaning Contractors\",\n\t    \"aliases\": [\"domestic cleaners\", \"office cleaners\", \"cleaning contractors\"]\n\t  },\n\t  {\n\t    \"trade\": \"Plant Hire\",\n\t    \"aliases\": [\"plant hire operator\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Wholesale\",\n\t    \"aliases\": [\"cash & carry\", \"beer & wine wholesalers\", \"food wholesalers\"]\n\t  },\n\t  {\n\t    \"trade\": \"Retail (non food and drink)\",\n\t    \"aliases\": [\"department store\", \"shopping centre\"]\n\t  },\n\t  {\n\t    \"trade\": \"Property Owners\",\n\t    \"aliases\": [\"landlords\", \"commercial property owners\"]\n\t  },\n\t  {\n\t    \"trade\": \"Telecommunications & IT\",\n\t    \"aliases\": [\"telecommunications installation\", \"it installation\", \"it services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Double Glazing\",\n\t    \"aliases\": [\"double glazing manufacture\", \"double glazing installation\"]\n\t  },\n\t  {\n\t    \"trade\": \"Landscape Gardener\",\n\t    \"aliases\": [\"gardening\", \"landscape gardener\"]\n\t  },\n\t  {\n\t    \"trade\": \"Other Prof/Sci/Tech\",\n\t    \"aliases\": [\"laboratory\"]\n\t  },\n\t  {\n\t    \"trade\": \"Removal Contractor\",\n\t    \"aliases\": [\"removals and storage\"]\n\t  },\n\t  {\n\t    \"trade\": \"Security and investigation\",\n\t    \"aliases\": [\"private detectives\", \"security guarding\", \"security services\"]\n\t  },\n\t  {\n\t    \"trade\": \"Builders Merchant\",\n\t    \"aliases\": [\"building supplies\", \"suppliers of aggregates\"]\n\t  },\n\t  {\n\t    \"trade\": \"Service Engineers\",\n\t    \"aliases\": [\"mechanical servicing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Asphalters/Pavers/Engineers\",\n\t    \"aliases\": [\"road maintenance\", \"road surfacing\"]\n\t  },\n\t  {\n\t    \"trade\": \"Flooring and Carpet\",\n\t    \"aliases\": [\"carpet fitters\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure Industry\",\n\t    \"aliases\": [\"nightclub\", \"pub\", \"leisure centre\", \"gym\"]\n\t  },\n\t  {\n\t    \"trade\": \"Social Religious or Charitable\",\n\t    \"aliases\": [\"charity\"]\n\t  },\n\t  {\n\t    \"trade\": \"Business Services\",\n\t    \"aliases\": [\"document storage\", \"administration\", \"consultants\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Services\",\n\t    \"aliases\": [\"food delivery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Food & Drink Retail\",\n\t    \"aliases\": [\"restaurant\", \"pub\", \"takeaway\"]\n\t  },\n\t  {\n\t    \"trade\": \"Estate Agent\",\n\t    \"aliases\": [\"lettings agents\", \"estate agent\"]\n\t  },\n\t  {\n\t    \"trade\": \"Furniture Sale and Manufacture\",\n\t    \"aliases\": [\"furniture retail\", \"furniture showroom\"]\n\t  },\n\t  {\n\t    \"trade\": \"Leisure (Hotel, clubs & pubs)\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Shop Fitting\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"NHS Trust\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Import/Export\",\n\t    \"aliases\": [\"Import\", \"Export\"]\n\t  },\n\t  {\n\t    \"trade\": \"Catering\",\n\t    \"aliases\": [\"licensed catering\", \"unlicensed catering\", \"outside catering\"]\n\t  },\n\t  {\n\t    \"trade\": \"Residential Care\",\n\t    \"aliases\": [\"care homes\", \"retirement homes\"]\n\t  },\n\t  {\n\t    \"trade\": \"Manufacturing Timber/Furniture\",\n\t    \"aliases\": [\"joinery\"]\n\t  },\n\t  {\n\t    \"trade\": \"Printers and publishers\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Demolition Contractors\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Housing Association\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Timber Merchant\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Private Ambulance Service\",\n\t    \"aliases\": []\n\t  },\n\t  {\n\t    \"trade\": \"Hauliers Overseas\",\n\t    \"aliases\": [\"haulage contractors - overseas\", \"hauliers overseas\"]\n\t  },\n\t  {\n\t    \"trade\": \"Textiles & Clothing\",\n\t    \"aliases\": [\"clothing manufacturing\", \"textile manufacturing\", \"clothing retail\"]\n\t  },\n\t  {\n\t    \"trade\": \"Automotive Industry\",\n\t    \"aliases\": [\"vehicle manufacturing\", \"vehicle repairs\", \"vehicle parts manufacturing\", \"vehicle parts wholesale\"]\n\t  },\n\t  {\n\t    \"trade\": \"Pharmaceutical\",\n\t    \"aliases\": [\"pharmacy\", \"medical laboratories\"]\n\t  },\n\t  {\n\t    \"trade\": \"Farmer\",\n\t    \"aliases\": [\"dairy farmers\", \"arable farmers\", \"livestock farmers\"]\n\t  }\n\t]\n\t\n\treturn data",
            "id": 17167,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/3bd30549-1d9d-42fe-831d-0af802ce1c4c_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/3bd30549-1d9d-42fe-831d-0af802ce1c4c.json",
            "type": "REFINER"
        },
        "17168": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "AXA Trade Description@0"
                },
                {
                    "data_type": "FIELD",
                    "name": "Business_Description",
                    "value": "Business Description"
                }
            ],
            "code": "\n\ndef clean(previous_line, Business_Description, context = {}, keys = {}, **kwargs):\n\tprint(Business_Description)\n\tif Business_Description:\n\t  if Business_Description == \"\":\n\t      return \"\"\n\t  else:\n\t      return previous_line\n\telse:\n\t    return \"\"",
            "docstring": null,
            "function_code": "\tprint(Business_Description)\n\tif Business_Description:\n\t  if Business_Description == \"\":\n\t      return \"\"\n\t  else:\n\t      return previous_line\n\telse:\n\t    return \"\"",
            "id": 17168,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/fb24f475-4b53-4bf5-b775-b792990858e7_output.json",
            "name": "clean",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/fb24f475-4b53-4bf5-b775-b792990858e7.json",
            "type": "REFINER"
        },
        "17169": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "AXA Trade Description@1"
                },
                {
                    "data_type": "FIELD",
                    "name": "Trade_Descriptions",
                    "value": "Trade Descriptions"
                }
            ],
            "code": "\n\ndef unnamed_custom_function(previous_line, Trade_Descriptions, context = {}, keys = {}, **kwargs):\n\timport json\n\t\n\tprint(repr(previous_line))\n\tif previous_line == '\"\"' or previous_line == '' or previous_line == \"\":\n\t    return \"\"\n\t\n\tif previous_line:\n\t\n\t  try:\n\t      Trade_Descriptions = eval(Trade_Descriptions)\n\t  except:\n\t      try:\n\t          Trade_Descriptions = json.loads(Trade_Descriptions)\n\t      except:\n\t          # return \"Fleet - Unclassified\"\n\t          return \"Fleet - Unclassified\"\n\t\n\t  Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t    \n\t  if previous_line.lower() in Trade_Descriptions_ls:\n\t    return previous_line\n\t  else:\n\t    return \"Fleet - Unclassified\"\n\telse:\n\t  return \"\"",
            "docstring": null,
            "function_code": "\timport json\n\t\n\tprint(repr(previous_line))\n\tif previous_line == '\"\"' or previous_line == '' or previous_line == \"\":\n\t    return \"\"\n\t\n\tif previous_line:\n\t\n\t  try:\n\t      Trade_Descriptions = eval(Trade_Descriptions)\n\t  except:\n\t      try:\n\t          Trade_Descriptions = json.loads(Trade_Descriptions)\n\t      except:\n\t          # return \"Fleet - Unclassified\"\n\t          return \"Fleet - Unclassified\"\n\t\n\t  Trade_Descriptions_ls = [item[\"trade\"].lower() for item in Trade_Descriptions]\n\t    \n\t  if previous_line.lower() in Trade_Descriptions_ls:\n\t    return previous_line\n\t  else:\n\t    return \"Fleet - Unclassified\"\n\telse:\n\t  return \"\"",
            "id": 17169,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/5c6a4ec4-43a3-4600-b524-44f11810e67d_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/5c6a4ec4-43a3-4600-b524-44f11810e67d.json",
            "type": "REFINER"
        },
        "17170": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Main Cover Type@0"
                }
            ],
            "code": "\n\ndef check_cover_type(previous_line, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'Comp'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'Comp'",
            "id": 17170,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8d05c1d1-7188-4a00-b2c7-3a47844c309b_output.json",
            "name": "check_cover_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8d05c1d1-7188-4a00-b2c7-3a47844c309b.json",
            "type": "REFINER"
        },
        "17171": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Vehicle Schedule Data@0"
                }
            ],
            "code": "\n\ndef refine_result(previous_line, context = {}, keys = {}, **kwargs):\n\t# import json, re\n\t# import pandas as pd\n\t\n\t# def convert_to_dataframe(data):\n\t#     if isinstance(data, list):\n\t#         if all(isinstance(item, dict) for item in data):\n\t#             # List of dictionaries\n\t#             return pd.DataFrame(data)\n\t#         elif all(isinstance(item, list) for item in data):\n\t#             # List of lists\n\t#             return pd.DataFrame(data[1:] , columns= data[0])\n\t#         else:\n\t#             raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t#     else:\n\t#         raise TypeError(\"Input data is not a list.\")\n\t\n\t# columns = ['Cover Vehicle', 'Vehicle Registration']\n\t\n\t# try:\n\t#     # print(previous_line)\n\t#     if previous_line in ['', None, 'N/A', '[]', [], 'N/A']:\n\t#         return [columns]\n\t\n\t#     df = pd.DataFrame(columns=columns)\n\t\n\t\n\t#     # ```text[]``` or ```json[]```\n\t#     matches = re.findall(r'\\w+\\[([^\\]]*)\\]', previous_line)\n\t#     result = [ [item.strip() for item in match.split(',')] for match in matches ]\n\t#     if result:\n\t#       df = convert_to_dataframe(result)\n\t    \n\t    \n\t#     if df.empty:\n\t#         # | Cover Vehicle | Vehicle Registration | \\n |:----------------|:-----------------------| \\n | FMV | RO11AYY |\n\t#         rows = [line.strip() for line in previous_line.splitlines() if previous_line.strip().startswith('|')]\n\t#         list_of_lists = [\n\t#             [cell.strip() for cell in row.split('|')[1:-1]]\n\t#             for row in rows\n\t#         ]\n\t#         if list_of_lists:\n\t#           # Remove row that contains only dashes and colons (alignment row)\n\t#             cleaned = [\n\t#                 row for row in list_of_lists\n\t#                 if not all(cell.strip().startswith(':') or set(cell.strip()) <= {'-', ':'} for cell in row)\n\t#             ]\n\t#             # print(cleaned)\n\t#             if cleaned:\n\t#                 df = convert_to_dataframe(cleaned)\n\t#                 # print(df)\n\t\n\t\n\t#     if df.empty:\n\t#       try:\n\t#         previous_line = json.loads(previous_line)\n\t#         df = convert_to_dataframe(previous_line)\n\t#       except: \n\t#         previous_line = eval(previous_line)\n\t#         df = convert_to_dataframe(previous_line)\n\t\n\t    \n\t#     df = df.replace(\"N/A\", \"\")\n\t#     df.fillna(\"\", inplace=True)\n\t#     df = df[~df.apply(lambda row: row.isna().all() or (row == '').all(), axis=1)]\n\t    \n\t#     return [df.columns.tolist()] + df.values.tolist()\n\t\n\t# except Exception as e:\n\t#     return [['Cover Vehicle', 'Vehicle Registration']]\n\t\n\timport pandas as pd\n\timport json, re, ast\n\timport traceback\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = [\"Cover Vehicle\", \"Vehicle Registration\"]\n\t\n\t\n\tdef get_df_with_regex_match(previous_line, columns_order):\n\t    parsed, inner_content = None, None\n\t    df = pd.DataFrame(columns=columns_order)\n\t    print(\"hii\")\n\t    try:\n\t        match = re.search(r\"```(?:\\w+)?\\s*([\\s\\S]*?)\\s*```\", previous_line)\n\t        print(\"match\",match)\n\t        if match:\n\t            inner_content = match.group(1).strip()\n\t    \n\t            # Step 2: Try to parse it as JSON\n\t            try:\n\t                parsed = json.loads(inner_content)\n\t                print(parsed)\n\t            except json.JSONDecodeError:\n\t                # If JSON parsing fails, fallback to literal_eval\n\t                try:\n\t                    parsed = ast.literal_eval(inner_content)\n\t                except Exception as e:\n\t                    print(\"Parsing failed:\", e)\n\t                    parsed = None\n\t          \n\t        if parsed:\n\t\n\t            df = convert_to_dataframe(parsed)\n\t            return True, df \n\t        else:\n\t            return False, df \n\t    except Exception as e:\n\t      return False, pd.DataFrame(columns=columns_order)\n\t\n\tdef parse_markdown_format(previous_line, columns_order):\n\t    try:\n\t        lines = [line.strip() for line in previous_line.strip().split('\\n') if line.strip().startswith('|') and '---' not in line]\n\t        list_of_lists = [ [cell.strip() for cell in line.strip('|').split('|')] for line in lines ]\n\t        if list_of_lists:\n\t            df = convert_to_dataframe(list_of_lists)\n\t            return True, df\n\t        else:\n\t            return False, pd.DataFrame(columns=columns_order)\n\t    except:\n\t        return False, pd.DataFrame(columns=columns_order)\n\t\n\ttry:\n\t    \n\t    df = pd.DataFrame(columns=columns_order)\n\t    print(df)\n\t    flg, df = get_df_with_regex_match(previous_line, columns_order)\n\t    \n\t    if not flg:\n\t        flg, df = parse_markdown_format(previous_line, columns_order)\n\t\n\t    if not flg:\n\t      try:\n\t          data = json.loads(previous_line)\n\t          df = convert_to_dataframe(data)\n\t      except Exception as e:\n\t          data = ast.literal_eval(previous_line)\n\t          df = convert_to_dataframe(data)\n\t      print(df)\n\t    \n\t    \n\t    df.fillna(\"\", inplace=True)\n\t    \n\t    if len(df.columns.tolist()) > 0:\n\t        df = df[columns_order]\n\t        df = [df.columns.tolist()] + df.values.tolist()\n\t        return df\n\t    else:\n\t        return [columns_order]\n\t      \n\texcept Exception as e:\n\t    return [columns_order]",
            "docstring": null,
            "function_code": "\t# import json, re\n\t# import pandas as pd\n\t\n\t# def convert_to_dataframe(data):\n\t#     if isinstance(data, list):\n\t#         if all(isinstance(item, dict) for item in data):\n\t#             # List of dictionaries\n\t#             return pd.DataFrame(data)\n\t#         elif all(isinstance(item, list) for item in data):\n\t#             # List of lists\n\t#             return pd.DataFrame(data[1:] , columns= data[0])\n\t#         else:\n\t#             raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t#     else:\n\t#         raise TypeError(\"Input data is not a list.\")\n\t\n\t# columns = ['Cover Vehicle', 'Vehicle Registration']\n\t\n\t# try:\n\t#     # print(previous_line)\n\t#     if previous_line in ['', None, 'N/A', '[]', [], 'N/A']:\n\t#         return [columns]\n\t\n\t#     df = pd.DataFrame(columns=columns)\n\t\n\t\n\t#     # ```text[]``` or ```json[]```\n\t#     matches = re.findall(r'\\w+\\[([^\\]]*)\\]', previous_line)\n\t#     result = [ [item.strip() for item in match.split(',')] for match in matches ]\n\t#     if result:\n\t#       df = convert_to_dataframe(result)\n\t    \n\t    \n\t#     if df.empty:\n\t#         # | Cover Vehicle | Vehicle Registration | \\n |:----------------|:-----------------------| \\n | FMV | RO11AYY |\n\t#         rows = [line.strip() for line in previous_line.splitlines() if previous_line.strip().startswith('|')]\n\t#         list_of_lists = [\n\t#             [cell.strip() for cell in row.split('|')[1:-1]]\n\t#             for row in rows\n\t#         ]\n\t#         if list_of_lists:\n\t#           # Remove row that contains only dashes and colons (alignment row)\n\t#             cleaned = [\n\t#                 row for row in list_of_lists\n\t#                 if not all(cell.strip().startswith(':') or set(cell.strip()) <= {'-', ':'} for cell in row)\n\t#             ]\n\t#             # print(cleaned)\n\t#             if cleaned:\n\t#                 df = convert_to_dataframe(cleaned)\n\t#                 # print(df)\n\t\n\t\n\t#     if df.empty:\n\t#       try:\n\t#         previous_line = json.loads(previous_line)\n\t#         df = convert_to_dataframe(previous_line)\n\t#       except: \n\t#         previous_line = eval(previous_line)\n\t#         df = convert_to_dataframe(previous_line)\n\t\n\t    \n\t#     df = df.replace(\"N/A\", \"\")\n\t#     df.fillna(\"\", inplace=True)\n\t#     df = df[~df.apply(lambda row: row.isna().all() or (row == '').all(), axis=1)]\n\t    \n\t#     return [df.columns.tolist()] + df.values.tolist()\n\t\n\t# except Exception as e:\n\t#     return [['Cover Vehicle', 'Vehicle Registration']]\n\t\n\timport pandas as pd\n\timport json, re, ast\n\timport traceback\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\tcolumns_order = [\"Cover Vehicle\", \"Vehicle Registration\"]\n\t\n\t\n\tdef get_df_with_regex_match(previous_line, columns_order):\n\t    parsed, inner_content = None, None\n\t    df = pd.DataFrame(columns=columns_order)\n\t    print(\"hii\")\n\t    try:\n\t        match = re.search(r\"```(?:\\w+)?\\s*([\\s\\S]*?)\\s*```\", previous_line)\n\t        print(\"match\",match)\n\t        if match:\n\t            inner_content = match.group(1).strip()\n\t    \n\t            # Step 2: Try to parse it as JSON\n\t            try:\n\t                parsed = json.loads(inner_content)\n\t                print(parsed)\n\t            except json.JSONDecodeError:\n\t                # If JSON parsing fails, fallback to literal_eval\n\t                try:\n\t                    parsed = ast.literal_eval(inner_content)\n\t                except Exception as e:\n\t                    print(\"Parsing failed:\", e)\n\t                    parsed = None\n\t          \n\t        if parsed:\n\t\n\t            df = convert_to_dataframe(parsed)\n\t            return True, df \n\t        else:\n\t            return False, df \n\t    except Exception as e:\n\t      return False, pd.DataFrame(columns=columns_order)\n\t\n\tdef parse_markdown_format(previous_line, columns_order):\n\t    try:\n\t        lines = [line.strip() for line in previous_line.strip().split('\\n') if line.strip().startswith('|') and '---' not in line]\n\t        list_of_lists = [ [cell.strip() for cell in line.strip('|').split('|')] for line in lines ]\n\t        if list_of_lists:\n\t            df = convert_to_dataframe(list_of_lists)\n\t            return True, df\n\t        else:\n\t            return False, pd.DataFrame(columns=columns_order)\n\t    except:\n\t        return False, pd.DataFrame(columns=columns_order)\n\t\n\ttry:\n\t    \n\t    df = pd.DataFrame(columns=columns_order)\n\t    print(df)\n\t    flg, df = get_df_with_regex_match(previous_line, columns_order)\n\t    \n\t    if not flg:\n\t        flg, df = parse_markdown_format(previous_line, columns_order)\n\t\n\t    if not flg:\n\t      try:\n\t          data = json.loads(previous_line)\n\t          df = convert_to_dataframe(data)\n\t      except Exception as e:\n\t          data = ast.literal_eval(previous_line)\n\t          df = convert_to_dataframe(data)\n\t      print(df)\n\t    \n\t    \n\t    df.fillna(\"\", inplace=True)\n\t    \n\t    if len(df.columns.tolist()) > 0:\n\t        df = df[columns_order]\n\t        df = [df.columns.tolist()] + df.values.tolist()\n\t        return df\n\t    else:\n\t        return [columns_order]\n\t      \n\texcept Exception as e:\n\t    return [columns_order]",
            "id": 17171,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a76e2ad7-7053-442f-978c-239662cb410a_output.json",
            "name": "refine_result",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/a76e2ad7-7053-442f-978c-239662cb410a.json",
            "type": "REFINER"
        },
        "17172": {
            "args": [],
            "code": "\n\ndef get_business_category(context = {}, keys = {}, **kwargs):\n\treturn \"Mid Market\"",
            "docstring": null,
            "function_code": "\treturn \"Mid Market\"",
            "id": 17172,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/d980b7ef-be62-46e6-a3b6-cd9939876034_output.json",
            "name": "get_business_category",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/d980b7ef-be62-46e6-a3b6-cd9939876034.json",
            "type": "REFINER"
        },
        "17173": {
            "args": [],
            "code": "\n\ndef get_transaction_type(context = {}, keys = {}, **kwargs):\n\treturn \"New Business\"\n\t",
            "docstring": null,
            "function_code": "\treturn \"New Business\"\n\t",
            "id": 17173,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8745e917-12da-4063-9e22-532f1892e38b_output.json",
            "name": "get_transaction_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8745e917-12da-4063-9e22-532f1892e38b.json",
            "type": "REFINER"
        },
        "17174": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Schedule_Data",
                    "value": "Vehicle Schedule Data"
                }
            ],
            "code": "\n\ndef get_unique_cover_values(Vehicle_Schedule_Data, context = {}, keys = {}, **kwargs):\n\t# # Import Python packages\n\timport json\n\timport pandas as pd\n\t# list_of_dct = json.loads(Vehicle_Schedule_Data)\n\t# # print(list_of_dct)\n\t\n\t# try:\n\t#     unique_cover_basis_set = set(item['Cover Vehicle'] for item in list_of_dct)\n\t#     unique_cover_basis_list = list(unique_cover_basis_set)\n\t#     unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t#     return unique_cover_basis_list_without_na\n\t\n\t# except Exception as e:\n\t#     # print(\"in ex\", e)\n\t#     # print(type(list_of_dct))\n\t#     unique_first_col = list(set(row[0] for row in list_of_dct))\n\t#     if 'Cover Vehicle' in unique_first_col:\n\t#         unique_first_col.remove('Cover Vehicle')\n\t\n\t#     return unique_first_col\n\t    \n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tVehicle_Schedule_Data = json.loads(Vehicle_Schedule_Data)\n\tprint(type(Vehicle_Schedule_Data))\n\t\n\tif Vehicle_Schedule_Data in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tVehicle_Schedule_Data = convert_to_dataframe(Vehicle_Schedule_Data)\n\tunique_vals = Vehicle_Schedule_Data['Cover Vehicle'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
            "docstring": null,
            "function_code": "\t# # Import Python packages\n\timport json\n\timport pandas as pd\n\t# list_of_dct = json.loads(Vehicle_Schedule_Data)\n\t# # print(list_of_dct)\n\t\n\t# try:\n\t#     unique_cover_basis_set = set(item['Cover Vehicle'] for item in list_of_dct)\n\t#     unique_cover_basis_list = list(unique_cover_basis_set)\n\t#     unique_cover_basis_list_without_na = [item for item in unique_cover_basis_list if item != 'N/A']\n\t#     return unique_cover_basis_list_without_na\n\t\n\t# except Exception as e:\n\t#     # print(\"in ex\", e)\n\t#     # print(type(list_of_dct))\n\t#     unique_first_col = list(set(row[0] for row in list_of_dct))\n\t#     if 'Cover Vehicle' in unique_first_col:\n\t#         unique_first_col.remove('Cover Vehicle')\n\t\n\t#     return unique_first_col\n\t    \n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\tVehicle_Schedule_Data = json.loads(Vehicle_Schedule_Data)\n\tprint(type(Vehicle_Schedule_Data))\n\t\n\tif Vehicle_Schedule_Data in ['',None,'N/A', '[]', []]:\n\t  return \"None\"\n\t\n\tVehicle_Schedule_Data = convert_to_dataframe(Vehicle_Schedule_Data)\n\tunique_vals = Vehicle_Schedule_Data['Cover Vehicle'].unique().tolist()\n\tunique_vals = [each_val for each_val in unique_vals if each_val not in ['N/A', \"\", None, \"null\"]]\n\tif unique_vals:\n\t    return unique_vals\n\telse:\n\t    return \"None\"",
            "id": 17174,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/39292694-6ef1-4eea-b3ed-b0e6cd2ebeb7_output.json",
            "name": "get_unique_cover_values",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/39292694-6ef1-4eea-b3ed-b0e6cd2ebeb7.json",
            "type": "REFINER"
        },
        "17175": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Schedule_Data",
                    "value": "Vehicle Schedule Data"
                },
                {
                    "data_type": "FIELD",
                    "name": "Unique__Values__Cover__Basis",
                    "value": "Unique_Values_Cover_Basis"
                },
                {
                    "data_type": "FIELD",
                    "name": "Cover_Basis_Mapping",
                    "value": "Cover Basis Mapping"
                }
            ],
            "code": "\n\ndef form_vehicle_schedule_table(Vehicle_Schedule_Data, Unique__Values__Cover__Basis, Cover_Basis_Mapping, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\t# columns_order = [ 'Vehicle Registration', \\\n\t#                 'Cover - Vehicle', \\\n\t#                 'Cover - Vehicle - Mapped']\n\t\n\tcolumns_order = ['Effective From', \\\n\t                  'Effective To', \\\n\t                'Vehicle Registration', \\\n\t                'Cover - Vehicle', \\\n\t                'Cover - Vehicle - Mapped']\n\t\n\t\n\tif Vehicle_Schedule_Data == \"[]\":\n\t    return [columns_order]\n\ttry:\n\t    vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t    vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t\n\t    vehicle_num_and_cover_type_table_df.rename(columns={\n\t        'Cover Vehicle' : 'Cover - Vehicle',\n\t        'Vehicle Registration Number' : 'Vehicle Registration' \n\t      }, inplace=True)\n\t\n\t    try:\n\t        Unique__Values__Cover__Basis = json.loads(Unique__Values__Cover__Basis)\n\t    except:\n\t        Unique__Values__Cover__Basis = eval(Unique__Values__Cover__Basis)\n\t\n\t\n\t    if Unique__Values__Cover__Basis:\n\t        try:\n\t            Cover_Basis_Mapping_dct = json.loads(Cover_Basis_Mapping)\n\t            vehicle_num_and_cover_type_table_df['Cover - Vehicle - Mapped'] = vehicle_num_and_cover_type_table_df['Cover - Vehicle'].replace(Cover_Basis_Mapping_dct)\n\t        except:\n\t            vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.copy()\n\t\n\t    invalid_values = [\"\", \"n/a\", \"null\", \"none\", \"nan\"]\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t   \n\t\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.replace('N/A', '')\n\t    vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t\n\t    for col in columns_order:\n\t      if col not in vehicle_num_and_cover_type_table_df.columns:\n\t        vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t    if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t        vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t        return vehicle_num_and_cover_type_table_df\n\t    else:\n\t        return [columns_order]\n\texcept:\n\t    try:\n\t        vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t        vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t        try:\n\t            vehicle_num_and_cover_type_table_df.rename(columns={\n\t                'Cover Vehicle' : 'Cover - Vehicle',\n\t                'Vehicle Registration Number' : 'Vehicle Registration' \n\t              }, inplace=True)\n\t\n\t        except:\n\t            print()\n\t\n\t        for col in columns_order:\n\t          if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t        invalid_values = [\"\", \"n/a\", \"N/A\", \"null\", \"None\", \"nan\"]\n\t\n\t        # Filter out rows with invalid vehicle registrations\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t\n\t\n\t        vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t\n\t        if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t            vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t            vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t            return vehicle_num_and_cover_type_table_df\n\t        else:\n\t            return [columns_order]\n\t    except:\n\t        return [columns_order]\n\t    return [columns_order]",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame(data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\t\n\t# columns_order = [ 'Vehicle Registration', \\\n\t#                 'Cover - Vehicle', \\\n\t#                 'Cover - Vehicle - Mapped']\n\t\n\tcolumns_order = ['Effective From', \\\n\t                  'Effective To', \\\n\t                'Vehicle Registration', \\\n\t                'Cover - Vehicle', \\\n\t                'Cover - Vehicle - Mapped']\n\t\n\t\n\tif Vehicle_Schedule_Data == \"[]\":\n\t    return [columns_order]\n\ttry:\n\t    vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t    vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t\n\t    vehicle_num_and_cover_type_table_df.rename(columns={\n\t        'Cover Vehicle' : 'Cover - Vehicle',\n\t        'Vehicle Registration Number' : 'Vehicle Registration' \n\t      }, inplace=True)\n\t\n\t    try:\n\t        Unique__Values__Cover__Basis = json.loads(Unique__Values__Cover__Basis)\n\t    except:\n\t        Unique__Values__Cover__Basis = eval(Unique__Values__Cover__Basis)\n\t\n\t\n\t    if Unique__Values__Cover__Basis:\n\t        try:\n\t            Cover_Basis_Mapping_dct = json.loads(Cover_Basis_Mapping)\n\t            vehicle_num_and_cover_type_table_df['Cover - Vehicle - Mapped'] = vehicle_num_and_cover_type_table_df['Cover - Vehicle'].replace(Cover_Basis_Mapping_dct)\n\t        except:\n\t            vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.copy()\n\t\n\t    invalid_values = [\"\", \"n/a\", \"null\", \"none\", \"nan\"]\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t   \n\t\n\t    vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df.replace('N/A', '')\n\t    vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t\n\t    for col in columns_order:\n\t      if col not in vehicle_num_and_cover_type_table_df.columns:\n\t        vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t    if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t        vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t        return vehicle_num_and_cover_type_table_df\n\t    else:\n\t        return [columns_order]\n\texcept:\n\t    try:\n\t        vehicle_num_and_cover_type_table = json.loads(Vehicle_Schedule_Data)\n\t        vehicle_num_and_cover_type_table_df = convert_to_dataframe(vehicle_num_and_cover_type_table)\n\t        try:\n\t            vehicle_num_and_cover_type_table_df.rename(columns={\n\t                'Cover Vehicle' : 'Cover - Vehicle',\n\t                'Vehicle Registration Number' : 'Vehicle Registration' \n\t              }, inplace=True)\n\t\n\t        except:\n\t            print()\n\t\n\t        for col in columns_order:\n\t          if col not in vehicle_num_and_cover_type_table_df.columns:\n\t            vehicle_num_and_cover_type_table_df[col] = ''\n\t\n\t        invalid_values = [\"\", \"n/a\", \"N/A\", \"null\", \"None\", \"nan\"]\n\t\n\t        # Filter out rows with invalid vehicle registrations\n\t        vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[~vehicle_num_and_cover_type_table_df[\"Vehicle Registration\"].astype(str).str.strip().str.lower().isin([str(i).lower() for i in invalid_values])]\n\t\n\t\n\t        vehicle_num_and_cover_type_table_df.fillna('', inplace=True)\n\t\n\t        if len(vehicle_num_and_cover_type_table_df.columns.tolist()) > 0:\n\t            vehicle_num_and_cover_type_table_df = vehicle_num_and_cover_type_table_df[columns_order]\n\t            vehicle_num_and_cover_type_table_df = [vehicle_num_and_cover_type_table_df.columns.tolist()] + vehicle_num_and_cover_type_table_df.values.tolist()\n\t            return vehicle_num_and_cover_type_table_df\n\t        else:\n\t            return [columns_order]\n\t    except:\n\t        return [columns_order]\n\t    return [columns_order]",
            "id": 17175,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2cfe597b-1daa-4453-9755-d2cba1b2aadb_output.json",
            "name": "form_vehicle_schedule_table",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2cfe597b-1daa-4453-9755-d2cba1b2aadb.json",
            "type": "REFINER"
        },
        "17176": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Schedule_Table",
                    "value": "Vehicle Schedule Table"
                }
            ],
            "code": "\n\ndef get_number_of_notifiable_vehicles(Vehicle_Schedule_Table, context = {}, keys = {}, **kwargs):\n\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame( data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\ttry:\n\t  if Vehicle_Schedule_Table == \"[]\" or Vehicle_Schedule_Table == 'n/a':\n\t    return \"n/a\"\n\t  else:\n\t    Vehicle_Schedule_Table = json.loads(Vehicle_Schedule_Table)\n\t    Vehicle_Schedule_Table = convert_to_dataframe(Vehicle_Schedule_Table)\n\t    return len(Vehicle_Schedule_Table['Vehicle Registration'])\n\texcept:\n\t  return \"0\"",
            "docstring": null,
            "function_code": "\timport json\n\timport pandas as pd\n\t\n\tdef convert_to_dataframe(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            return pd.DataFrame( data[1:] , columns= data[0])\n\t        else:\n\t            raise ValueError(\"The list must contain either all dictionaries or all lists.\")\n\t    else:\n\t        raise TypeError(\"Input data is not a list.\")\n\t\n\ttry:\n\t  if Vehicle_Schedule_Table == \"[]\" or Vehicle_Schedule_Table == 'n/a':\n\t    return \"n/a\"\n\t  else:\n\t    Vehicle_Schedule_Table = json.loads(Vehicle_Schedule_Table)\n\t    Vehicle_Schedule_Table = convert_to_dataframe(Vehicle_Schedule_Table)\n\t    return len(Vehicle_Schedule_Table['Vehicle Registration'])\n\texcept:\n\t  return \"0\"",
            "id": 17176,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/b3ccc549-d4ba-4de9-9880-0156d851be03_output.json",
            "name": "get_number_of_notifiable_vehicles",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/b3ccc549-d4ba-4de9-9880-0156d851be03.json",
            "type": "REFINER"
        },
        "17177": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Number_of_Notifiable_Vehicles",
                    "value": "Number of Notifiable Vehicles"
                }
            ],
            "code": "\n\ndef get_offering_type(Number_of_Notifiable_Vehicles, context = {}, keys = {}, **kwargs):\n\t\n\t\n\t# def determine_offering_type(vehicle_count, premium_cleaned=None):\n\t\n\t#     if vehicle_count is None:\n\t#         return \"\"\n\t#     if vehicle_count <= 19:\n\t#         if premium_cleaned is None or premium_cleaned < 10000:\n\t#             return \"Mini Fleet\"\n\t#     elif 20 <= vehicle_count <= 149:\n\t#         if premium_cleaned is None or 10000 <= premium_cleaned < 250000:\n\t#             # print(\"yes\")\n\t#             return \"Vantage Fleet\"\n\t#     elif vehicle_count >= 150:\n\t#         if premium_cleaned is None or premium_cleaned >= 250000:\n\t#             return \"Mid Corp\"\n\t\n\t#     return \"\"\n\t\n\t# try:\n\t  \n\t#     Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\t#     offering_type = determine_offering_type( Number_of_Notifiable_Vehicles )\n\t#     print(offering_type)\n\t#     return offering_type\n\t\n\t# except Exception as e:\n\t#   print(e)\n\t#   return \"\"\n\t\n\t \n\tdef clean_premium(premium_value):\n\t    if premium_value is None:\n\t        return None\n\t    if isinstance(premium_value, (int, float)):\n\t        return float(premium_value)\n\t    try:\n\t        # Remove currency symbols, commas, and whitespace\n\t        # print(premium_value)\n\t        # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t        cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t        print(cleaned)\n\t        return float(cleaned)\n\t    except Exception:\n\t        return None\n\t\n\tdef determine_offering_type(vehicle_count, premium_cleaned=None):    \n\t\n\t    if vehicle_count == 0:\n\t        if premium_cleaned is None or premium_cleaned < 10000:\n\t            return \"Mini Fleet\"\n\t        \n\t        elif 10000 <= premium_cleaned < 250000:\n\t            return \"Vantage Fleet\"\n\t        \n\t        elif premium_cleaned >= 250000:\n\t            return \"Mid Corp\"\n\t\n\t    if vehicle_count <= 19:\n\t            return \"Mini Fleet\"\n\t            \n\t    elif 20 <= vehicle_count <= 149:\n\t            return \"Vantage Fleet\"\n\t        \n\t    elif vehicle_count >= 150:\n\t            return \"Mid Corp\"\n\t\n\t\n\t\n\ttry:\n\t    Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\texcept:\n\t    Number_of_Notifiable_Vehicles = 0\n\t\n\t\n\toffering_type = determine_offering_type( Number_of_Notifiable_Vehicles)\n\treturn offering_type",
            "docstring": null,
            "function_code": "\t\n\t\n\t# def determine_offering_type(vehicle_count, premium_cleaned=None):\n\t\n\t#     if vehicle_count is None:\n\t#         return \"\"\n\t#     if vehicle_count <= 19:\n\t#         if premium_cleaned is None or premium_cleaned < 10000:\n\t#             return \"Mini Fleet\"\n\t#     elif 20 <= vehicle_count <= 149:\n\t#         if premium_cleaned is None or 10000 <= premium_cleaned < 250000:\n\t#             # print(\"yes\")\n\t#             return \"Vantage Fleet\"\n\t#     elif vehicle_count >= 150:\n\t#         if premium_cleaned is None or premium_cleaned >= 250000:\n\t#             return \"Mid Corp\"\n\t\n\t#     return \"\"\n\t\n\t# try:\n\t  \n\t#     Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\t#     offering_type = determine_offering_type( Number_of_Notifiable_Vehicles )\n\t#     print(offering_type)\n\t#     return offering_type\n\t\n\t# except Exception as e:\n\t#   print(e)\n\t#   return \"\"\n\t\n\t \n\tdef clean_premium(premium_value):\n\t    if premium_value is None:\n\t        return None\n\t    if isinstance(premium_value, (int, float)):\n\t        return float(premium_value)\n\t    try:\n\t        # Remove currency symbols, commas, and whitespace\n\t        # print(premium_value)\n\t        # cleaned = re.sub(r\"[\u00a3\u20ac,]\", \"\", premium_value).strip()\n\t        cleaned = premium_value.replace('\u00a3','').replace('\u20ac','').replace(' ', '').replace(',', '').strip()\n\t        print(cleaned)\n\t        return float(cleaned)\n\t    except Exception:\n\t        return None\n\t\n\tdef determine_offering_type(vehicle_count, premium_cleaned=None):    \n\t\n\t    if vehicle_count == 0:\n\t        if premium_cleaned is None or premium_cleaned < 10000:\n\t            return \"Mini Fleet\"\n\t        \n\t        elif 10000 <= premium_cleaned < 250000:\n\t            return \"Vantage Fleet\"\n\t        \n\t        elif premium_cleaned >= 250000:\n\t            return \"Mid Corp\"\n\t\n\t    if vehicle_count <= 19:\n\t            return \"Mini Fleet\"\n\t            \n\t    elif 20 <= vehicle_count <= 149:\n\t            return \"Vantage Fleet\"\n\t        \n\t    elif vehicle_count >= 150:\n\t            return \"Mid Corp\"\n\t\n\t\n\t\n\ttry:\n\t    Number_of_Notifiable_Vehicles = int(Number_of_Notifiable_Vehicles)\n\texcept:\n\t    Number_of_Notifiable_Vehicles = 0\n\t\n\t\n\toffering_type = determine_offering_type( Number_of_Notifiable_Vehicles)\n\treturn offering_type",
            "id": 17177,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8e9350ab-ebb4-41d7-b3c9-67c9aeb5835d_output.json",
            "name": "get_offering_type",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/8e9350ab-ebb4-41d7-b3c9-67c9aeb5835d.json",
            "type": "REFINER"
        },
        "17178": {
            "args": [
                {
                    "data_type": "LINE",
                    "name": "previous_line",
                    "value": "Main Cover Type Mapped@0"
                }
            ],
            "code": "\n\ndef check_cover_type_mapped(previous_line, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the cleaned output\n\t# return previous_line\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tprevious_line = previous_line.strip()\n\tif previous_line:\n\t  if contains_only_quotes(previous_line):\n\t    print('1')\n\t    return 'COMP'\n\t  else :\n\t    print('2')\n\t    return previous_line\n\tprint('3')\n\treturn 'COMP'",
            "id": 17178,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/ee30279d-751b-4e29-b8ec-77b3ec6f0a63_output.json",
            "name": "check_cover_type_mapped",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/ee30279d-751b-4e29-b8ec-77b3ec6f0a63.json",
            "type": "REFINER"
        },
        "17179": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_addressline2(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
            "docstring": null,
            "function_code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 2\")\n\treturn val",
            "id": 17179,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/857bdb23-fdf4-43eb-82ac-2eebf084190d_output.json",
            "name": "get_addressline2",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/857bdb23-fdf4-43eb-82ac-2eebf084190d.json",
            "type": "REFINER"
        },
        "17180": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef unnamed_custom_function(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 3\")\n\treturn val",
            "docstring": null,
            "function_code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 3\")\n\treturn val",
            "id": 17180,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/47dd45e5-af16-4bdf-a556-ff74509c151f_output.json",
            "name": "unnamed_custom_function",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/47dd45e5-af16-4bdf-a556-ff74509c151f.json",
            "type": "REFINER"
        },
        "17181": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_country(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Country\")\n\treturn val",
            "docstring": null,
            "function_code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Country\")\n\treturn val",
            "id": 17181,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/d23b232c-6268-43cd-a5e4-0dd2c11cedf4_output.json",
            "name": "get_country",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/d23b232c-6268-43cd-a5e4-0dd2c11cedf4.json",
            "type": "REFINER"
        },
        "17182": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_state(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"State\")\n\treturn val",
            "docstring": null,
            "function_code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"State\")\n\treturn val",
            "id": 17182,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/badcb033-d0ff-482b-bdb4-6ab791ab8584_output.json",
            "name": "get_state",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/badcb033-d0ff-482b-bdb4-6ab791ab8584.json",
            "type": "REFINER"
        },
        "17183": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_city(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"City\")\n\treturn val",
            "docstring": null,
            "function_code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"City\")\n\treturn val",
            "id": 17183,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/1c6b65e4-fd70-4171-b14d-cc939f048810_output.json",
            "name": "get_city",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/1c6b65e4-fd70-4171-b14d-cc939f048810.json",
            "type": "REFINER"
        },
        "17184": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_postcode(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Postcode\")\n\treturn val",
            "docstring": null,
            "function_code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Postcode\")\n\treturn val",
            "id": 17184,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/855cbdd5-e0c9-416f-adbe-9f777482085f_output.json",
            "name": "get_postcode",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/855cbdd5-e0c9-416f-adbe-9f777482085f.json",
            "type": "REFINER"
        },
        "17185": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Details",
                    "value": "Party Address Details"
                }
            ],
            "code": "\n\ndef get_address_line1(Party_Address_Details, context = {}, keys = {}, **kwargs):\n\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
            "docstring": null,
            "function_code": "\t\n\timport json\n\t\n\tdef get_value(data, key):\n\t    if isinstance(data, dict):\n\t        return data.get(key, \"\")\n\t    return \"\"\n\t\n\ttry:\n\t  Party_Address_Details = eval(Party_Address_Details)\n\t  # print(type(previous_line))\n\texcept:\n\t  Party_Address_Details = json.loads(Party_Address_Details)\n\t  # print(type(previous_line))\n\t\n\t\n\tval = get_value(Party_Address_Details, \"Address Line 1\")\n\treturn val",
            "id": 17185,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2238345c-5492-4faf-ba17-601ae8959748_output.json",
            "name": "get_address_line1",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/2238345c-5492-4faf-ba17-601ae8959748.json",
            "type": "REFINER"
        },
        "17186": {
            "args": [],
            "code": "\n\ndef is_haulage_fact_finder_received(context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the desired output\n\treturn \"Yes\"",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\t# Return the desired output\n\treturn \"Yes\"",
            "id": 17186,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/c681f554-770c-4c46-aecd-cc57f7ab63ba_output.json",
            "name": "is_haulage_fact_finder_received",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/REFINER/c681f554-770c-4c46-aecd-cc57f7ab63ba.json",
            "type": "REFINER"
        },
        "17187": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "CCE_Table",
                    "value": "CCE Table"
                },
                {
                    "data_type": "FIELD",
                    "name": "Incepts_On",
                    "value": "Incepts On"
                }
            ],
            "code": "\n\ndef compare_inception_date_with_policy_period_start_date(CCE_Table, Incepts_On, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t\tCCE_Table = json.loads(CCE_Table)\n\texcept:\n\t\ttry:\n\t\t\t\tCCE_Table = eval(CCE_Table)\n\t\texcept:\n\t\t\t\treturn None\n\t\n\ttry:\n\t    CCE_Table = convert_to_df(CCE_Table)\n\t\n\t    # Convert the column to datetime\n\t    CCE_Table['Policy Year Start Date'] = pd.to_datetime(CCE_Table['Policy Year Start Date'], format='%d/%m/%Y')\n\t\n\t    incepts_on_date = pd.to_datetime(Incepts_On, format='%d/%m/%Y')\n\t\n\t    # Compare day and month of inception date with policy period start date\n\t    all_match = CCE_Table['Policy Year Start Date'].apply(\n\t        lambda x: x.day == incepts_on_date.day and x.month == incepts_on_date.month\n\t    ).all()\n\t\n\t    # Final result\n\t    result = None if all_match else \"Inception date doesn't matches with Policy Period Start Dates\"\n\t\n\t    return result\n\t\n\texcept:\n\t    return None",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            print(\"yes\")\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu  med as header\n\t            print(\"yes\")\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t\tCCE_Table = json.loads(CCE_Table)\n\texcept:\n\t\ttry:\n\t\t\t\tCCE_Table = eval(CCE_Table)\n\t\texcept:\n\t\t\t\treturn None\n\t\n\ttry:\n\t    CCE_Table = convert_to_df(CCE_Table)\n\t\n\t    # Convert the column to datetime\n\t    CCE_Table['Policy Year Start Date'] = pd.to_datetime(CCE_Table['Policy Year Start Date'], format='%d/%m/%Y')\n\t\n\t    incepts_on_date = pd.to_datetime(Incepts_On, format='%d/%m/%Y')\n\t\n\t    # Compare day and month of inception date with policy period start date\n\t    all_match = CCE_Table['Policy Year Start Date'].apply(\n\t        lambda x: x.day == incepts_on_date.day and x.month == incepts_on_date.month\n\t    ).all()\n\t\n\t    # Final result\n\t    result = None if all_match else \"Inception date doesn't matches with Policy Period Start Dates\"\n\t\n\t    return result\n\t\n\texcept:\n\t    return None",
            "id": 17187,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/79a525b0-e2ad-493b-a8dd-b388cd1c42ab_output.json",
            "name": "compare_inception_date_with_policy_period_start_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/79a525b0-e2ad-493b-a8dd-b388cd1c42ab.json",
            "type": "VALIDATIONS"
        },
        "17188": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Expires_On",
                    "value": "Expires On"
                }
            ],
            "code": "\n\ndef validate_expires_on(Expires_On, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Expires_On:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Expires_On = str(Expires_On).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\n\t\tif contains_only_quotes(Expires_On):\n\t\t\treturn None\n\t\tif not isinstance(Expires_On,str):\n\t\t\treturn \"Expires On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Expires_On):\n\t\t\treturn \"Expires On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Expires_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Expires_On:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Expires_On = str(Expires_On).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\n\t\tif contains_only_quotes(Expires_On):\n\t\t\treturn None\n\t\tif not isinstance(Expires_On,str):\n\t\t\treturn \"Expires On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Expires_On):\n\t\t\treturn \"Expires On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Expires_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17188,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/6b358df5-6ac5-4e46-831d-8e4bc35a1b09_output.json",
            "name": "validate_expires_on",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/6b358df5-6ac5-4e46-831d-8e4bc35a1b09.json",
            "type": "VALIDATIONS"
        },
        "17189": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_Accident_Damage",
                    "value": "Excess Type Accident Damage"
                }
            ],
            "code": "\n\ndef validate_excess_type_accident_damage(Excess_Type_Accident_Damage, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Accident_Damage:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Accident_Damage = str(Excess_Type_Accident_Damage).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Accident_Damage)\n\t  except Exception:\n\t    return \"Invalid Excess Type Accident Damage\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Accident_Damage:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Accident_Damage = str(Excess_Type_Accident_Damage).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Accident_Damage)\n\t  except Exception:\n\t    return \"Invalid Excess Type Accident Damage\"\n\telse:\n\t  return None\n\t",
            "id": 17189,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/b5351b80-7739-46a7-88ca-a73b497d84f3_output.json",
            "name": "validate_excess_type_accident_damage",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/b5351b80-7739-46a7-88ca-a73b497d84f3.json",
            "type": "VALIDATIONS"
        },
        "17190": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_Fire",
                    "value": "Excess Type Fire"
                }
            ],
            "code": "\n\ndef validate_excess_type_fire(Excess_Type_Fire, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Fire:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Fire = str(Excess_Type_Fire).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Fire)\n\t  except Exception:\n\t    return \"Invalid Excess Type Fire Price\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Fire:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Fire = str(Excess_Type_Fire).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Fire)\n\t  except Exception:\n\t    return \"Invalid Excess Type Fire Price\"\n\telse:\n\t  return None\n\t",
            "id": 17190,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/7019a71c-0bc5-4732-8cfd-aac2ea86599f_output.json",
            "name": "validate_excess_type_fire",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/7019a71c-0bc5-4732-8cfd-aac2ea86599f.json",
            "type": "VALIDATIONS"
        },
        "17191": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_Theft",
                    "value": "Excess Type Theft"
                }
            ],
            "code": "\n\ndef validate_excess_type_theft(Excess_Type_Theft, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Theft:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Theft = str(Excess_Type_Theft).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Theft)\n\t  except Exception:\n\t    return \"Invalid Excess Type Theft Price\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Theft:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Theft = str(Excess_Type_Theft).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Theft)\n\t  except Exception:\n\t    return \"Invalid Excess Type Theft Price\"\n\telse:\n\t  return None\n\t",
            "id": 17191,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/a3ec32ef-814c-42c0-b06c-d7a8139c4e97_output.json",
            "name": "validate_excess_type_theft",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/a3ec32ef-814c-42c0-b06c-d7a8139c4e97.json",
            "type": "VALIDATIONS"
        },
        "17192": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_WS",
                    "value": "Excess Type WS"
                }
            ],
            "code": "\n\ndef validate_excess_type_ws(Excess_Type_WS, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_WS:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_WS = str(Excess_Type_WS).strip()\n\t    clean_and_validate_price_loose(Excess_Type_WS)\n\t  except Exception:\n\t    return \"Invalid Excess Type WS Price\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_WS:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_WS = str(Excess_Type_WS).strip()\n\t    clean_and_validate_price_loose(Excess_Type_WS)\n\t  except Exception:\n\t    return \"Invalid Excess Type WS Price\"\n\telse:\n\t  return None\n\t",
            "id": 17192,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/bf1135b3-6c3e-4692-b537-e748c7edfaa0_output.json",
            "name": "validate_excess_type_ws",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/bf1135b3-6c3e-4692-b537-e748c7edfaa0.json",
            "type": "VALIDATIONS"
        },
        "17193": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Number_of_Notifiable_Vehicles",
                    "value": "Number of Notifiable Vehicles"
                }
            ],
            "code": "\n\ndef validate_number_of_notifiable_vehicles(Number_of_Notifiable_Vehicles, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\tif Number_of_Notifiable_Vehicles:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t\tNumber_of_Notifiable_Vehicles = str(Number_of_Notifiable_Vehicles).strip()\n\t\texcept:\n\t\t\treturn None\n\t\ttry:\n\t\t\tn = int (Number_of_Notifiable_Vehicles)\n\t\t\tif n < 0:\n\t\t\t\treturn \"Number of Notifiable Vehicles must be a non-negative interger.\"\n\t\t\treturn None\n\t\texcept(ValueError, TypeError):\n\t\t\treturn \"Number of Notifiable Vehicles must be a number.\"\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\tif Number_of_Notifiable_Vehicles:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t\tNumber_of_Notifiable_Vehicles = str(Number_of_Notifiable_Vehicles).strip()\n\t\texcept:\n\t\t\treturn None\n\t\ttry:\n\t\t\tn = int (Number_of_Notifiable_Vehicles)\n\t\t\tif n < 0:\n\t\t\t\treturn \"Number of Notifiable Vehicles must be a non-negative interger.\"\n\t\t\treturn None\n\t\texcept(ValueError, TypeError):\n\t\t\treturn \"Number of Notifiable Vehicles must be a number.\"\n\telse:\n\t\treturn None",
            "id": 17193,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/36071fa0-8075-4775-847e-a3f0c0d7f9b0_output.json",
            "name": "validate_number_of_notifiable_vehicles",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/36071fa0-8075-4775-847e-a3f0c0d7f9b0.json",
            "type": "VALIDATIONS"
        },
        "17194": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Postcode",
                    "value": "Party Address Postcode"
                }
            ],
            "code": "\n\ndef validate_party_address_postcode(Party_Address_Postcode, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
            "id": 17194,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/dba207cc-6ef4-4ac6-a6c6-b800a5a0b4fb_output.json",
            "name": "validate_party_address_postcode",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/dba207cc-6ef4-4ac6-a6c6-b800a5a0b4fb.json",
            "type": "VALIDATIONS"
        },
        "17195": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Target_Price",
                    "value": "Target Price"
                }
            ],
            "code": "\n\ndef validate_target_price(Target_Price, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_target_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Target_Price:\n\t\t# Return None to indicate validation passed\n\t  try:\n\t      Target_Price = str(Target_Price).strip()\n\t  except:\n\t      None\n\t  try:\n\t      clean_and_validate_target_price_loose(Target_Price)\n\t  except Exception:\n\t      print(traceback.format_exc())\n\t      return \"Invalid Target Price\"\n\telse:\n\t      return None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_target_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Target_Price:\n\t\t# Return None to indicate validation passed\n\t  try:\n\t      Target_Price = str(Target_Price).strip()\n\t  except:\n\t      None\n\t  try:\n\t      clean_and_validate_target_price_loose(Target_Price)\n\t  except Exception:\n\t      print(traceback.format_exc())\n\t      return \"Invalid Target Price\"\n\telse:\n\t      return None",
            "id": 17195,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/40c2169d-2fe6-4c78-812f-d8225207b753_output.json",
            "name": "validate_target_price",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/40c2169d-2fe6-4c78-812f-d8225207b753.json",
            "type": "VALIDATIONS"
        },
        "17196": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Agency_Enquiry_Reference",
                    "value": "Agency Enquiry Reference"
                }
            ],
            "code": "\n\ndef validate_agency_enquiry_reference(Agency_Enquiry_Reference, context = {}, keys = {}, **kwargs):\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Agency_Enquiry_Reference:\n\t\t# Return None to indicate validation passed\n\t\tif not isinstance(Agency_Enquiry_Reference, str):\n\t\t\treturn \"Agency Enquiry Reference must be a string\"\n\t\tAgency_Enquiry_Reference = Agency_Enquiry_Reference.strip()\n\t\tif not Agency_Enquiry_Reference:\n\t\t\treturn \"Agency Enquiry Reference cannot be empty or whitespace.\"\n\t\tif contains_only_quotes(Agency_Enquiry_Reference):\n\t\t\t\treturn None\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Agency_Enquiry_Reference:\n\t\t# Return None to indicate validation passed\n\t\tif not isinstance(Agency_Enquiry_Reference, str):\n\t\t\treturn \"Agency Enquiry Reference must be a string\"\n\t\tAgency_Enquiry_Reference = Agency_Enquiry_Reference.strip()\n\t\tif not Agency_Enquiry_Reference:\n\t\t\treturn \"Agency Enquiry Reference cannot be empty or whitespace.\"\n\t\tif contains_only_quotes(Agency_Enquiry_Reference):\n\t\t\t\treturn None\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17196,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/2d248f76-3f6b-4f6e-a6d8-139cbbae5b50_output.json",
            "name": "validate_agency_enquiry_reference",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/2d248f76-3f6b-4f6e-a6d8-139cbbae5b50.json",
            "type": "VALIDATIONS"
        },
        "17197": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Broker_Deadline",
                    "value": "Broker Deadline"
                }
            ],
            "code": "\n\ndef validate_broker_deadline(Broker_Deadline, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Broker_Deadline:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Broker_Deadline = str(Broker_Deadline).strip()\n\t\texcept:\n\t\t\treturn None\n\t\n\t\tif contains_only_quotes(Broker_Deadline):\n\t\t\treturn None\n\t\tif not isinstance(Broker_Deadline,str):\n\t\t\treturn \"Broker Deadline must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Broker_Deadline):\n\t\t\treturn \"Broker Deadline must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Broker_Deadline,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Broker_Deadline:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Broker_Deadline = str(Broker_Deadline).strip()\n\t\texcept:\n\t\t\treturn None\n\t\n\t\tif contains_only_quotes(Broker_Deadline):\n\t\t\treturn None\n\t\tif not isinstance(Broker_Deadline,str):\n\t\t\treturn \"Broker Deadline must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Broker_Deadline):\n\t\t\treturn \"Broker Deadline must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Broker_Deadline,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17197,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/2546cf60-b928-435c-997f-4cec4e89be3e_output.json",
            "name": "validate_broker_deadline",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/2546cf60-b928-435c-997f-4cec4e89be3e.json",
            "type": "VALIDATIONS"
        },
        "17198": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "CCE_Table",
                    "value": "CCE Table"
                }
            ],
            "code": "\n\ndef compare_inception_date_with_policy_period_start_date(CCE_Table, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t  CCE_Table = str(CCE_Table).strip()\n\texcept:\n\t  return None\n\ttry:\n\t\tCCE_Table = json.loads(CCE_Table)\n\texcept:\n\t\ttry:\n\t\t\t\tCCE_Table = eval(CCE_Table)\n\t\texcept:\n\t\t\t\treturn None\n\t\n\ttry:\n\t    CCE_Table = convert_to_df(CCE_Table)\n\t\n\t    # Convert the column to datetime\n\t    CCE_Table['Policy Year Start Date'] = pd.to_datetime(CCE_Table['Policy Year Start Date'], format='%d/%m/%Y')\n\t\n\t    incepts_on_date = pd.to_datetime(Incepts_On, format='%d/%m/%Y')\n\t\n\t    # Compare day and month of inception date with policy period start date\n\t    all_match = CCE_Table['Policy Year Start Date'].apply(\n\t        lambda x: x.day == incepts_on_date.day and x.month == incepts_on_date.month\n\t    ).all()\n\t\n\t    # Final result\n\t    result = None if all_match else \"Inception date doesn't matches with Policy Period Start Dates\"\n\t\n\t    return result\n\t\n\texcept:\n\t    return None",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\t\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\ttry:\n\t  CCE_Table = str(CCE_Table).strip()\n\texcept:\n\t  return None\n\ttry:\n\t\tCCE_Table = json.loads(CCE_Table)\n\texcept:\n\t\ttry:\n\t\t\t\tCCE_Table = eval(CCE_Table)\n\t\texcept:\n\t\t\t\treturn None\n\t\n\ttry:\n\t    CCE_Table = convert_to_df(CCE_Table)\n\t\n\t    # Convert the column to datetime\n\t    CCE_Table['Policy Year Start Date'] = pd.to_datetime(CCE_Table['Policy Year Start Date'], format='%d/%m/%Y')\n\t\n\t    incepts_on_date = pd.to_datetime(Incepts_On, format='%d/%m/%Y')\n\t\n\t    # Compare day and month of inception date with policy period start date\n\t    all_match = CCE_Table['Policy Year Start Date'].apply(\n\t        lambda x: x.day == incepts_on_date.day and x.month == incepts_on_date.month\n\t    ).all()\n\t\n\t    # Final result\n\t    result = None if all_match else \"Inception date doesn't matches with Policy Period Start Dates\"\n\t\n\t    return result\n\t\n\texcept:\n\t    return None",
            "id": 17198,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/ccf167c8-ff9a-49ca-96fd-95adab0ba079_output.json",
            "name": "compare_inception_date_with_policy_period_start_date",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/ccf167c8-ff9a-49ca-96fd-95adab0ba079.json",
            "type": "VALIDATIONS"
        },
        "17199": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Date_Established",
                    "value": "Date Established"
                }
            ],
            "code": "\n\ndef validate_date_established(Date_Established, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Date_Established:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Date_Established = str(Date_Established).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\t\n\t\tif contains_only_quotes(Date_Established):\n\t\t\treturn None\n\t\tif not isinstance(Date_Established,str):\n\t\t\treturn \"Date Established must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Date_Established):\n\t\t\treturn \"Date Established must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Date_Established,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Date_Established:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Date_Established = str(Date_Established).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\t\n\t\tif contains_only_quotes(Date_Established):\n\t\t\treturn None\n\t\tif not isinstance(Date_Established,str):\n\t\t\treturn \"Date Established must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Date_Established):\n\t\t\treturn \"Date Established must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Date_Established,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17199,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/814fe66a-71fc-4cef-abc4-afbcedbf487c_output.json",
            "name": "validate_date_established",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/814fe66a-71fc-4cef-abc4-afbcedbf487c.json",
            "type": "VALIDATIONS"
        },
        "17200": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Incepts_On",
                    "value": "Incepts On"
                }
            ],
            "code": "\n\ndef validate_incepts_on(Incepts_On, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Incepts_On:\n\t\ttry:\n\t\t\tIncepts_On = str(Incepts_On).strip()\n\t\texcept:\n\t\t\treturn None \n\t\t# Return None to indicate validation passed\n\t\t# Incepts_On = Incepts_On.strip()\n\t\tif contains_only_quotes(Incepts_On):\n\t\t\treturn None\n\t\tif not isinstance(Incepts_On,str):\n\t\t\treturn \"Incepts On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Incepts_On):\n\t\t\treturn \"Incepts On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Incepts_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Incepts_On:\n\t\ttry:\n\t\t\tIncepts_On = str(Incepts_On).strip()\n\t\texcept:\n\t\t\treturn None \n\t\t# Return None to indicate validation passed\n\t\t# Incepts_On = Incepts_On.strip()\n\t\tif contains_only_quotes(Incepts_On):\n\t\t\treturn None\n\t\tif not isinstance(Incepts_On,str):\n\t\t\treturn \"Incepts On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Incepts_On):\n\t\t\treturn \"Incepts On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Incepts_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17200,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/08e1d47f-e2c0-457f-aad8-3f3990de5a66_output.json",
            "name": "validate_incepts_on",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/08e1d47f-e2c0-457f-aad8-3f3990de5a66.json",
            "type": "VALIDATIONS"
        },
        "17201": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Expires_On",
                    "value": "Expires On"
                }
            ],
            "code": "\n\ndef validate_expires_on(Expires_On, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Expires_On:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Expires_On = str(Expires_On).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\n\t\tif contains_only_quotes(Expires_On):\n\t\t\treturn None\n\t\tif not isinstance(Expires_On,str):\n\t\t\treturn \"Expires On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Expires_On):\n\t\t\treturn \"Expires On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Expires_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Expires_On:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Expires_On = str(Expires_On).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\n\t\tif contains_only_quotes(Expires_On):\n\t\t\treturn None\n\t\tif not isinstance(Expires_On,str):\n\t\t\treturn \"Expires On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Expires_On):\n\t\t\treturn \"Expires On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Expires_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17201,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/b6bd4d8a-fde4-4b44-ad5d-9ee087834163_output.json",
            "name": "validate_expires_on",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/b6bd4d8a-fde4-4b44-ad5d-9ee087834163.json",
            "type": "VALIDATIONS"
        },
        "17202": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Number_of_Notifiable_Vehicles",
                    "value": "Number of Notifiable Vehicles"
                }
            ],
            "code": "\n\ndef validate_number_of_notifiable_vehicles(Number_of_Notifiable_Vehicles, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\tif Number_of_Notifiable_Vehicles:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t\tNumber_of_Notifiable_Vehicles = str(Number_of_Notifiable_Vehicles).strip()\n\t\texcept:\n\t\t\treturn None\n\t\ttry:\n\t\t\tn = int (Number_of_Notifiable_Vehicles)\n\t\t\tif n < 0:\n\t\t\t\treturn \"Number of Notifiable Vehicles must be a non-negative interger.\"\n\t\t\treturn None\n\t\texcept(ValueError, TypeError):\n\t\t\treturn \"Number of Notifiable Vehicles must be a number.\"\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\tif Number_of_Notifiable_Vehicles:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t\tNumber_of_Notifiable_Vehicles = str(Number_of_Notifiable_Vehicles).strip()\n\t\texcept:\n\t\t\treturn None\n\t\ttry:\n\t\t\tn = int (Number_of_Notifiable_Vehicles)\n\t\t\tif n < 0:\n\t\t\t\treturn \"Number of Notifiable Vehicles must be a non-negative interger.\"\n\t\t\treturn None\n\t\texcept(ValueError, TypeError):\n\t\t\treturn \"Number of Notifiable Vehicles must be a number.\"\n\telse:\n\t\treturn None",
            "id": 17202,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/ed67588a-f6b8-4ad5-b871-24809bb3de17_output.json",
            "name": "validate_number_of_notifiable_vehicles",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/ed67588a-f6b8-4ad5-b871-24809bb3de17.json",
            "type": "VALIDATIONS"
        },
        "17203": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_Accident_Damage",
                    "value": "Excess Type Accident Damage"
                }
            ],
            "code": "\n\ndef validate_excess_type_accident_damage(Excess_Type_Accident_Damage, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Accident_Damage:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Accident_Damage = str(Excess_Type_Accident_Damage).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Accident_Damage)\n\t  except Exception:\n\t    return \"Invalid Excess Type Accident Damage\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Accident_Damage:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Accident_Damage = str(Excess_Type_Accident_Damage).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Accident_Damage)\n\t  except Exception:\n\t    return \"Invalid Excess Type Accident Damage\"\n\telse:\n\t  return None\n\t",
            "id": 17203,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/52b4f047-db65-4b45-b59e-8bd167b3513b_output.json",
            "name": "validate_excess_type_accident_damage",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/52b4f047-db65-4b45-b59e-8bd167b3513b.json",
            "type": "VALIDATIONS"
        },
        "17204": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_Fire",
                    "value": "Excess Type Fire"
                }
            ],
            "code": "\n\ndef validate_excess_type_fire(Excess_Type_Fire, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Fire:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Fire = str(Excess_Type_Fire).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Fire)\n\t  except Exception:\n\t    return \"Invalid Excess Type Fire Price\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Fire:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Fire = str(Excess_Type_Fire).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Fire)\n\t  except Exception:\n\t    return \"Invalid Excess Type Fire Price\"\n\telse:\n\t  return None\n\t",
            "id": 17204,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/26287cc8-2dfb-4e88-a8b2-0196b3d1a555_output.json",
            "name": "validate_excess_type_fire",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/26287cc8-2dfb-4e88-a8b2-0196b3d1a555.json",
            "type": "VALIDATIONS"
        },
        "17205": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_Theft",
                    "value": "Excess Type Theft"
                }
            ],
            "code": "\n\ndef validate_excess_type_theft(Excess_Type_Theft, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Theft:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Theft = str(Excess_Type_Theft).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Theft)\n\t  except Exception:\n\t    return \"Invalid Excess Type Theft Price\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Theft:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Theft = str(Excess_Type_Theft).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Theft)\n\t  except Exception:\n\t    return \"Invalid Excess Type Theft Price\"\n\telse:\n\t  return None\n\t",
            "id": 17205,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/8a44db3e-92e1-452c-847a-4954940009db_output.json",
            "name": "validate_excess_type_theft",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/8a44db3e-92e1-452c-847a-4954940009db.json",
            "type": "VALIDATIONS"
        },
        "17206": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_WS",
                    "value": "Excess Type WS"
                }
            ],
            "code": "\n\ndef validate_excess_type_ws(Excess_Type_WS, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_WS:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_WS = str(Excess_Type_WS).strip()\n\t    clean_and_validate_price_loose(Excess_Type_WS)\n\t  except Exception:\n\t    return \"Invalid Excess Type WS Price\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_WS:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_WS = str(Excess_Type_WS).strip()\n\t    clean_and_validate_price_loose(Excess_Type_WS)\n\t  except Exception:\n\t    return \"Invalid Excess Type WS Price\"\n\telse:\n\t  return None\n\t",
            "id": 17206,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/f6b9a550-9ff1-4017-9f5e-733dabd6ebdb_output.json",
            "name": "validate_excess_type_ws",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/f6b9a550-9ff1-4017-9f5e-733dabd6ebdb.json",
            "type": "VALIDATIONS"
        },
        "17207": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Incepts_On",
                    "value": "Incepts On"
                }
            ],
            "code": "\n\ndef validate_incepts_on(Incepts_On, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Incepts_On:\n\t\ttry:\n\t\t\tIncepts_On = str(Incepts_On).strip()\n\t\texcept:\n\t\t\treturn None \n\t\t# Return None to indicate validation passed\n\t\t# Incepts_On = Incepts_On.strip()\n\t\tif contains_only_quotes(Incepts_On):\n\t\t\treturn None\n\t\tif not isinstance(Incepts_On,str):\n\t\t\treturn \"Incepts On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Incepts_On):\n\t\t\treturn \"Incepts On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Incepts_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Incepts_On:\n\t\ttry:\n\t\t\tIncepts_On = str(Incepts_On).strip()\n\t\texcept:\n\t\t\treturn None \n\t\t# Return None to indicate validation passed\n\t\t# Incepts_On = Incepts_On.strip()\n\t\tif contains_only_quotes(Incepts_On):\n\t\t\treturn None\n\t\tif not isinstance(Incepts_On,str):\n\t\t\treturn \"Incepts On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Incepts_On):\n\t\t\treturn \"Incepts On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Incepts_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17207,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/d30722fc-9456-4ae1-be47-dfd529a9b8cc_output.json",
            "name": "validate_incepts_on",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/d30722fc-9456-4ae1-be47-dfd529a9b8cc.json",
            "type": "VALIDATIONS"
        },
        "17208": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Expires_On",
                    "value": "Expires On"
                }
            ],
            "code": "\n\ndef validate_expires_on(Expires_On, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Expires_On:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Expires_On = str(Expires_On).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\n\t\tif contains_only_quotes(Expires_On):\n\t\t\treturn None\n\t\tif not isinstance(Expires_On,str):\n\t\t\treturn \"Expires On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Expires_On):\n\t\t\treturn \"Expires On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Expires_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Expires_On:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Expires_On = str(Expires_On).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\n\t\tif contains_only_quotes(Expires_On):\n\t\t\treturn None\n\t\tif not isinstance(Expires_On,str):\n\t\t\treturn \"Expires On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Expires_On):\n\t\t\treturn \"Expires On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Expires_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17208,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/a649725a-cb51-43a8-99b6-5f35bb6a6b5b_output.json",
            "name": "validate_expires_on",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/a649725a-cb51-43a8-99b6-5f35bb6a6b5b.json",
            "type": "VALIDATIONS"
        },
        "17209": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_Theft_CCE",
                    "value": "Excess Type Theft CCE"
                }
            ],
            "code": "\n\ndef validate_excess_type_theft(Excess_Type_Theft_CCE, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Theft_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Theft_CCE = str(Excess_Type_Theft_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Theft_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type Theft Price\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Theft_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Theft_CCE = str(Excess_Type_Theft_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Theft_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type Theft Price\"\n\telse:\n\t  return None\n\t",
            "id": 17209,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/c38ce58c-06cd-4d79-af62-ab781cdd6700_output.json",
            "name": "validate_excess_type_theft",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/c38ce58c-06cd-4d79-af62-ab781cdd6700.json",
            "type": "VALIDATIONS"
        },
        "17210": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_Accident_Damage_CCE",
                    "value": "Excess Type Accident Damage CCE"
                }
            ],
            "code": "\n\ndef validate_excess_type_accident_damage(Excess_Type_Accident_Damage_CCE, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Accident_Damage_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Accident_Damage_CCE = str(Excess_Type_Accident_Damage_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Accident_Damage_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type Accident Damage\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Accident_Damage_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Accident_Damage_CCE = str(Excess_Type_Accident_Damage_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Accident_Damage_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type Accident Damage\"\n\telse:\n\t  return None\n\t",
            "id": 17210,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/1e8cf7c9-60ae-4df8-9931-cedf97773710_output.json",
            "name": "validate_excess_type_accident_damage",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/1e8cf7c9-60ae-4df8-9931-cedf97773710.json",
            "type": "VALIDATIONS"
        },
        "17211": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_Fire_CCE",
                    "value": "Excess Type Fire CCE"
                }
            ],
            "code": "\n\ndef validate_excess_type_fire(Excess_Type_Fire_CCE, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Fire_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Fire_CCE = str(Excess_Type_Fire_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Fire_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type Fire Price\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_Fire_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_Fire_CCE = str(Excess_Type_Fire_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_Fire_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type Fire Price\"\n\telse:\n\t  return None\n\t",
            "id": 17211,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/0d3c5b44-34be-45a0-ae57-92efbea5e332_output.json",
            "name": "validate_excess_type_fire",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/0d3c5b44-34be-45a0-ae57-92efbea5e332.json",
            "type": "VALIDATIONS"
        },
        "17212": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Excess_Type_WS_CCE",
                    "value": "Excess Type WS CCE"
                }
            ],
            "code": "\n\ndef validate_excess_type_ws(Excess_Type_WS_CCE, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_WS_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_WS_CCE = str(Excess_Type_WS_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_WS_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type WS Price\"\n\telse:\n\t  return None\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Excess_Type_WS_CCE:\n\t  # Return None to indicate validation passed\n\t  try:\n\t    Excess_Type_WS_CCE = str(Excess_Type_WS_CCE).strip()\n\t    clean_and_validate_price_loose(Excess_Type_WS_CCE)\n\t  except Exception:\n\t    return \"Invalid Excess Type WS Price\"\n\telse:\n\t  return None\n\t",
            "id": 17212,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/44e718f0-9942-4898-b24a-f9fe374920f7_output.json",
            "name": "validate_excess_type_ws",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/44e718f0-9942-4898-b24a-f9fe374920f7.json",
            "type": "VALIDATIONS"
        },
        "17213": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Postcode",
                    "value": "Party Address Postcode"
                }
            ],
            "code": "\n\ndef validate_party_address_postcode(Party_Address_Postcode, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
            "id": 17213,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/20f4447a-f7e4-4c53-bfb5-426f1124de2a_output.json",
            "name": "validate_party_address_postcode",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/20f4447a-f7e4-4c53-bfb5-426f1124de2a.json",
            "type": "VALIDATIONS"
        },
        "17214": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Postcode",
                    "value": "Party Address Postcode"
                }
            ],
            "code": "\n\ndef validate_party_address_postcode(Party_Address_Postcode, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
            "id": 17214,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/7ad7a77a-36ce-4b08-8346-a4c43c198b09_output.json",
            "name": "validate_party_address_postcode",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/7ad7a77a-36ce-4b08-8346-a4c43c198b09.json",
            "type": "VALIDATIONS"
        },
        "17215": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Number_of_Notifiable_Vehicles",
                    "value": "Number of Notifiable Vehicles"
                }
            ],
            "code": "\n\ndef validate_number_of_notifiable_vehicles(Number_of_Notifiable_Vehicles, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\tif Number_of_Notifiable_Vehicles:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t\tNumber_of_Notifiable_Vehicles = str(Number_of_Notifiable_Vehicles).strip()\n\t\texcept:\n\t\t\treturn None\n\t\ttry:\n\t\t\tn = int (Number_of_Notifiable_Vehicles)\n\t\t\tif n < 0:\n\t\t\t\treturn \"Number of Notifiable Vehicles must be a non-negative interger.\"\n\t\t\treturn None\n\t\texcept(ValueError, TypeError):\n\t\t\treturn \"Number of Notifiable Vehicles must be a number.\"\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\tif Number_of_Notifiable_Vehicles:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t\tNumber_of_Notifiable_Vehicles = str(Number_of_Notifiable_Vehicles).strip()\n\t\texcept:\n\t\t\treturn None\n\t\ttry:\n\t\t\tn = int (Number_of_Notifiable_Vehicles)\n\t\t\tif n < 0:\n\t\t\t\treturn \"Number of Notifiable Vehicles must be a non-negative interger.\"\n\t\t\treturn None\n\t\texcept(ValueError, TypeError):\n\t\t\treturn \"Number of Notifiable Vehicles must be a number.\"\n\telse:\n\t\treturn None",
            "id": 17215,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/5e065508-0d6a-4dd8-9709-67db6e7fb9ba_output.json",
            "name": "validate_number_of_notifiable_vehicles",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/5e065508-0d6a-4dd8-9709-67db6e7fb9ba.json",
            "type": "VALIDATIONS"
        },
        "17216": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Party_Address_Postcode",
                    "value": "Party Address Postcode"
                }
            ],
            "code": "\n\ndef validate_party_address_postcode(Party_Address_Postcode, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Party_Address_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
            "id": 17216,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/7478d8e2-b54a-4456-8ac2-3001e2bcc7ca_output.json",
            "name": "validate_party_address_postcode",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/7478d8e2-b54a-4456-8ac2-3001e2bcc7ca.json",
            "type": "VALIDATIONS"
        },
        "17217": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Target_Price",
                    "value": "Target Price"
                }
            ],
            "code": "\n\ndef validate_target_price(Target_Price, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_target_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Target_Price:\n\t\t# Return None to indicate validation passed\n\t  try:\n\t      Target_Price = str(Target_Price).strip()\n\t  except:\n\t      None\n\t  try:\n\t      clean_and_validate_target_price_loose(Target_Price)\n\t  except Exception:\n\t      print(traceback.format_exc())\n\t      return \"Invalid Target Price\"\n\telse:\n\t      return None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\t\n\timport re\n\timport traceback\n\t\n\tALLOWED_CURRENCY_SYMBOLS = ['\u00a3', '$', '\u20ac', '\u20b9', '\u00a5']\n\t\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tdef clean_and_validate_target_price_loose(value):\n\t    \"\"\"\n\t    Validates and cleans a target price string supporting multiple currencies.\n\t    Loose version: allows any comma usage.\n\t    \"\"\"\n\t    value = value.strip()\n\t    if contains_only_quotes(value):  \n\t      return None\n\t\n\t    if not value or not isinstance(value, str):\n\t        raise ValueError(\"Target Price must be a non-empty string.\")\n\t\n\t    # Escape currency symbols\n\t    escaped_symbols = ''.join(re.escape(sym) for sym in ALLOWED_CURRENCY_SYMBOLS)\n\t\n\t    # Relaxed pattern: currency + digits/commas + optional decimal\n\t    pattern = rf\"\"\"\n\t        ^\\s*\n\t        [{escaped_symbols}]?      # optional currency\n\t        \\s*\n\t        [\\d,]+                    # digits and commas\n\t        (?:\\.\\d+)?                # optional decimal\n\t        \\s*$\n\t    \"\"\"\n\t\n\t    if not re.fullmatch(pattern, value, re.VERBOSE):\n\t        raise ValueError(f\"Invalid Target Price format: '{value}'\")\n\t\n\t    # Strip currency, commas, spaces\n\t    cleaned = re.sub(rf\"[{escaped_symbols},\\s]\", \"\", value)\n\t    try:\n\t        price = float(cleaned)\n\t    except ValueError:\n\t        raise ValueError(f\"Unable to convert '{value}' to a numeric price.\")\n\t    return None\n\t\n\t\n\tif Target_Price:\n\t\t# Return None to indicate validation passed\n\t  try:\n\t      Target_Price = str(Target_Price).strip()\n\t  except:\n\t      None\n\t  try:\n\t      clean_and_validate_target_price_loose(Target_Price)\n\t  except Exception:\n\t      print(traceback.format_exc())\n\t      return \"Invalid Target Price\"\n\telse:\n\t      return None",
            "id": 17217,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/b38fefc1-bfb0-4903-8dde-21104c886358_output.json",
            "name": "validate_target_price",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/b38fefc1-bfb0-4903-8dde-21104c886358.json",
            "type": "VALIDATIONS"
        },
        "17218": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Agency_Enquiry_Reference",
                    "value": "Agency Enquiry Reference"
                }
            ],
            "code": "\n\ndef validate_agency_enquiry_reference(Agency_Enquiry_Reference, context = {}, keys = {}, **kwargs):\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Agency_Enquiry_Reference:\n\t\t# Return None to indicate validation passed\n\t\tif not isinstance(Agency_Enquiry_Reference, str):\n\t\t\treturn \"Agency Enquiry Reference must be a string\"\n\t\tAgency_Enquiry_Reference = Agency_Enquiry_Reference.strip()\n\t\tif not Agency_Enquiry_Reference:\n\t\t\treturn \"Agency Enquiry Reference cannot be empty or whitespace.\"\n\t\tif contains_only_quotes(Agency_Enquiry_Reference):\n\t\t\t\treturn None\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Agency_Enquiry_Reference:\n\t\t# Return None to indicate validation passed\n\t\tif not isinstance(Agency_Enquiry_Reference, str):\n\t\t\treturn \"Agency Enquiry Reference must be a string\"\n\t\tAgency_Enquiry_Reference = Agency_Enquiry_Reference.strip()\n\t\tif not Agency_Enquiry_Reference:\n\t\t\treturn \"Agency Enquiry Reference cannot be empty or whitespace.\"\n\t\tif contains_only_quotes(Agency_Enquiry_Reference):\n\t\t\t\treturn None\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17218,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/f125326e-65b9-4fcc-b7d4-140bb9ba4ffb_output.json",
            "name": "validate_agency_enquiry_reference",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/f125326e-65b9-4fcc-b7d4-140bb9ba4ffb.json",
            "type": "VALIDATIONS"
        },
        "17219": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Broker_Deadline",
                    "value": "Broker Deadline"
                }
            ],
            "code": "\n\ndef validate_broker_deadline(Broker_Deadline, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Broker_Deadline:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Broker_Deadline = str(Broker_Deadline).strip()\n\t\texcept:\n\t\t\treturn None\n\t\n\t\tif contains_only_quotes(Broker_Deadline):\n\t\t\treturn None\n\t\tif not isinstance(Broker_Deadline,str):\n\t\t\treturn \"Broker Deadline must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Broker_Deadline):\n\t\t\treturn \"Broker Deadline must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Broker_Deadline,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Broker_Deadline:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Broker_Deadline = str(Broker_Deadline).strip()\n\t\texcept:\n\t\t\treturn None\n\t\n\t\tif contains_only_quotes(Broker_Deadline):\n\t\t\treturn None\n\t\tif not isinstance(Broker_Deadline,str):\n\t\t\treturn \"Broker Deadline must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Broker_Deadline):\n\t\t\treturn \"Broker Deadline must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Broker_Deadline,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17219,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/280b5ca7-75ef-43dc-b16b-3ab465ca2dbf_output.json",
            "name": "validate_broker_deadline",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/280b5ca7-75ef-43dc-b16b-3ab465ca2dbf.json",
            "type": "VALIDATIONS"
        },
        "17220": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "CCE_Table",
                    "value": "CCE Table"
                }
            ],
            "code": "\n\ndef numeric_validations_for_CCE_Table(CCE_Table, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_numeric_columns_old(df, columns_to_check):\n\t\n\t    invalid_columns = []\n\t    \n\t    for col in columns_to_check:\n\t        # Treat empty strings as NaN\n\t        # series = df[col].replace(\"\", np.nan)\n\t        \n\t        # Convert to numeric (non-numeric -> NaN)\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        \n\t        # Compare only non-empty rows\n\t        mask_non_empty = series.notna()\n\t        non_numeric_mask = mask_non_empty & numeric_check.isna()\n\t\n\t        if non_numeric_mask.any():\n\t            # Store non-numeric values for this column\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t\n\t    # print(invalid_columns)\n\t\n\t    # Return None if all columns are valid\n\t    if len(invalid_columns)==0:\n\t        return None\n\t    # else:\n\t    #     print(str(invalid_columns) + \" has non numeric columns\")\n\t    #     return str(invalid_columns) + \"has non numeric columns\"\n\t    elif len(invalid_columns) == 1:\n\t        return f\"The column '{invalid_columns[0]}' contains non-numeric values.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_columns)\n\t        return f\"The columns {cols_str} contain non-numeric values.\"\n\t\n\t\n\t  \n\tdef validate_numeric_columns_old_2(df, columns_to_check):\n\t    invalid_columns = {}\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns[col] = invalid_values\n\t    if not invalid_columns:\n\t        return None\n\t    else:\n\t        return str(invalid_columns) + \" has non-numeric values\"\n\t\n\tdef validate_numeric_columns(df, columns_to_check):\n\t    invalid_columns = []\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            # invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t    if not invalid_columns:\n\t        return None\n\t    # else:\n\t    #     return str(invalid_columns) + \" has non-numeric values\"\n\t    elif len(invalid_columns) == 1:\n\t        return f\"The column '{invalid_columns[0]}' contains non-numeric values.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_columns)\n\t        return f\"The columns {cols_str} contain non-numeric values.\"\n\t\n\t# Columns you want to check\n\tcolumns_to_check = [\n\t\n\t'Excess: AD', \n\t'Excess: Fire', \n\t'Excess: Theft', \n\t'Excess: WS',\n\t\n\t'Incurreds - Paid: AD&WS',\n\t'Incurreds - Paid: FT', \n\t'Incurreds - Paid: TP',\n\t'Incurreds - Outstanding: AD&WS',\n\t'Incurreds - Outstanding: FT',\n\t'Incurreds - Outstanding: TP', \n\t'Total Incurred Paid +  Outstanding',\n\t\n\t'Vehicle Years Earned',\n\t'Claim Count: All',\n\t]\n\t\n\t\n\ttry:\n\t    try:\n\t        CCE_Table = json.loads(str(CCE_Table).strip())\n\t    except:\n\t        try:\n\t            CCE_Table = eval(str(CCE_Table).strip())\n\t        except:\n\t            return None\n\t    CCE_Table = convert_to_df( CCE_Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t\n\tresult = validate_numeric_columns( CCE_Table, columns_to_check )\n\tprint(result)\n\treturn result",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_numeric_columns_old(df, columns_to_check):\n\t\n\t    invalid_columns = []\n\t    \n\t    for col in columns_to_check:\n\t        # Treat empty strings as NaN\n\t        # series = df[col].replace(\"\", np.nan)\n\t        \n\t        # Convert to numeric (non-numeric -> NaN)\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        \n\t        # Compare only non-empty rows\n\t        mask_non_empty = series.notna()\n\t        non_numeric_mask = mask_non_empty & numeric_check.isna()\n\t\n\t        if non_numeric_mask.any():\n\t            # Store non-numeric values for this column\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t\n\t    # print(invalid_columns)\n\t\n\t    # Return None if all columns are valid\n\t    if len(invalid_columns)==0:\n\t        return None\n\t    # else:\n\t    #     print(str(invalid_columns) + \" has non numeric columns\")\n\t    #     return str(invalid_columns) + \"has non numeric columns\"\n\t    elif len(invalid_columns) == 1:\n\t        return f\"The column '{invalid_columns[0]}' contains non-numeric values.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_columns)\n\t        return f\"The columns {cols_str} contain non-numeric values.\"\n\t\n\t\n\t  \n\tdef validate_numeric_columns_old_2(df, columns_to_check):\n\t    invalid_columns = {}\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns[col] = invalid_values\n\t    if not invalid_columns:\n\t        return None\n\t    else:\n\t        return str(invalid_columns) + \" has non-numeric values\"\n\t\n\tdef validate_numeric_columns(df, columns_to_check):\n\t    invalid_columns = []\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            # invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t    if not invalid_columns:\n\t        return None\n\t    # else:\n\t    #     return str(invalid_columns) + \" has non-numeric values\"\n\t    elif len(invalid_columns) == 1:\n\t        return f\"The column '{invalid_columns[0]}' contains non-numeric values.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_columns)\n\t        return f\"The columns {cols_str} contain non-numeric values.\"\n\t\n\t# Columns you want to check\n\tcolumns_to_check = [\n\t\n\t'Excess: AD', \n\t'Excess: Fire', \n\t'Excess: Theft', \n\t'Excess: WS',\n\t\n\t'Incurreds - Paid: AD&WS',\n\t'Incurreds - Paid: FT', \n\t'Incurreds - Paid: TP',\n\t'Incurreds - Outstanding: AD&WS',\n\t'Incurreds - Outstanding: FT',\n\t'Incurreds - Outstanding: TP', \n\t'Total Incurred Paid +  Outstanding',\n\t\n\t'Vehicle Years Earned',\n\t'Claim Count: All',\n\t]\n\t\n\t\n\ttry:\n\t    try:\n\t        CCE_Table = json.loads(str(CCE_Table).strip())\n\t    except:\n\t        try:\n\t            CCE_Table = eval(str(CCE_Table).strip())\n\t        except:\n\t            return None\n\t    CCE_Table = convert_to_df( CCE_Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t\n\tresult = validate_numeric_columns( CCE_Table, columns_to_check )\n\tprint(result)\n\treturn result",
            "id": 17220,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/ebb43805-c431-4943-a956-bcca783d986d_output.json",
            "name": "numeric_validations_for_CCE_Table",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/ebb43805-c431-4943-a956-bcca783d986d.json",
            "type": "VALIDATIONS"
        },
        "17221": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "CCE_Table",
                    "value": "CCE Table"
                }
            ],
            "code": "\n\ndef data_validations(CCE_Table, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Policy Year Start Date',\n\t  'Policy Year End Date'\n\t]\n\tTable = CCE_Table\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Policy Year Start Date',\n\t  'Policy Year End Date'\n\t]\n\tTable = CCE_Table\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "id": 17221,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/3b7568e4-68e5-4067-b750-a526f4ec8013_output.json",
            "name": "data_validations",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/3b7568e4-68e5-4067-b750-a526f4ec8013.json",
            "type": "VALIDATIONS"
        },
        "17222": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "CCE_Table",
                    "value": "CCE Table"
                }
            ],
            "code": "\n\ndef date_validations(CCE_Table, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Policy Year Start Date',\n\t  'Policy Year End Date'\n\t]\n\tTable = CCE_Table\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Policy Year Start Date',\n\t  'Policy Year End Date'\n\t]\n\tTable = CCE_Table\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "id": 17222,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/0662137c-a5a9-47ac-a367-1fc172a95b25_output.json",
            "name": "date_validations",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/0662137c-a5a9-47ac-a367-1fc172a95b25.json",
            "type": "VALIDATIONS"
        },
        "17223": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Schedule_Table",
                    "value": "Vehicle Schedule Table"
                }
            ],
            "code": "\n\ndef date_validations(Vehicle_Schedule_Table, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Effective From',\n\t  'Effective To'\n\t]\n\tTable = Vehicle_Schedule_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Effective From',\n\t  'Effective To'\n\t]\n\tTable = Vehicle_Schedule_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "id": 17223,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/f1840a78-d927-4600-b1a6-f61a3d3557e2_output.json",
            "name": "date_validations",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/f1840a78-d927-4600-b1a6-f61a3d3557e2.json",
            "type": "VALIDATIONS"
        },
        "17224": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Date_Established",
                    "value": "Date Established"
                }
            ],
            "code": "\n\ndef validate_date_established(Date_Established, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Date_Established:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Date_Established = str(Date_Established).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\t\n\t\tif contains_only_quotes(Date_Established):\n\t\t\treturn None\n\t\tif not isinstance(Date_Established,str):\n\t\t\treturn \"Date Established must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Date_Established):\n\t\t\treturn \"Date Established must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Date_Established,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Date_Established:\n\t\t# Return None to indicate validation passed\n\t\ttry:\n\t\t  Date_Established = str(Date_Established).strip()\n\t\texcept:\n\t\t\treturn None\n\t\t\t\t\n\t\tif contains_only_quotes(Date_Established):\n\t\t\treturn None\n\t\tif not isinstance(Date_Established,str):\n\t\t\treturn \"Date Established must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Date_Established):\n\t\t\treturn \"Date Established must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Date_Established,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17224,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/1d238f7d-21e5-4c38-ba3d-d434120b89c6_output.json",
            "name": "validate_date_established",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/1d238f7d-21e5-4c38-ba3d-d434120b89c6.json",
            "type": "VALIDATIONS"
        },
        "17225": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Vehicle_Schedule_Table",
                    "value": "Vehicle Schedule Table"
                }
            ],
            "code": "\n\ndef date_validations(Vehicle_Schedule_Table, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t    \n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Effective From',\n\t  'Effective To'\n\t]\n\tTable = Vehicle_Schedule_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t    \n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Effective From',\n\t  'Effective To'\n\t]\n\tTable = Vehicle_Schedule_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "id": 17225,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/ecd79cd2-2b18-4d45-9ea8-02c88ebb4dd3_output.json",
            "name": "date_validations",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/ecd79cd2-2b18-4d45-9ea8-02c88ebb4dd3.json",
            "type": "VALIDATIONS"
        },
        "17226": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Driver_Party_Table",
                    "value": "Driver Party Table"
                }
            ],
            "code": "\n\ndef date_validations(Driver_Party_Table, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Driver D.O.B',\n\t]\n\tTable = Driver_Party_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Driver D.O.B',\n\t]\n\tTable = Driver_Party_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "id": 17226,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/48a79a1c-fdda-41ae-9d42-ae0a4cb236c0_output.json",
            "name": "date_validations",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/48a79a1c-fdda-41ae-9d42-ae0a4cb236c0.json",
            "type": "VALIDATIONS"
        },
        "17227": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Driver_Party_Table",
                    "value": "Driver Party Table"
                }
            ],
            "code": "\n\ndef date_validations(Driver_Party_Table, context = {}, keys = {}, **kwargs):\n\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    # Beautify the list of columns\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Driver D.O.B',\n\t]\n\tTable = Driver_Party_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "docstring": null,
            "function_code": "\tfrom datetime import datetime\n\timport pandas as pd\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_date_columns(df, date_columns, date_format=\"%d/%m/%Y\"):\n\t    invalid_date_columns = []\n\t    for col in date_columns:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        \n\t        # Drop nulls and convert to string\n\t        col_data = df[col].dropna().astype(str).str.strip()\n\t\n\t        # Try parsing dates using pd.to_datetime with errors='coerce'\n\t        parsed_dates = pd.to_datetime(col_data, format=date_format, errors='coerce')\n\t\n\t        # If any parsing failed (NaT), mark the column as invalid\n\t        if parsed_dates.isna().any():\n\t            invalid_date_columns.append(col)\n\t    \n\t    if not invalid_date_columns:\n\t        return None\n\t    # return f\"{invalid_date_columns} contain invalid date formats\"\n\t    # Beautify the list of columns\n\t    if len(invalid_date_columns) == 1:\n\t        return f\"The column '{invalid_date_columns[0]}' contains invalid date formats.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_date_columns)\n\t        return f\"The columns {cols_str} contain invalid date formats.\"\n\t\n\t\n\t\n\tcolumns_to_check = [\n\t  'Driver D.O.B',\n\t]\n\tTable = Driver_Party_Table\n\t\n\ttry:\n\t    try:\n\t        Table = json.loads(str(Table).strip())\n\t    except:\n\t        try:\n\t            Table = eval(str(Table).strip())\n\t        except:\n\t            return None\n\t    Table = convert_to_df( Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t# CCE_Table['Excess: AD'] = [\"234Rs\", \"234\"]\n\t\n\tresult = validate_date_columns( Table, columns_to_check )\n\tprint(\"result\",result)\n\treturn result",
            "id": 17227,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/75d0ee40-4dcc-4634-a962-6751bacdcf7d_output.json",
            "name": "date_validations",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/75d0ee40-4dcc-4634-a962-6751bacdcf7d.json",
            "type": "VALIDATIONS"
        },
        "17228": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "CCE_Table",
                    "value": "CCE Table"
                }
            ],
            "code": "\n\ndef numeric_validations_for_CCE_Table(CCE_Table, context = {}, keys = {}, **kwargs):\n\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_numeric_columns_old(df, columns_to_check):\n\t\n\t    invalid_columns = []\n\t    \n\t    for col in columns_to_check:\n\t        # Treat empty strings as NaN\n\t        # series = df[col].replace(\"\", np.nan)\n\t        \n\t        # Convert to numeric (non-numeric -> NaN)\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        \n\t        # Compare only non-empty rows\n\t        mask_non_empty = series.notna()\n\t        non_numeric_mask = mask_non_empty & numeric_check.isna()\n\t\n\t        if non_numeric_mask.any():\n\t            # Store non-numeric values for this column\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t\n\t    # print(invalid_columns)\n\t\n\t    # Return None if all columns are valid\n\t    if len(invalid_columns)==0:\n\t        return None\n\t    else:\n\t        print(str(invalid_columns) + \" has non numeric columns\")\n\t        return str(invalid_columns) + \"has non numeric columns\"\n\t\n\t  \n\tdef validate_numeric_columns_old(df, columns_to_check):\n\t    invalid_columns = {}\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns[col] = invalid_values\n\t    if not invalid_columns:\n\t        return None\n\t    else:\n\t        return str(invalid_columns) + \" has non-numeric values\"\n\t\n\tdef validate_numeric_columns(df, columns_to_check):\n\t    invalid_columns = []\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            # invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t    if not invalid_columns:\n\t        return None\n\t    # else:\n\t    #     return str(invalid_columns) + \" has non-numeric values\"\n\t    elif len(invalid_columns) == 1:\n\t        return f\"The column '{invalid_columns[0]}' contains non-numeric values.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_columns)\n\t        return f\"The columns {cols_str} contain non-numeric values.\"\n\t\n\t\n\t# Columns you want to check\n\tcolumns_to_check = [\n\t\n\t'Excess: AD', \n\t'Excess: Fire', \n\t'Excess: Theft', \n\t'Excess: WS',\n\t\n\t'Incurreds - Paid: AD&WS',\n\t'Incurreds - Paid: FT', \n\t'Incurreds - Paid: TP',\n\t'Incurreds - Outstanding: AD&WS',\n\t'Incurreds - Outstanding: FT',\n\t'Incurreds - Outstanding: TP', \n\t'Total Incurred Paid +  Outstanding',\n\t\n\t'Vehicle Years Earned',\n\t'Claim Count: All',\n\t]\n\t\n\t\n\ttry:\n\t    try:\n\t        CCE_Table = json.loads(str(CCE_Table).strip())\n\t    except:\n\t        try:\n\t            CCE_Table = eval(str(CCE_Table).strip())\n\t        except:\n\t            return None\n\t    CCE_Table = convert_to_df( CCE_Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t\n\tresult = validate_numeric_columns( CCE_Table, columns_to_check )\n\tprint(result)\n\treturn result",
            "docstring": null,
            "function_code": "\timport pandas as pd\n\timport json\n\timport numpy as np\n\t\n\t## Converting to dataframe format\n\tdef convert_to_df(data):\n\t    if isinstance(data, list):\n\t        if all(isinstance(item, dict) for item in data):\n\t            # List of dictionaries\n\t            return pd.DataFrame(data)\n\t        elif all(isinstance(item, list) for item in data):\n\t            # List of lists\n\t            # First row is assu med as header\n\t            return pd.DataFrame(data[1:], columns=data[0])\n\t        else:\n\t            return pd.DataFrame(data)\n\t\n\tdef validate_numeric_columns_old(df, columns_to_check):\n\t\n\t    invalid_columns = []\n\t    \n\t    for col in columns_to_check:\n\t        # Treat empty strings as NaN\n\t        # series = df[col].replace(\"\", np.nan)\n\t        \n\t        # Convert to numeric (non-numeric -> NaN)\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        \n\t        # Compare only non-empty rows\n\t        mask_non_empty = series.notna()\n\t        non_numeric_mask = mask_non_empty & numeric_check.isna()\n\t\n\t        if non_numeric_mask.any():\n\t            # Store non-numeric values for this column\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t\n\t    # print(invalid_columns)\n\t\n\t    # Return None if all columns are valid\n\t    if len(invalid_columns)==0:\n\t        return None\n\t    else:\n\t        print(str(invalid_columns) + \" has non numeric columns\")\n\t        return str(invalid_columns) + \"has non numeric columns\"\n\t\n\t  \n\tdef validate_numeric_columns_old(df, columns_to_check):\n\t    invalid_columns = {}\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns[col] = invalid_values\n\t    if not invalid_columns:\n\t        return None\n\t    else:\n\t        return str(invalid_columns) + \" has non-numeric values\"\n\t\n\tdef validate_numeric_columns(df, columns_to_check):\n\t    invalid_columns = []\n\t    for col in columns_to_check:\n\t        if col not in df.columns:\n\t            continue  # Skip if column doesn't exist\n\t        series = df[col].astype(str).str.strip()  # Ensure everything is string\n\t        numeric_check = pd.to_numeric(series, errors='coerce')\n\t        # Now consider everything that couldn't be converted as invalid\n\t        non_numeric_mask = numeric_check.isna()\n\t        if non_numeric_mask.any():\n\t            # invalid_values = series[non_numeric_mask].tolist()\n\t            invalid_columns.append(col)\n\t    if not invalid_columns:\n\t        return None\n\t    # else:\n\t    #     return str(invalid_columns) + \" has non-numeric values\"\n\t    elif len(invalid_columns) == 1:\n\t        return f\"The column '{invalid_columns[0]}' contains non-numeric values.\"\n\t    else:\n\t        cols_str = \", \".join(f\"'{col}'\" for col in invalid_columns)\n\t        return f\"The columns {cols_str} contain non-numeric values.\"\n\t\n\t\n\t# Columns you want to check\n\tcolumns_to_check = [\n\t\n\t'Excess: AD', \n\t'Excess: Fire', \n\t'Excess: Theft', \n\t'Excess: WS',\n\t\n\t'Incurreds - Paid: AD&WS',\n\t'Incurreds - Paid: FT', \n\t'Incurreds - Paid: TP',\n\t'Incurreds - Outstanding: AD&WS',\n\t'Incurreds - Outstanding: FT',\n\t'Incurreds - Outstanding: TP', \n\t'Total Incurred Paid +  Outstanding',\n\t\n\t'Vehicle Years Earned',\n\t'Claim Count: All',\n\t]\n\t\n\t\n\ttry:\n\t    try:\n\t        CCE_Table = json.loads(str(CCE_Table).strip())\n\t    except:\n\t        try:\n\t            CCE_Table = eval(str(CCE_Table).strip())\n\t        except:\n\t            return None\n\t    CCE_Table = convert_to_df( CCE_Table )\n\t    \n\texcept Exception as e:\n\t    print(e)\n\t    return None\n\t\n\t\n\tresult = validate_numeric_columns( CCE_Table, columns_to_check )\n\tprint(result)\n\treturn result",
            "id": 17228,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/bdf5d47a-444e-4197-b403-fd8de3fdcb0d_output.json",
            "name": "numeric_validations_for_CCE_Table",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/bdf5d47a-444e-4197-b403-fd8de3fdcb0d.json",
            "type": "VALIDATIONS"
        },
        "17229": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Risk_Postcode",
                    "value": "Risk Postcode"
                }
            ],
            "code": "\n\ndef validate_post_code(Risk_Postcode, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t\n\t\n\t\n\t\n\t\n\t",
            "id": 17229,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/0e29abf1-7c12-4c0a-a21b-ea40dc0453ab_output.json",
            "name": "validate_post_code",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/0e29abf1-7c12-4c0a-a21b-ea40dc0453ab.json",
            "type": "VALIDATIONS"
        },
        "17230": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Risk_Postcode",
                    "value": "Risk Postcode"
                }
            ],
            "code": "\n\ndef validate_post_code(Risk_Postcode, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t",
            "id": 17230,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/77a1f32f-b0f1-43ac-ab20-199d3749476f_output.json",
            "name": "validate_post_code",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/77a1f32f-b0f1-43ac-ab20-199d3749476f.json",
            "type": "VALIDATIONS"
        },
        "17231": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Risk_Postcode",
                    "value": "Risk Postcode"
                }
            ],
            "code": "\n\ndef validate_post_code(Risk_Postcode, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t",
            "id": 17231,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/b8734241-14c7-4162-8708-1aa9bb16bc8c_output.json",
            "name": "validate_post_code",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/b8734241-14c7-4162-8708-1aa9bb16bc8c.json",
            "type": "VALIDATIONS"
        },
        "17232": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Risk_Postcode",
                    "value": "Risk Postcode"
                }
            ],
            "code": "\n\ndef validate_post_code(Risk_Postcode, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\tdef contains_only_quotes(s):\n\t  return all(c in {'\"', \"'\"} for c in s)\n\timport re\n\t# UK postcode regex (standard style)\n\tPOSTCODE_REGEX = re.compile(\n\t    r'^([G][I][R] 0[A]{2}|'\n\t    r'((([A-Z][0-9]{1,2})|'\n\t    r'([A-Z][A-HJ-Y][0-9]{1,2})|'\n\t    r'([A-Z][0-9][A-Z])|'\n\t    r'([A-Z][A-HJ-Y][0-9][A-Z]?)))\\s?[0-9][A-Z]{2})$'\n\t)\n\tpostcode = Risk_Postcode\n\tif postcode:\n\t    try:\n\t      postcode = str(postcode).strip()\n\t    except:\n\t      return None\n\t    if contains_only_quotes(postcode):\n\t      print('inside')\n\t      return None\n\t    postcode = postcode.strip().upper()\n\t    if not POSTCODE_REGEX.match(postcode):\n\t        return \"Invalid postcode format. Please check and enter a valid UK postcode.\"\n\t    return None\n\telse:\n\t    print(\"empty\")\n\t    return None\n\t\n\t",
            "id": 17232,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/f0bced7e-134e-4fb5-8526-634c5517e4fa_output.json",
            "name": "validate_post_code",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/f0bced7e-134e-4fb5-8526-634c5517e4fa.json",
            "type": "VALIDATIONS"
        },
        "17233": {
            "args": [
                {
                    "data_type": "FIELD",
                    "name": "Incepts_On",
                    "value": "Incepts On"
                }
            ],
            "code": "\n\ndef validate_incepts_on(Incepts_On, context = {}, keys = {}, **kwargs):\n\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Incepts_On:\n\t\ttry:\n\t\t\tIncepts_On = str(Incepts_On).strip()\n\t\texcept:\n\t\t\treturn None \n\t\t# Return None to indicate validation passed\n\t\t# Incepts_On = Incepts_On.strip()\n\t\tif contains_only_quotes(Incepts_On):\n\t\t\treturn None\n\t\tif not isinstance(Incepts_On,str):\n\t\t\treturn \"Incepts On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Incepts_On):\n\t\t\treturn \"Incepts On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Incepts_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "docstring": null,
            "function_code": "\t# Import Python packages\n\t# import json\n\t\n\t# Log statements using print()\n\t# print(\"This will appear in the logs\")\n\timport re\n\tfrom datetime import datetime\n\t\n\tdef contains_only_quotes(s):\n\t\treturn all(c in {'\"', \"'\"} for c in s)\n\t\n\tif Incepts_On:\n\t\ttry:\n\t\t\tIncepts_On = str(Incepts_On).strip()\n\t\texcept:\n\t\t\treturn None \n\t\t# Return None to indicate validation passed\n\t\t# Incepts_On = Incepts_On.strip()\n\t\tif contains_only_quotes(Incepts_On):\n\t\t\treturn None\n\t\tif not isinstance(Incepts_On,str):\n\t\t\treturn \"Incepts On must be a string\"\n\t\tpattern = r\"^\\d{2}/\\d{2}/\\d{4}$\"\n\t\tif not re.fullmatch(pattern,Incepts_On):\n\t\t\treturn \"Incepts On must be in dd/mm/yyyy format exactly (e.g. 25/03/2024).\"\n\t\ttry:\n\t\t\tdatetime.strptime(Incepts_On,\"%d/%m/%Y\")\n\t\texcept ValueError:\n\t\t\treturn \"Invalid calendar date.\"\n\t\treturn None\n\telse:\n\t\treturn None",
            "id": 17233,
            "lambda_udf_id": null,
            "most_recent_results_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/4482bb67-1cda-4793-a9ed-bba295c6ea41_output.json",
            "name": "validate_incepts_on",
            "project_uuid": "0198ef37-d998-7864-878d-539c9c7ddd98",
            "return_type": "string",
            "scripts_full_path": "axauk/dev-si-fleet/fs/Instabase Drive/aihub/0198ef37-d998-7864-878d-539c9c7ddd98/project/modules/scripts/VALIDATIONS/4482bb67-1cda-4793-a9ed-bba295c6ea41.json",
            "type": "VALIDATIONS"
        }
    },
    "project_validations": [
        {
            "affected_fields": [
                193513
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360115000,
            "description": "",
            "id": 3158,
            "input_fields": [
                193513,
                193466
            ],
            "name": "Rule 1",
            "params": {
                "udf_id": 17187
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360115000
        },
        {
            "affected_fields": [
                193413
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360116000,
            "description": "",
            "id": 3159,
            "input_fields": [
                193413
            ],
            "name": "Rule 10",
            "params": {
                "udf_id": 17188
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360116000
        },
        {
            "affected_fields": [
                193429
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360116000,
            "description": "",
            "id": 3160,
            "input_fields": [
                193429
            ],
            "name": "Rule 11",
            "params": {
                "udf_id": 17189
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360116000
        },
        {
            "affected_fields": [
                193430
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360116000,
            "description": "",
            "id": 3161,
            "input_fields": [
                193430
            ],
            "name": "Rule 12",
            "params": {
                "udf_id": 17190
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360116000
        },
        {
            "affected_fields": [
                193431
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360116000,
            "description": "",
            "id": 3162,
            "input_fields": [
                193431
            ],
            "name": "Rule 13",
            "params": {
                "udf_id": 17191
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360116000
        },
        {
            "affected_fields": [
                193432
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360117000,
            "description": "",
            "id": 3163,
            "input_fields": [
                193432
            ],
            "name": "Rule 14",
            "params": {
                "udf_id": 17192
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360117000
        },
        {
            "affected_fields": [
                193441
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360117000,
            "description": "",
            "id": 3164,
            "input_fields": [
                193441
            ],
            "name": "Rule 15",
            "params": {
                "udf_id": 17193
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360117000
        },
        {
            "affected_fields": [
                193504
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360117000,
            "description": "",
            "id": 3165,
            "input_fields": [
                193504
            ],
            "name": "Rule 16",
            "params": {
                "udf_id": 17194
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360117000
        },
        {
            "affected_fields": [
                193456
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360118000,
            "description": "",
            "id": 3166,
            "input_fields": [
                193456
            ],
            "name": "Rule 17",
            "params": {
                "udf_id": 17195
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360118000
        },
        {
            "affected_fields": [
                193457
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360118000,
            "description": "",
            "id": 3167,
            "input_fields": [
                193457
            ],
            "name": "Rule 18",
            "params": {
                "udf_id": 17196
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360118000
        },
        {
            "affected_fields": [
                193458
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360118000,
            "description": "",
            "id": 3168,
            "input_fields": [
                193458
            ],
            "name": "Rule 19",
            "params": {
                "udf_id": 17197
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360118000
        },
        {
            "affected_fields": [
                193533
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360118000,
            "description": "",
            "id": 3169,
            "input_fields": [
                193533
            ],
            "name": "Rule 2",
            "params": {
                "udf_id": 17198
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360118000
        },
        {
            "affected_fields": [
                193464
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360119000,
            "description": "",
            "id": 3170,
            "input_fields": [
                193464
            ],
            "name": "Rule 20",
            "params": {
                "udf_id": 17199
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360119000
        },
        {
            "affected_fields": [
                193466
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360119000,
            "description": "",
            "id": 3171,
            "input_fields": [
                193466
            ],
            "name": "Rule 21",
            "params": {
                "udf_id": 17200
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360119000
        },
        {
            "affected_fields": [
                193467
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360119000,
            "description": "",
            "id": 3172,
            "input_fields": [
                193467
            ],
            "name": "Rule 22",
            "params": {
                "udf_id": 17201
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360119000
        },
        {
            "affected_fields": [
                193490
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360119000,
            "description": "",
            "id": 3173,
            "input_fields": [
                193490
            ],
            "name": "Rule 23",
            "params": {
                "udf_id": 17202
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360119000
        },
        {
            "affected_fields": [
                193470
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360120000,
            "description": "",
            "id": 3174,
            "input_fields": [
                193470
            ],
            "name": "Rule 24",
            "params": {
                "udf_id": 17203
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360120000
        },
        {
            "affected_fields": [
                193471
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360120000,
            "description": "",
            "id": 3175,
            "input_fields": [
                193471
            ],
            "name": "Rule 25",
            "params": {
                "udf_id": 17204
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360120000
        },
        {
            "affected_fields": [
                193472
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360120000,
            "description": "",
            "id": 3176,
            "input_fields": [
                193472
            ],
            "name": "Rule 26",
            "params": {
                "udf_id": 17205
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360120000
        },
        {
            "affected_fields": [
                193473
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360121000,
            "description": "",
            "id": 3177,
            "input_fields": [
                193473
            ],
            "name": "Rule 27",
            "params": {
                "udf_id": 17206
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360121000
        },
        {
            "affected_fields": [
                193520
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360121000,
            "description": "",
            "id": 3178,
            "input_fields": [
                193520
            ],
            "name": "Rule 28",
            "params": {
                "udf_id": 17207
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360121000
        },
        {
            "affected_fields": [
                193534
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360121000,
            "description": "",
            "id": 3179,
            "input_fields": [
                193534
            ],
            "name": "Rule 29",
            "params": {
                "udf_id": 17208
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360121000
        },
        {
            "affected_fields": [
                193528
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360122000,
            "description": "",
            "id": 3180,
            "input_fields": [
                193528
            ],
            "name": "Rule 3",
            "params": {
                "udf_id": 17209
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360122000
        },
        {
            "affected_fields": [
                193525
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360122000,
            "description": "",
            "id": 3181,
            "input_fields": [
                193525
            ],
            "name": "Rule 30",
            "params": {
                "udf_id": 17210
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360122000
        },
        {
            "affected_fields": [
                193527
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360122000,
            "description": "",
            "id": 3182,
            "input_fields": [
                193527
            ],
            "name": "Rule 31",
            "params": {
                "udf_id": 17211
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360122000
        },
        {
            "affected_fields": [
                193529
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360123000,
            "description": "",
            "id": 3183,
            "input_fields": [
                193529
            ],
            "name": "Rule 32",
            "params": {
                "udf_id": 17212
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360123000
        },
        {
            "affected_fields": [
                193546
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360123000,
            "description": "",
            "id": 3184,
            "input_fields": [
                193546
            ],
            "name": "Rule 33",
            "params": {
                "udf_id": 17213
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360123000
        },
        {
            "affected_fields": [
                193571
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360123000,
            "description": "",
            "id": 3185,
            "input_fields": [
                193571
            ],
            "name": "Rule 34",
            "params": {
                "udf_id": 17214
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360123000
        },
        {
            "affected_fields": [
                193562
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360123000,
            "description": "",
            "id": 3186,
            "input_fields": [
                193562
            ],
            "name": "Rule 35",
            "params": {
                "udf_id": 17215
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360123000
        },
        {
            "affected_fields": [
                193422
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360123000,
            "description": "",
            "id": 3187,
            "input_fields": [
                193422
            ],
            "name": "Rule 36",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360123000
        },
        {
            "affected_fields": [
                193442
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3188,
            "input_fields": [
                193442
            ],
            "name": "Rule 37",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193432
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3189,
            "input_fields": [
                193432
            ],
            "name": "Rule 38",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193431
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3190,
            "input_fields": [
                193431
            ],
            "name": "Rule 39",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193449
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3191,
            "input_fields": [
                193449
            ],
            "name": "Rule 4",
            "params": {
                "udf_id": 17216
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193430
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3192,
            "input_fields": [
                193430
            ],
            "name": "Rule 40",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193429
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3193,
            "input_fields": [
                193429
            ],
            "name": "Rule 41",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193414
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3194,
            "input_fields": [
                193414
            ],
            "name": "Rule 42",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193409
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3195,
            "input_fields": [
                193409
            ],
            "name": "Rule 43",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193408
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3196,
            "input_fields": [
                193408
            ],
            "name": "Rule 44",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193412
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3197,
            "input_fields": [
                193412
            ],
            "name": "Rule 45",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193402
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3198,
            "input_fields": [
                193402
            ],
            "name": "Rule 46",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193417
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3199,
            "input_fields": [
                193417
            ],
            "name": "Rule 47",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193401
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3200,
            "input_fields": [
                193401
            ],
            "name": "Rule 48",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193407
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360124000,
            "description": "",
            "id": 3201,
            "input_fields": [
                193407
            ],
            "name": "Rule 49",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360124000
        },
        {
            "affected_fields": [
                193401
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3202,
            "input_fields": [
                193401
            ],
            "name": "Rule 5",
            "params": {
                "udf_id": 17217
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193410
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3203,
            "input_fields": [
                193410
            ],
            "name": "Rule 50",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193406
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3204,
            "input_fields": [
                193406
            ],
            "name": "Rule 51",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193405
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3205,
            "input_fields": [
                193405
            ],
            "name": "Rule 52",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193473
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3206,
            "input_fields": [
                193473
            ],
            "name": "Rule 53",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193472
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3207,
            "input_fields": [
                193472
            ],
            "name": "Rule 54",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193471
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3208,
            "input_fields": [
                193471
            ],
            "name": "Rule 55",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193470
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3209,
            "input_fields": [
                193470
            ],
            "name": "Rule 56",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193468
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3210,
            "input_fields": [
                193468
            ],
            "name": "Rule 57",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193464
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3211,
            "input_fields": [
                193464
            ],
            "name": "Rule 58",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193463
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3212,
            "input_fields": [
                193463
            ],
            "name": "Rule 59",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193402
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3213,
            "input_fields": [
                193402
            ],
            "name": "Rule 6",
            "params": {
                "udf_id": 17218
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193466
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3214,
            "input_fields": [
                193466
            ],
            "name": "Rule 60",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193457
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3215,
            "input_fields": [
                193457
            ],
            "name": "Rule 61",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193474
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3216,
            "input_fields": [
                193474
            ],
            "name": "Rule 62",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193456
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3217,
            "input_fields": [
                193456
            ],
            "name": "Rule 63",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193462
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3218,
            "input_fields": [
                193462
            ],
            "name": "Rule 64",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193461
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3219,
            "input_fields": [
                193461
            ],
            "name": "Rule 65",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193465
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3220,
            "input_fields": [
                193465
            ],
            "name": "Rule 66",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193460
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3221,
            "input_fields": [
                193460
            ],
            "name": "Rule 67",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193521
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3222,
            "input_fields": [
                193521
            ],
            "name": "Rule 68",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193520
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360125000,
            "description": "",
            "id": 3223,
            "input_fields": [
                193520
            ],
            "name": "Rule 69",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360125000
        },
        {
            "affected_fields": [
                193403
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360126000,
            "description": "",
            "id": 3224,
            "input_fields": [
                193403
            ],
            "name": "Rule 7",
            "params": {
                "udf_id": 17219
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360126000
        },
        {
            "affected_fields": [
                193518
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360126000,
            "description": "",
            "id": 3225,
            "input_fields": [
                193518
            ],
            "name": "Rule 70",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360126000
        },
        {
            "affected_fields": [
                193519
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360126000,
            "description": "",
            "id": 3226,
            "input_fields": [
                193519
            ],
            "name": "Rule 71",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360126000
        },
        {
            "affected_fields": [
                193517
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360126000,
            "description": "",
            "id": 3227,
            "input_fields": [
                193517
            ],
            "name": "Rule 72",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360126000
        },
        {
            "affected_fields": [
                193553
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360126000,
            "description": "",
            "id": 3228,
            "input_fields": [
                193553
            ],
            "name": "Rule 73",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360126000
        },
        {
            "affected_fields": [
                193554
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360126000,
            "description": "",
            "id": 3229,
            "input_fields": [
                193554
            ],
            "name": "Rule 74",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360126000
        },
        {
            "affected_fields": [
                193555
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360126000,
            "description": "",
            "id": 3230,
            "input_fields": [
                193555
            ],
            "name": "Rule 75",
            "params": {
                "confidence_threshold": 0.95
            },
            "scope": "FIELD",
            "type": "CONFIDENCE",
            "updated_at": 1756360126000
        },
        {
            "affected_fields": [
                193533
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360126000,
            "description": "",
            "id": 3231,
            "input_fields": [
                193533
            ],
            "name": "Rule 76",
            "params": {
                "udf_id": 17220
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360126000
        },
        {
            "affected_fields": [
                193513
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360127000,
            "description": "",
            "id": 3232,
            "input_fields": [
                193513
            ],
            "name": "Rule 77",
            "params": {
                "udf_id": 17221
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360127000
        },
        {
            "affected_fields": [
                193533
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360127000,
            "description": "",
            "id": 3233,
            "input_fields": [
                193533
            ],
            "name": "Rule 78",
            "params": {
                "udf_id": 17222
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360127000
        },
        {
            "affected_fields": [
                193489
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360127000,
            "description": "",
            "id": 3234,
            "input_fields": [
                193489
            ],
            "name": "Rule 79",
            "params": {
                "udf_id": 17223
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360127000
        },
        {
            "affected_fields": [
                193409
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360127000,
            "description": "",
            "id": 3235,
            "input_fields": [
                193409
            ],
            "name": "Rule 8",
            "params": {
                "udf_id": 17224
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360127000
        },
        {
            "affected_fields": [
                193439
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360128000,
            "description": "",
            "id": 3236,
            "input_fields": [
                193439
            ],
            "name": "Rule 80",
            "params": {
                "udf_id": 17225
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360128000
        },
        {
            "affected_fields": [
                193428
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360128000,
            "description": "",
            "id": 3237,
            "input_fields": [
                193428
            ],
            "name": "Rule 81",
            "params": {
                "udf_id": 17226
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360128000
        },
        {
            "affected_fields": [
                193494
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360128000,
            "description": "",
            "id": 3238,
            "input_fields": [
                193494
            ],
            "name": "Rule 82",
            "params": {
                "udf_id": 17227
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360128000
        },
        {
            "affected_fields": [
                193513
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360129000,
            "description": "",
            "id": 3239,
            "input_fields": [
                193513
            ],
            "name": "Rule 83",
            "params": {
                "udf_id": 17228
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360129000
        },
        {
            "affected_fields": [
                193415
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360129000,
            "description": "",
            "id": 3240,
            "input_fields": [
                193415
            ],
            "name": "Rule 84",
            "params": {
                "udf_id": 17229
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360129000
        },
        {
            "affected_fields": [
                193469
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360129000,
            "description": "",
            "id": 3241,
            "input_fields": [
                193469
            ],
            "name": "Rule 85",
            "params": {
                "udf_id": 17230
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360129000
        },
        {
            "affected_fields": [
                193522
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360129000,
            "description": "",
            "id": 3242,
            "input_fields": [
                193522
            ],
            "name": "Rule 86",
            "params": {
                "udf_id": 17231
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360129000
        },
        {
            "affected_fields": [
                193556
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360130000,
            "description": "",
            "id": 3243,
            "input_fields": [
                193556
            ],
            "name": "Rule 87",
            "params": {
                "udf_id": 17232
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360130000
        },
        {
            "affected_fields": [
                193412
            ],
            "alert_level": "FAILURE",
            "created_at": 1756360130000,
            "description": "",
            "id": 3244,
            "input_fields": [
                193412
            ],
            "name": "Rule 9",
            "params": {
                "udf_id": 17233
            },
            "scope": "FIELD",
            "type": "UDF",
            "updated_at": 1756360130000
        }
    ],
    "reader_profile": {
        "createdBy": "",
        "createdOn": 1753791095973,
        "defaultProfile": "",
        "foundationVersion": "",
        "inputPath": null,
        "lastModifiedBy": "",
        "lastModifiedOn": 1753791095973,
        "outputPath": null,
        "profiles": [
            {
                "entitySettings": [],
                "name": "aihub",
                "readSettings": {
                    "add_barcode_identifier": true,
                    "cache_pdf_results": false,
                    "correct_inversion": false,
                    "correct_orientation": false,
                    "correct_resolution": false,
                    "correct_resolution_auto": false,
                    "detect_barcodes": false,
                    "detect_blurry_files": false,
                    "dewarp_page": true,
                    "document_mapping": "document_is_record_skip_empty_page",
                    "enable_ibdoc_v2": true,
                    "enable_paragraph_ordering": false,
                    "enable_table_markdown_enrichment": false,
                    "encryption_config": null,
                    "entity_models": [
                        {
                            "model_name": "signature_model",
                            "model_version": "0.0.4"
                        },
                        {
                            "model_name": "barcode_qrcode_detection",
                            "model_version": "1.0.0"
                        }
                    ],
                    "extract_all_pdf_layers": false,
                    "find_lines": false,
                    "fonts": null,
                    "force_image_ocr": true,
                    "image_filters": null,
                    "languages": [
                        "en"
                    ],
                    "layout_algorithm": "layout_algo_spatial",
                    "model_specific_settings": {
                        "marx_v1": {
                            "version": "layout_v3"
                        }
                    },
                    "native_excel_processing": true,
                    "ocr_page_type": "marx",
                    "ocr_timeout": null,
                    "output_format_layout": "layout_per_page",
                    "output_formats": null,
                    "page_range_str": "",
                    "preprocess_excel_files": false,
                    "process_type": "auto_to_txt",
                    "produce_metadata_list": true,
                    "produce_word_metadata": true,
                    "remove_boxes": false,
                    "remove_boxes_over_height_percent": null,
                    "remove_boxes_over_width_percent": null,
                    "remove_space_wordpolys": true,
                    "remove_vertical_text": false,
                    "reorient_words": false,
                    "repair_pdfs": false,
                    "scripts_dir": "",
                    "write_converted_image": true,
                    "write_thumbnail": true
                }
            }
        ],
        "schema": "1"
    }
}